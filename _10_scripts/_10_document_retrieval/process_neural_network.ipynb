{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os \n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 1\n",
    "\n",
    "claim_tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataNeuralNetwork\n",
      "- data already created\n",
      "NeuralNetwork\n",
      "- load model\n"
     ]
    }
   ],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "# load model\n",
    "# === variables === #\n",
    "claim_data_set = 'dev'\n",
    "method_database = 'equal_class' # include_all, equal_class\n",
    "setup = 1\n",
    "settings_model = {}\n",
    "settings_model['fraction_training'] = 0.9\n",
    "settings_model['use_cuda'] = False\n",
    "settings_model['seed'] = 1\n",
    "settings_model['lr'] = 0.001\n",
    "settings_model['momentum'] = 0.9 # 0.5\n",
    "settings_model['params'] = {'batch_size': 64, 'shuffle': True}\n",
    "settings_model['nr_epochs'] = 10\n",
    "settings_model['log_interval'] = 10\n",
    "settings_model['width'] = 2000\n",
    "\n",
    "neural_network = NeuralNetwork(claim_data_set, method_database, setup, settings_model)\n",
    "    \n",
    "model_nn = neural_network.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_db import dict_load_json, dict_save_json\n",
    "from utils_doc_results import Claim\n",
    "from tqdm import tqdm \n",
    "from doc_results_db import ClaimFile\n",
    "\n",
    "class PredictLabels():\n",
    "    def __init__(self, K, threshold, method, claim_tensor_db, wiki_database, neural_network):\n",
    "        # --- process input --- #\n",
    "        self.K = K\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.claim_tensor_db = claim_tensor_db\n",
    "        self.model_nn = neural_network.model\n",
    "        # --- variables --- #\n",
    "        self.nr_claims = self.claim_tensor_db.settings['nr_total']\n",
    "        self.path_predict_label_dir = os.path.join(claim_tensor_db.path_setup_dir, 'Predictions_' + str(K) + '_' + method + '_' + neural_network.file_name)\n",
    "        mkdir_if_not_exist(self.path_predict_label_dir)\n",
    "        self.path_settings = os.path.join(self.path_predict_label_dir, 'settings.json')\n",
    "        \n",
    "        if os.path.isfile(self.path_settings):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "        else:\n",
    "            self.settings = {}\n",
    "            self.get_accuracy_save_results(wiki_database)\n",
    "            dict_save_json(self.settings, self.path_settings)\n",
    "        \n",
    "    def get_accuracy_save_results(self, wiki_database):\n",
    "        nr_correct = 0\n",
    "        nr_documents_selected = 0\n",
    "        \n",
    "        for id_nr in tqdm(range(self.nr_claims)):\n",
    "            path_file = os.path.join(self.claim_tensor_db.path_dict_variable_list_dir, str(id_nr) + '.json')\n",
    "            dict_variables = dict_load_json(path_file)\n",
    "            id = dict_variables['id']\n",
    "            selected_documents_list = list(dict_variables['selected_documents'].keys())\n",
    "            pred_value_list = []\n",
    "            for selected_document_str in selected_documents_list:\n",
    "                flag_process = 0\n",
    "                if method == 'generated':\n",
    "                    if int(selected_document_str) in dict_variables['ids_generated']:\n",
    "                        flag_process = 1\n",
    "                elif method == 'correct':\n",
    "                    if int(selected_document_str) in dict_variables['ids_correct_docs']:\n",
    "                        flag_process = 1\n",
    "                elif method == 'selected':\n",
    "                    flag_process = 1\n",
    "                else:\n",
    "                    raise ValueError('method not in method_list', method)\n",
    "                    \n",
    "                if flag_process == 1:\n",
    "                    variable_list = dict_variables['selected_documents'][selected_document_str]['list_variables']\n",
    "                    variable_tensor = torch.FloatTensor(variable_list)\n",
    "                    pred_value_list += [self.model_nn(variable_tensor.unsqueeze(0)).item()]\n",
    "                    if 'predicted_true' in dict_variables:\n",
    "                        dict_variables['predicted_true'].append(selected_document_str) \n",
    "                    else:\n",
    "                        dict_variables['predicted_true'] = selected_document_str\n",
    "                        \n",
    "            pred_value_list_sorted = [x for x,_ in sorted(zip(pred_value_list, selected_documents_list))]\n",
    "            pred_id_correct_list = [x for _,x in sorted(zip(pred_value_list, selected_documents_list))]\n",
    "            id_correct_list = []\n",
    "            length_list = len(pred_id_correct_list)\n",
    "\n",
    "            for i in range(length_list):\n",
    "                if i<self.K:\n",
    "                    id_correct_list.append(pred_id_correct_list[length_list-1-i])\n",
    "                elif pred_value_list_sorted[length_list-1-i] > self.threshold:\n",
    "                    id_correct_list.append(pred_id_correct_list[length_list-1-i])\n",
    "                else:\n",
    "                    break\n",
    "                nr_documents_selected += 1\n",
    "\n",
    "            file = ClaimFile(id = id, path_dir_files = self.claim_tensor_db.path_claims_dir)\n",
    "            claim_dict = claim_database.get_claim_from_id(id)\n",
    "            claim = Claim(claim_dict)\n",
    "            for interpreter in claim.evidence:\n",
    "                flag_correctly_predicted = True\n",
    "                for proof in interpreter:\n",
    "                    title_proof = proof[2]\n",
    "                    if title_proof == None:\n",
    "                        raise ValueError('should contain proof')\n",
    "                    else:\n",
    "                        id_proof = wiki_database.get_id_from_title(title_proof)\n",
    "                        if str(id_proof) not in id_correct_list:\n",
    "                            flag_correctly_predicted = False\n",
    "                            break\n",
    "                if flag_correctly_predicted == True:\n",
    "                    nr_correct += 1\n",
    "                    break\n",
    "        \n",
    "        print('accuracy', accuracy, nr_documents_selected)\n",
    "        self.settings['accuracy'] = nr_correct / float(nr_claims)\n",
    "        self.settings['nr_documents_selected'] = nr_documents_selected\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NeuralNetwork' object has no attribute 'file_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-728a94358c81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;31m# nr_claims = claim_tensor_db.settings['nr_total']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mpredict_labels_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPredictLabels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclaim_tensor_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwiki_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-1a585d8689ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, K, threshold, method, claim_tensor_db, wiki_database, neural_network)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# --- variables --- #\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnr_claims\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclaim_tensor_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nr_total'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_predict_label_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim_tensor_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_setup_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Predictions_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mmkdir_if_not_exist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_predict_label_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_predict_label_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'settings.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NeuralNetwork' object has no attribute 'file_name'"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "# claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 1\n",
    "\n",
    "claim_tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "\n",
    "method_list = ['generated', 'correct', 'selected']\n",
    "method = 'generated'\n",
    "K = 0\n",
    "threshold = 0.2\n",
    "\n",
    "# nr_claims = claim_tensor_db.settings['nr_total']\n",
    "\n",
    "predict_labels_db = PredictLabels(K, threshold, method, claim_tensor_db, wiki_database, neural_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
