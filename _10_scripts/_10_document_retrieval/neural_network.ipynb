{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "from utils_db import mkdir_if_not_exist, dict_save_json, dict_load_json\n",
    "\n",
    "import config\n",
    "\n",
    "class DataNeuralNetwork():\n",
    "    def __init__(self, method_database, setup):\n",
    "        self.setup = setup\n",
    "        self.method_database = method_database\n",
    "        self.path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "        self.path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "        self.tensor_db = ClaimTensorDatabase(self.path_wiki_pages, self.path_wiki_database_dir, self.setup)\n",
    "        self.path_combined_data_dir = os.path.join(self.tensor_db.path_results_dir, 'neural_network', 'data_setup_' + str(self.setup) + '_' + self.method_database)\n",
    "        self.path_settings = os.path.join(self.path_combined_data_dir, 'settings.json')\n",
    "        \n",
    "        print('DataNeuralNetwork')\n",
    "        if os.path.isdir(self.path_combined_data_dir):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "        else:\n",
    "            self.settings = {}\n",
    "            self.create_folder_combined_files()\n",
    "            \n",
    "    def create_folder_combined_files(self):\n",
    "        mkdir_if_not_exist(self.path_combined_data_dir)\n",
    "\n",
    "        dir_list = [self.tensor_db.path_label_correct_evidence_true_dir, \n",
    "                   self.tensor_db.path_label_correct_evidence_false_dir,\n",
    "                   self.tensor_db.path_label_refuted_evidence_true_dir,\n",
    "                   self.tensor_db.path_label_refuted_evidence_false_dir]\n",
    "        \n",
    "        nr_observations_list = [self.tensor_db.settings['nr_correct_true'], \n",
    "                                self.tensor_db.settings['nr_correct_false'], \n",
    "                                self.tensor_db.settings['nr_refuted_true'], \n",
    "                                self.tensor_db.settings['nr_refuted_false']]\n",
    "\n",
    "        if method_database == 'include_all':\n",
    "            total_nr_observations = sum(nr_observations_list)\n",
    "            random_id_list = list(range(total_nr_observations))\n",
    "            shuffle(random_id_list)\n",
    "\n",
    "            for i in tqdm(range(len(dir_list))):\n",
    "                dir = dir_list[i]\n",
    "                nr_obervations = nr_observations_list[i]\n",
    "                for idx in range(nr_obervations):\n",
    "                    transformed_ids = random_id_list[idx + sum(nr_observations_list[0:i])]\n",
    "                    \n",
    "                    file_name_variables_load = os.path.join(dir, 'variable_' + str(idx) + '.pt')\n",
    "                    file_name_label_load = os.path.join(dir, 'label_' + str(idx) + '.pt')\n",
    "                    \n",
    "                    file_name_variables_write = os.path.join(self.path_combined_data_dir, 'variable_' + str(transformed_ids) + '.pt')\n",
    "                    file_name_label_write = os.path.join(self.path_combined_data_dir, 'label_' + str(transformed_ids) + '.pt')\n",
    "\n",
    "                    X = torch.load(file_name_variables_load)\n",
    "                    Y = torch.load(file_name_label_load)\n",
    "\n",
    "                    torch.save(X, file_name_variables_write)\n",
    "                    torch.save(Y, file_name_label_write)\n",
    "\n",
    "            self.settings['nr_observations'] = len(random_id_list)\n",
    "\n",
    "        elif method_database == 'equal_class':\n",
    "            min_nr_observations = min(nr_observations_list)\n",
    "            random_id_list = list(range(min_nr_observations*4))\n",
    "            shuffle(random_id_list)\n",
    "\n",
    "            for i in tqdm(range(len(dir_list))):\n",
    "                dir = dir_list[i]\n",
    "                nr_obervations = nr_observations_list[i]\n",
    "                random_id_list_setting = list(range(nr_obervations))\n",
    "                shuffle(random_id_list_setting)\n",
    "                j=0\n",
    "                for idx in random_id_list_setting[0:min_nr_observations]:\n",
    "                    transformed_ids = random_id_list[j + min_nr_observations*i]\n",
    "                    \n",
    "                    file_name_variables_load = os.path.join(dir, 'variable_' + str(idx) + '.pt')\n",
    "                    file_name_label_load = os.path.join(dir, 'label_' + str(idx) + '.pt')                    \n",
    "                    file_name_variables_write = os.path.join(self.path_combined_data_dir, 'variable_' + str(transformed_ids) + '.pt')\n",
    "                    file_name_label_write = os.path.join(self.path_combined_data_dir, 'label_' + str(transformed_ids) + '.pt')\n",
    "\n",
    "                    X = torch.load(file_name_variables_load)\n",
    "                    Y = torch.load(file_name_label_load)\n",
    "\n",
    "                    torch.save(X, file_name_variables_write)\n",
    "                    torch.save(Y, file_name_label_write)\n",
    "                    j += 1\n",
    "\n",
    "            self.settings['nr_observations'] = len(random_id_list)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('method not in method options', method_database)\n",
    "\n",
    "        dict_save_json(self.settings, self.path_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nr_input_variables, nr_hidden_neurons, nr_output_variables):\n",
    "        super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "#         self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc_input = nn.Linear(nr_input_variables, nr_hidden_neurons)\n",
    "        self.fc_hidden = nn.Linear(nr_hidden_neurons, nr_hidden_neurons)\n",
    "        self.fc_output = nn.Linear(nr_hidden_neurons, nr_output_variables)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm1d(nr_hidden_neurons)\n",
    "        self.bn2 = nn.BatchNorm1d(nr_hidden_neurons)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_input(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc_hidden(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc_output(x))\n",
    "    \n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def train_performance(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "            output = model(data.float())\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "def test_performance(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "            output = model(data.float())\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = data_nn.tensor_db.settings\n",
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_variables = data_nn.tensor_db.settings['nr_variables']\n",
    "# nr_variables\n",
    "# nr_variables = 0\n",
    "# list_keys = ['observation_key_list_claim', 'observation_key_list_title', 'observation_key_list_text']\n",
    "# for key in list_keys:\n",
    "#     nr_variables += len(settings[key])\n",
    "# nr_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, path_data_set, list_ids):\n",
    "        'Initialization'\n",
    "        self.path_data_set = path_data_set\n",
    "        self.list_ids = list_ids\n",
    "        self.nr_observations = len(list_ids)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.nr_observations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        file_name_variables = os.path.join(self.path_data_set, 'variable_' + str(self.list_ids[index])  + '.pt')\n",
    "        file_name_label = os.path.join(self.path_data_set, 'label_' + str(self.list_ids[index])  + '.pt')\n",
    "        \n",
    "        X = torch.load(file_name_variables)\n",
    "        y = torch.load(file_name_label)[0]\n",
    "        \n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "import config\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, claim_data_set, method_database, setup, settings_model):\n",
    "        self.claim_data_set = claim_data_set\n",
    "        self.method_database = method_database\n",
    "        self.setup = setup\n",
    "        \n",
    "        self.fraction_training = settings_model['fraction_training']\n",
    "        self.use_cuda = settings_model['use_cuda']\n",
    "        self.seed = settings_model['seed']\n",
    "        self.lr = settings_model['lr']\n",
    "        self.momentum = settings_model['momentum']\n",
    "        self.params = settings_model['params']\n",
    "        self.nr_epochs = settings_model['nr_epochs']\n",
    "        self.log_interval = settings_model['log_interval']\n",
    "        \n",
    "        self.data_nn = self.get_data()\n",
    "        self.nr_observations = self.data_nn.settings['nr_observations']\n",
    "        self.nr_variables = self.data_nn.tensor_db.settings['nr_variables']\n",
    "        \n",
    "        self.path_model_dir = os.path.join(self.data_nn.tensor_db.path_results_dir, 'neural_network', 'model')\n",
    "        self.path_model = os.path.join(self.path_model_dir ,'model.pt')\n",
    "        self.path_settings = os.path.join(self.path_model_dir, 'settings.json')\n",
    "        \n",
    "        mkdir_if_not_exist(self.path_model_dir)\n",
    "        print('NeuralNetwork')\n",
    "        if os.path.isfile(self.path_model):\n",
    "            print('- load model')\n",
    "            self.model = torch.load(self.path_model)\n",
    "        else:\n",
    "            print('- train model')\n",
    "            self.partition = self.get_partition()\n",
    "            self.training_data_loader, self.validation_data_loader = self.get_data_generators()\n",
    "            self.model = self.train_model()\n",
    "            torch.save(self.model.state_dict(), self.path_model)\n",
    "            \n",
    "        \n",
    "    def get_data(self):\n",
    "        return DataNeuralNetwork(self.method_database, self.setup)\n",
    "    \n",
    "    def get_partition(self):\n",
    "        partition = {}\n",
    "        partition['train'] = list(range(0, int(self.nr_observations*self.fraction_training)))\n",
    "        partition['validation'] = list(range(int(self.nr_observations*self.fraction_training), self.nr_observations))\n",
    "        return partition\n",
    "    \n",
    "    def get_data_generators(self):\n",
    "        training_set = Dataset(self.data_nn.path_combined_data_dir, self.partition['train'])\n",
    "        training_data_loader = data.DataLoader(training_set, **self.params)\n",
    "\n",
    "        validation_set = Dataset(self.data_nn.path_combined_data_dir, self.partition['validation'])\n",
    "        validation_data_loader = data.DataLoader(validation_set, **self.params)\n",
    "        return training_data_loader, validation_data_loader\n",
    "        \n",
    "    def train_model(self):\n",
    "        torch.manual_seed(self.seed)\n",
    "\n",
    "        kwargs = {'num_workers': 6, 'pin_memory': True} if self.use_cuda else {}\n",
    "\n",
    "        device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "        model = Net(nr_input_variables = self.nr_variables, nr_hidden_neurons = 10, nr_output_variables = 2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        \n",
    "        for epoch in range(1, self.nr_epochs + 1):\n",
    "            train(self.log_interval, model, device, self.training_data_loader, optimizer, epoch)\n",
    "            train_performance(model, device, self.training_data_loader)\n",
    "            test_performance(model, device, self.validation_data_loader)\n",
    "        \n",
    "        return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataNeuralNetwork\n",
      "NeuralNetwork\n",
      "- load model\n"
     ]
    }
   ],
   "source": [
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "# === constants === #\n",
    "\n",
    "# === variables === #\n",
    "claim_data_set = 'dev'\n",
    "method_database = 'equal_class' # include_all, equal_class\n",
    "setup = 1\n",
    "settings_model = {}\n",
    "settings_model['fraction_training'] = 0.9\n",
    "settings_model['use_cuda'] = False\n",
    "settings_model['seed'] = 1\n",
    "settings_model['lr'] = 0.001\n",
    "settings_model['momentum'] = 0.9 # 0.5\n",
    "settings_model['params'] = {'batch_size': 64, 'shuffle': True}\n",
    "settings_model['nr_epochs'] = 5\n",
    "settings_model['log_interval'] = 10\n",
    "\n",
    "neural_network = NeuralNetwork(claim_data_set, method_database, setup, settings_model)\n",
    "    \n",
    "model = neural_network.model\n",
    "# for every \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('json file does not exist', '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/setup_3/settings.json')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b9fe16457338>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0msetup\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mclaim_tensor_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClaimTensorDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_wiki_pages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_wiki_database_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/C_disk/02_university/06_thesis/01_code/fever/_10_scripts/_10_document_retrieval/doc_results_db.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_wiki_pages, path_wiki_database_dir, setup)\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnr_claims\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClaimFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_dir_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_claims_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 316\u001b[0;31m                 \u001b[0mclaim_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mperformance_tf_idf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclaim_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_claim_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    317\u001b[0m                 \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_claims_selected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_database\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/02_university/06_thesis/01_code/fever/_10_scripts/_10_document_retrieval/utils_db.py\u001b[0m in \u001b[0;36mdict_load_json\u001b[0;34m(path_file)\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json file does not exist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdictionary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('json file does not exist', '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/setup_3/settings.json')"
     ]
    }
   ],
   "source": [
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 3\n",
    "\n",
    "claim_tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print('claim database: save results to folder with tensors')\n",
    "\n",
    "settings_dict = {}\n",
    "\n",
    "id = 5\n",
    "file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "\n",
    "id_list_title = list(file.claim_dict['title']['1_gram'].keys())\n",
    "id_list_text = list(file.claim_dict['text']['1_gram'].keys())\n",
    "\n",
    "observation_key_list_claim, _ = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "\n",
    "if len(id_list_title) > 0:\n",
    "    observation_key_list_title , _ = get_list_properties(file.claim_dict['title']['1_gram'][id_list_title[0]], [], [], []) \n",
    "else:\n",
    "    observation_key_list_title = []\n",
    "\n",
    "if len(id_list_text) > 0:\n",
    "    observation_key_list_text,  _ = get_list_properties(file.claim_dict['text']['1_gram'][id_list_text[0]], [], [], [])\n",
    "else:\n",
    "    observation_key_list_text = []\n",
    "\n",
    "settings_dict['observation_key_list_claim'] = observation_key_list_claim\n",
    "settings_dict['observation_key_list_title'] = observation_key_list_title\n",
    "settings_dict['observation_key_list_text'] = observation_key_list_text\n",
    "\n",
    "idx = 0\n",
    "\n",
    "nr_correct_false = 0\n",
    "nr_correct_true = 0\n",
    "nr_refuted_false = 0\n",
    "nr_refuted_true = 0\n",
    "\n",
    "for id in tqdm(range(self.claim_database.nr_claims), desc = 'save_2_tensor'):\n",
    "    file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "\n",
    "    label = file.claim_dict['claim']['label']\n",
    "\n",
    "    if label != 'NOT ENOUGH INFO':\n",
    "        label_nr = label_2_num(label)\n",
    "\n",
    "        for id_document in file.claim_dict['ids_selected']:\n",
    "\n",
    "            if label == 'SUPPORTS':\n",
    "                if id_document in file.claim_dict['ids_correct_docs']:\n",
    "                    file_name_variables = os.path.join(self.path_label_correct_evidence_true_dir, 'variable_' + str(nr_correct_true) + '.pt')\n",
    "                    file_name_label = os.path.join(self.path_label_correct_evidence_true_dir, 'label_' + str(nr_correct_true) + '.pt')\n",
    "                    nr_correct_true += 1\n",
    "                else:\n",
    "                    file_name_variables = os.path.join(self.path_label_correct_evidence_false_dir, 'variable_' + str(nr_correct_false) + '.pt')\n",
    "                    file_name_label = os.path.join(self.path_label_correct_evidence_false_dir, 'label_' + str(nr_correct_false) + '.pt')\n",
    "                    nr_correct_false += 1\n",
    "\n",
    "            elif label == 'REFUTES':\n",
    "                if id_document in file.claim_dict['ids_correct_docs']:\n",
    "                    file_name_variables = os.path.join(self.path_label_refuted_evidence_true_dir, 'variable_' + str(nr_refuted_true) + '.pt')\n",
    "                    file_name_label = os.path.join(self.path_label_refuted_evidence_true_dir, 'label_' + str(nr_refuted_true) + '.pt')\n",
    "                    nr_refuted_true += 1\n",
    "                else:\n",
    "                    file_name_variables = os.path.join(self.path_label_refuted_evidence_false_dir, 'variable_' + str(nr_refuted_false) + '.pt')\n",
    "                    file_name_label = os.path.join(self.path_label_refuted_evidence_false_dir, 'label_' + str(nr_refuted_false) + '.pt')\n",
    "                    nr_refuted_false += 1\n",
    "            else:\n",
    "                raise ValueError('label not correct', label)\n",
    "\n",
    "            list_variables = []\n",
    "\n",
    "            id_list = list(file.claim_dict['title']['1_gram'].keys())\n",
    "\n",
    "            _, values_claim = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "            list_variables += values_claim\n",
    "            if str(id_document) in file.claim_dict['title']['1_gram']:\n",
    "                _, values_title = get_list_properties(file.claim_dict['title']['1_gram'][str(id_document)], [], [], [])\n",
    "                list_variables += values_title\n",
    "\n",
    "            if str(id_document) in file.claim_dict['text']['1_gram']:\n",
    "                _, values_text  = get_list_properties(file.claim_dict['text']['1_gram'][str(id_document)], [], [], [])\n",
    "                list_variables += values_text\n",
    "\n",
    "            tensor_variable = torch.FloatTensor(list_variables)  \n",
    "            tensor_label = torch.LongTensor([label_nr])\n",
    "\n",
    "            torch.save(tensor_variable, file_name_variables)\n",
    "            torch.save(tensor_label, file_name_label)\n",
    "\n",
    "            idx += 1\n",
    "\n",
    "nr_variables = 0\n",
    "list_keys = ['observation_key_list_claim', 'observation_key_list_title', 'observation_key_list_text']\n",
    "for key in list_keys:\n",
    "    nr_variables += len(settings_dict[key])\n",
    "\n",
    "settings_dict['nr_variables'] = nr_variables\n",
    "settings_dict['nr_total'] = idx\n",
    "settings_dict['nr_correct_false'] = nr_correct_false\n",
    "settings_dict['nr_correct_true'] = nr_correct_true\n",
    "settings_dict['nr_refuted_false'] = nr_refuted_false\n",
    "settings_dict['nr_refuted_true'] = nr_refuted_true\n",
    "\n",
    "settings_dict['nr_claims'] = self.claim_database.nr_claims\n",
    "\n",
    "dict_save_json(settings_dict, self.path_settings_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_6.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1909f773a38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/label_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_6.pt'"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "file_name_variables = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_' + str(id) + '.pt'\n",
    "file_name_label = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/label_' + str(id) + '.pt'\n",
    "\n",
    "X = torch.load(file_name_variables)\n",
    "y = torch.load(file_name_label)\n",
    "y, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/setup_1/dev_correct_false_tensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]),\n",
       " tensor([9.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         2.0000, 1.0000, 0.0000, 0.0000, 3.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 3.0780, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 3.0785, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0780, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0785]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 1\n",
    "file_name_variables = os.path.join(dir,'variable_' + str(id) + '.pt')\n",
    "file_name_label = os.path.join(dir,'label_' + str(id) + '.pt')\n",
    "\n",
    "X = torch.load(file_name_variables)\n",
    "y = torch.load(file_name_label)\n",
    "y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_nr = 0\n",
    "tensor_label = torch.LongTensor([label_nr])\n",
    "torch.save(tensor_label, 'tmp.pt')\n",
    "tensor_temp = torch.load('tmp.pt')\n",
    "torch.save(tensor_temp, 'tmp.pt')\n",
    "torch.load('tmp.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
