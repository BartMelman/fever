{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "from wiki_database import WikiDatabaseSqlite, Text\n",
    "from doc_results_db_utils import get_empty_tag_dict, get_tag_2_id_dict, get_tag_dict, get_vocab_tf_idf_from_exp, get_tf_idf_name, get_vocab_tf_idf_from_exp\n",
    "from doc_results_db_utils import get_dict_from_n_gram\n",
    "from utils_db import dict_load_json, dict_save_json, HiddenPrints, load_jsonl\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer, get_tag_word_from_wordtag\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing settings file\n",
      "Load title dictionary\n",
      "tags file already exists\n"
     ]
    }
   ],
   "source": [
    "# === variables === #\n",
    "n_gram = 1\n",
    "claim_data_set = 'dev'\n",
    "folder_name_score_combination = 'score_combination'\n",
    "\n",
    "path_dir_results = os.path.join(config.ROOT, config.RESULTS_DIR, folder_name_score_combination)\n",
    "path_tags = os.path.join(path_dir_results, 'tags_' + claim_data_set + '_n_gram_' + str(n_gram) + '.json')\n",
    "# path_dir_files = os.path.join(path_dir_results, claim_data_set)\n",
    "path_dir_claims = os.path.join(path_dir_results, 'claims_' + claim_data_set)\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "path_claim_data_set = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR, claim_data_set + \".jsonl\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_results, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_results)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_claims, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_claim)\n",
    "\n",
    "results = load_jsonl(path_claim_data_set)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "tag_dict = get_tag_dict(claim_data_set, n_gram, path_tags, wiki_database)\n",
    "nr_claims = 20#len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('claim database: insert claim\\'s text and claim\\'s tag_list')\n",
    "\n",
    "for str_id, tag_list in tqdm(tag_dict.items(), total = len(tag_dict), desc = 'tag'):\n",
    "    id = int(str_id)\n",
    "    if id < nr_claims:\n",
    "        file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "        file.process_tags(tag_list, n_gram)\n",
    "        file.process_claim(results[id])\n",
    "\n",
    "print('claim database: insert nr words per tag for claim')\n",
    "\n",
    "experiment_nr = 37\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "for id in tqdm(range(nr_claims), desc = 'nr words per pos'):\n",
    "    file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "    file.process_nr_words_per_pos(tf_idf_db, tag_2_id_dict)\n",
    "\n",
    "print('claim database: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tf_idf nr_words_pos\n"
     ]
    }
   ],
   "source": [
    "from sqlitedict import SqliteDict\n",
    "from doc_results_db_utils import get_vocab_tf_idf_from_exp\n",
    "from utils_db import HiddenPrints\n",
    "\n",
    "# === run experiment === #\n",
    "experiment_nr = 31\n",
    "\n",
    "print('load tf_idf nr_words_pos')\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "mydict_ids = SqliteDict(tf_idf_db.path_ids_dict)\n",
    "mydict_tf_idf = SqliteDict(tf_idf_db.path_tf_idf_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_gram': {'nr_words': None,\n",
       "  'nr_words_per_pos': {'0': 0,\n",
       "   '1': 0,\n",
       "   '2': 0,\n",
       "   '3': 0,\n",
       "   '4': 0,\n",
       "   '5': 0,\n",
       "   '6': 0,\n",
       "   '7': 0,\n",
       "   '8': 0,\n",
       "   '9': 0,\n",
       "   '10': 0,\n",
       "   '11': 0,\n",
       "   '12': 0,\n",
       "   '13': 0,\n",
       "   '14': 0,\n",
       "   '15': 0,\n",
       "   '16': 0,\n",
       "   '17': 0,\n",
       "   '18': 0}}}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = ClaimFile(id = 0, path_dir_files = path_dir_results)\n",
    "file.claim_dict['claim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/score_combination/claims_dev'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dir_claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "nr words per pos:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   0%|          | 1/1000 [00:14<3:56:22, 14.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   0%|          | 2/1000 [00:14<2:48:32, 10.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   0%|          | 3/1000 [00:41<4:10:56, 15.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   0%|          | 4/1000 [00:46<3:22:02, 12.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   0%|          | 5/1000 [00:53<2:52:36, 10.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 6/1000 [00:55<2:11:49,  7.96s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 7/1000 [01:01<2:04:08,  7.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 8/1000 [01:02<1:28:43,  5.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 9/1000 [01:09<1:40:11,  6.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 10/1000 [01:10<1:12:58,  4.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 11/1000 [01:15<1:14:50,  4.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|          | 12/1000 [01:15<53:59,  3.28s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|▏         | 13/1000 [01:15<39:19,  2.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   1%|▏         | 14/1000 [01:17<34:15,  2.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 15/1000 [01:22<49:05,  2.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 16/1000 [01:25<51:29,  3.14s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 17/1000 [01:46<2:15:02,  8.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 18/1000 [01:57<2:29:45,  9.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 19/1000 [02:06<2:28:31,  9.08s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 20/1000 [02:23<3:09:56, 11.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 21/1000 [02:30<2:44:52, 10.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 22/1000 [02:38<2:36:18,  9.59s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 23/1000 [02:45<2:19:43,  8.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▏         | 24/1000 [03:00<2:51:23, 10.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   2%|▎         | 25/1000 [03:06<2:29:45,  9.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 26/1000 [03:12<2:15:44,  8.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 27/1000 [03:15<1:47:27,  6.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 28/1000 [03:20<1:38:24,  6.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 29/1000 [03:35<2:21:57,  8.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 30/1000 [03:45<2:30:10,  9.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 31/1000 [03:47<1:54:36,  7.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 32/1000 [04:07<2:58:17, 11.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 33/1000 [04:19<3:01:25, 11.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   3%|▎         | 34/1000 [04:21<2:17:06,  8.52s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▎         | 35/1000 [04:30<2:17:13,  8.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▎         | 36/1000 [04:39<2:20:59,  8.78s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▎         | 37/1000 [04:41<1:49:11,  6.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 38/1000 [04:50<1:57:59,  7.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 39/1000 [05:00<2:08:40,  8.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 40/1000 [05:09<2:13:33,  8.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 41/1000 [05:23<2:43:41, 10.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 42/1000 [05:26<2:09:24,  8.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 43/1000 [05:37<2:23:10,  8.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 44/1000 [05:49<2:35:03,  9.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   4%|▍         | 45/1000 [06:03<2:53:37, 10.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▍         | 46/1000 [06:03<2:05:13,  7.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▍         | 47/1000 [06:06<1:38:17,  6.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▍         | 48/1000 [06:13<1:44:40,  6.60s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▍         | 49/1000 [06:23<2:02:10,  7.71s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▌         | 50/1000 [06:33<2:10:08,  8.22s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▌         | 51/1000 [06:43<2:18:59,  8.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▌         | 52/1000 [06:51<2:13:29,  8.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▌         | 53/1000 [06:54<1:48:31,  6.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   5%|▌         | 54/1000 [07:00<1:43:20,  6.55s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 55/1000 [07:02<1:23:42,  5.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 56/1000 [07:05<1:11:56,  4.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 57/1000 [07:08<1:06:08,  4.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 58/1000 [07:18<1:32:14,  5.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 59/1000 [07:29<1:55:58,  7.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 60/1000 [07:41<2:15:11,  8.63s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▌         | 62/1000 [07:44<1:42:43,  6.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▋         | 63/1000 [07:57<2:10:45,  8.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▋         | 64/1000 [08:03<2:01:08,  7.77s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   6%|▋         | 65/1000 [08:12<2:07:00,  8.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 66/1000 [08:21<2:10:15,  8.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 67/1000 [08:25<1:51:46,  7.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 68/1000 [08:40<2:26:01,  9.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 69/1000 [08:45<2:04:18,  8.01s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 70/1000 [08:45<1:30:19,  5.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 71/1000 [08:48<1:15:12,  4.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 72/1000 [08:53<1:14:25,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 73/1000 [09:02<1:33:13,  6.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   7%|▋         | 74/1000 [09:14<2:03:07,  7.98s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 75/1000 [09:20<1:55:35,  7.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 76/1000 [09:30<2:07:09,  8.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 77/1000 [09:31<1:31:17,  5.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 78/1000 [09:32<1:09:15,  4.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 79/1000 [09:44<1:42:02,  6.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 80/1000 [09:51<1:45:25,  6.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 81/1000 [09:52<1:17:55,  5.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 82/1000 [09:57<1:17:20,  5.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 83/1000 [09:59<1:02:08,  4.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 84/1000 [10:04<1:07:45,  4.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   8%|▊         | 85/1000 [10:12<1:23:23,  5.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▊         | 86/1000 [10:22<1:44:20,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▊         | 87/1000 [10:38<2:27:37,  9.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 88/1000 [10:39<1:46:22,  7.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 89/1000 [10:42<1:26:31,  5.70s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 90/1000 [10:45<1:13:52,  4.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 91/1000 [10:52<1:25:39,  5.65s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 92/1000 [10:57<1:19:36,  5.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 93/1000 [11:05<1:33:51,  6.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:   9%|▉         | 94/1000 [11:12<1:39:24,  6.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|▉         | 95/1000 [11:25<2:07:09,  8.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|▉         | 96/1000 [11:30<1:50:22,  7.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|▉         | 97/1000 [11:33<1:31:53,  6.11s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|▉         | 98/1000 [11:41<1:39:52,  6.64s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|▉         | 99/1000 [11:44<1:23:04,  5.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 100/1000 [11:48<1:16:31,  5.10s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 101/1000 [11:51<1:04:29,  4.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 102/1000 [12:03<1:39:46,  6.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 103/1000 [12:09<1:39:00,  6.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 104/1000 [12:10<1:12:10,  4.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  10%|█         | 105/1000 [12:13<1:02:13,  4.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 106/1000 [12:17<1:02:16,  4.18s/it]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nr words per pos:  11%|█         | 107/1000 [12:39<2:22:31,  9.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 108/1000 [12:41<1:46:47,  7.18s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 109/1000 [12:48<1:48:02,  7.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 110/1000 [13:03<2:20:23,  9.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 111/1000 [13:16<2:39:18, 10.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█         | 112/1000 [13:33<3:05:55, 12.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█▏        | 113/1000 [13:38<2:31:23, 10.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  11%|█▏        | 114/1000 [13:40<1:55:00,  7.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 115/1000 [13:47<1:48:57,  7.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 116/1000 [13:54<1:48:57,  7.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 117/1000 [13:58<1:35:09,  6.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 118/1000 [13:59<1:11:24,  4.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 119/1000 [14:14<1:55:43,  7.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 120/1000 [14:22<1:56:11,  7.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 121/1000 [14:26<1:37:34,  6.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 122/1000 [14:27<1:13:09,  5.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 123/1000 [14:36<1:28:35,  6.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▏        | 124/1000 [14:47<1:53:00,  7.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  12%|█▎        | 125/1000 [14:50<1:29:54,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 126/1000 [14:55<1:27:16,  5.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 127/1000 [14:58<1:11:42,  4.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 128/1000 [15:00<59:33,  4.10s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 129/1000 [15:09<1:19:54,  5.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 130/1000 [15:09<57:48,  3.99s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 131/1000 [15:16<1:07:41,  4.67s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 132/1000 [15:16<48:21,  3.34s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 133/1000 [15:19<48:21,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  13%|█▎        | 134/1000 [15:29<1:17:32,  5.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▎        | 135/1000 [15:37<1:28:22,  6.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▎        | 136/1000 [15:54<2:15:13,  9.39s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▎        | 137/1000 [16:00<1:58:29,  8.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 138/1000 [16:02<1:31:17,  6.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 139/1000 [16:05<1:19:33,  5.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 140/1000 [16:14<1:33:22,  6.51s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 141/1000 [16:20<1:29:49,  6.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 142/1000 [16:25<1:24:03,  5.88s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 143/1000 [16:25<59:25,  4.16s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 144/1000 [16:38<1:37:49,  6.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  14%|█▍        | 145/1000 [16:39<1:13:39,  5.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▍        | 146/1000 [16:46<1:21:57,  5.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▍        | 147/1000 [16:55<1:33:15,  6.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▍        | 148/1000 [16:57<1:14:17,  5.23s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▍        | 149/1000 [17:07<1:33:53,  6.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▌        | 150/1000 [17:07<1:06:29,  4.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▌        | 151/1000 [17:26<2:05:06,  8.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▌        | 152/1000 [17:35<2:07:36,  9.03s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▌        | 153/1000 [17:40<1:51:56,  7.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  15%|█▌        | 154/1000 [17:44<1:34:22,  6.69s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 155/1000 [17:59<2:07:30,  9.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 156/1000 [18:02<1:43:28,  7.36s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 157/1000 [18:04<1:21:57,  5.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 158/1000 [18:07<1:08:47,  4.90s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 159/1000 [18:14<1:15:58,  5.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 160/1000 [18:22<1:26:24,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 161/1000 [18:23<1:07:12,  4.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▌        | 162/1000 [18:34<1:31:46,  6.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▋        | 163/1000 [18:42<1:37:54,  7.02s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▋        | 164/1000 [18:42<1:08:58,  4.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  16%|█▋        | 165/1000 [19:13<2:58:33, 12.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 166/1000 [19:19<2:29:16, 10.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 167/1000 [19:28<2:20:08, 10.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 168/1000 [19:36<2:13:56,  9.66s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 169/1000 [19:45<2:09:08,  9.32s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 170/1000 [19:57<2:19:07, 10.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 171/1000 [20:06<2:14:14,  9.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 172/1000 [20:07<1:39:59,  7.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 173/1000 [20:15<1:40:15,  7.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  17%|█▋        | 174/1000 [20:19<1:28:04,  6.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 175/1000 [20:30<1:49:22,  7.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 176/1000 [20:40<1:56:09,  8.46s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 177/1000 [20:41<1:24:29,  6.16s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 178/1000 [20:48<1:29:32,  6.54s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 179/1000 [20:55<1:29:47,  6.56s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 180/1000 [21:06<1:48:02,  7.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 181/1000 [21:16<1:54:50,  8.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 182/1000 [21:24<1:52:52,  8.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 183/1000 [21:33<1:56:48,  8.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 184/1000 [21:43<2:04:11,  9.13s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  18%|█▊        | 185/1000 [21:44<1:27:54,  6.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▊        | 186/1000 [21:51<1:33:06,  6.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▊        | 187/1000 [21:58<1:32:50,  6.85s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 188/1000 [22:07<1:41:26,  7.50s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 189/1000 [22:16<1:47:30,  7.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 190/1000 [22:25<1:50:20,  8.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 191/1000 [22:33<1:48:52,  8.07s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 192/1000 [22:51<2:28:10, 11.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 193/1000 [22:51<1:44:50,  7.79s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  19%|█▉        | 194/1000 [22:54<1:25:33,  6.37s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|█▉        | 195/1000 [22:54<1:01:49,  4.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|█▉        | 196/1000 [23:00<1:06:55,  4.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|█▉        | 197/1000 [23:05<1:04:33,  4.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|█▉        | 198/1000 [23:11<1:09:40,  5.21s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|█▉        | 199/1000 [23:17<1:13:00,  5.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 200/1000 [23:20<1:03:09,  4.74s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 201/1000 [23:25<1:04:21,  4.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 202/1000 [23:34<1:20:22,  6.04s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 203/1000 [23:35<59:19,  4.47s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 204/1000 [23:37<50:44,  3.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  20%|██        | 205/1000 [23:43<1:01:16,  4.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 206/1000 [23:53<1:19:19,  5.99s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 207/1000 [23:59<1:20:51,  6.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 208/1000 [24:09<1:36:32,  7.31s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 209/1000 [24:10<1:09:14,  5.25s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 210/1000 [24:15<1:11:08,  5.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██        | 211/1000 [24:27<1:36:09,  7.31s/it]\u001b[A\u001b[A\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nr words per pos:  21%|██        | 212/1000 [24:30<1:19:34,  6.06s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██▏       | 213/1000 [24:33<1:08:04,  5.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  21%|██▏       | 214/1000 [24:39<1:09:47,  5.33s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 215/1000 [24:44<1:09:01,  5.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 216/1000 [24:47<1:01:08,  4.68s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 217/1000 [24:55<1:12:50,  5.58s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 218/1000 [25:04<1:23:57,  6.44s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 219/1000 [25:07<1:10:26,  5.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 220/1000 [25:23<1:51:59,  8.61s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 221/1000 [25:25<1:27:14,  6.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 222/1000 [25:32<1:28:53,  6.86s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 223/1000 [25:43<1:42:40,  7.93s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▏       | 224/1000 [25:46<1:23:39,  6.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  22%|██▎       | 225/1000 [25:51<1:20:52,  6.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 226/1000 [26:08<2:01:54,  9.45s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 227/1000 [26:11<1:33:55,  7.29s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 228/1000 [26:22<1:48:01,  8.40s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 229/1000 [26:35<2:09:08, 10.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 230/1000 [26:36<1:33:28,  7.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 231/1000 [26:41<1:24:50,  6.62s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 232/1000 [26:43<1:06:26,  5.19s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 233/1000 [26:54<1:28:23,  6.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  23%|██▎       | 234/1000 [26:58<1:17:43,  6.09s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▎       | 235/1000 [27:02<1:06:44,  5.24s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▎       | 236/1000 [27:11<1:22:22,  6.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▎       | 237/1000 [27:12<1:00:12,  4.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 238/1000 [27:21<1:19:33,  6.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 239/1000 [27:23<1:00:13,  4.75s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 240/1000 [27:27<59:54,  4.73s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 241/1000 [27:40<1:30:02,  7.12s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 242/1000 [27:45<1:21:12,  6.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 243/1000 [27:51<1:20:50,  6.41s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  24%|██▍       | 244/1000 [27:57<1:17:44,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▍       | 246/1000 [27:58<55:42,  4.43s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▍       | 247/1000 [28:11<1:31:19,  7.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▍       | 248/1000 [28:24<1:49:50,  8.76s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▍       | 249/1000 [28:24<1:17:16,  6.17s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▌       | 250/1000 [28:32<1:26:22,  6.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▌       | 251/1000 [28:33<1:01:17,  4.91s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▌       | 252/1000 [28:42<1:18:07,  6.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▌       | 253/1000 [28:47<1:14:16,  5.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  25%|██▌       | 254/1000 [28:53<1:14:14,  5.97s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 255/1000 [29:07<1:41:46,  8.20s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 256/1000 [29:18<1:52:16,  9.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 257/1000 [29:21<1:30:05,  7.28s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 258/1000 [29:24<1:14:14,  6.00s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 259/1000 [29:33<1:23:57,  6.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 260/1000 [29:33<1:00:42,  4.92s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 261/1000 [29:35<48:27,  3.93s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▌       | 262/1000 [29:37<41:14,  3.35s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▋       | 263/1000 [29:45<59:47,  4.87s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▋       | 264/1000 [29:48<52:44,  4.30s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  26%|██▋       | 265/1000 [30:02<1:27:36,  7.15s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 266/1000 [30:02<1:01:45,  5.05s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 267/1000 [30:10<1:10:50,  5.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 268/1000 [30:12<58:47,  4.82s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 269/1000 [30:13<42:17,  3.47s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 270/1000 [30:24<1:10:58,  5.83s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 271/1000 [30:25<54:52,  4.52s/it]  \u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 272/1000 [30:30<53:48,  4.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 273/1000 [30:42<1:21:26,  6.72s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  27%|██▋       | 274/1000 [30:45<1:10:40,  5.84s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 275/1000 [30:53<1:17:35,  6.42s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 276/1000 [31:07<1:42:58,  8.53s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 277/1000 [31:12<1:29:29,  7.43s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 278/1000 [31:17<1:22:02,  6.82s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 279/1000 [31:21<1:11:27,  5.95s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 280/1000 [31:30<1:21:42,  6.81s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 281/1000 [31:39<1:30:42,  7.57s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 282/1000 [31:49<1:38:47,  8.26s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 283/1000 [31:52<1:20:28,  6.73s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 284/1000 [31:57<1:14:51,  6.27s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  28%|██▊       | 285/1000 [32:02<1:09:04,  5.80s/it]\u001b[A\u001b[A\n",
      "\n",
      "nr words per pos:  29%|██▊       | 286/1000 [32:16<1:37:27,  8.19s/it]\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nr_claims_selected = 1000 #nr_claims\n",
    "\n",
    "for id in tqdm(range(nr_claims_selected), desc = 'nr words per pos'):\n",
    "    file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "    file.process_tf_idf_experiment(tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_db import dict_save_json\n",
    "from doc_results_db_utils import get_tf_idf_name\n",
    "from wiki_database import Text\n",
    "from doc_results_db import get_dict_from_n_gram\n",
    "from tqdm import tnrange, tqdm\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer\n",
    "class ClaimFile:\n",
    "    \"\"\"A sample Employee class\"\"\"\n",
    "    def __init__(self, id, path_dir_files):\n",
    "        self.path_claim = os.path.join(path_dir_files, str(id) + '.json')\n",
    "        if os.path.isfile(self.path_claim):\n",
    "            self.claim_dict = dict_load_json(self.path_claim)\n",
    "        else:\n",
    "            self.claim_dict = {}\n",
    "            self.claim_dict['claim'] = {}\n",
    "            self.claim_dict['claim']['1_gram'] = {}\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = None\n",
    "            self.claim_dict['claim']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "            self.claim_dict['title'] = {}\n",
    "            self.claim_dict['title']['1_gram'] = {}\n",
    "#             self.claim_dict['title']['1_gram']['nr_words'] = None\n",
    "#             self.claim_dict['title']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "#             self.claim_dict['title']['ids'] = {}\n",
    "            self.save_claim()\n",
    "\n",
    "    def process_claim(self, claim):\n",
    "        self.claim_dict['claim']['text'] = claim\n",
    "        self.save_claim()\n",
    "\n",
    "    def process_tags(self, tag_list, n_gram):\n",
    "#         self.claim_dict['claim'][str(n_gram) +'_gram'] = {}\n",
    "        if n_gram == 1:\n",
    "            self.claim_dict['claim'][str(n_gram) +'_gram']['tag_list'] = tag_list\n",
    "        else:\n",
    "            raise ValueError('written for n_gram == 1')\n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_tf_idf_experiment(self, tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf):\n",
    "        tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            \n",
    "            tag_list = [word.pos_ for word in doc]        \n",
    "#             claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "#             n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "            # === write tf-idf values === #\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "#             print(tag_list, tokenized_claim_list)\n",
    "            idx = 0\n",
    "            for i in range(len(tag_list)):\n",
    "                tag = tag_list[i]\n",
    "                word = tokenized_claim_list[i]\n",
    "            \n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                \n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "#                 print(len(dictionary))\n",
    "                if len(dictionary) < 2000:\n",
    "                    for id, tf_idf_value in dictionary.items():\n",
    "                        # === create dictionary if does not exist === #\n",
    "                        if str(id) not in self.claim_dict['title']['1_gram']:\n",
    "                            self.claim_dict['title']['1_gram'][str(id)] = {}\n",
    "                            title = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "\n",
    "                            doc = tf_idf_db.vocab.wiki_database.nlp(title)\n",
    "                            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                            n_grams_dict_title, nr_words_title = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                            self.claim_dict['title']['1_gram'][str(id)]['nr_words'] = nr_words_title\n",
    "\n",
    "                        if vocab.method_tokenization[0] not in self.claim_dict['title']['1_gram'][str(id)].keys():\n",
    "                            self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]] = {}\n",
    "                            if tf_idf_name not in self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]].keys():\n",
    "                                self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]][tf_idf_name] = get_empty_tag_dict()\n",
    "\n",
    "                        self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "                idx += 1\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "        \n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_nr_words_per_pos(self, tf_idf_db, tag_2_id_dict):\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            for key, count in n_grams_dict.items():\n",
    "                tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "\n",
    "    def save_claim(self):\n",
    "        with HiddenPrints():\n",
    "            dict_save_json(self.claim_dict, self.path_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 save list claim tags\n",
    "\n",
    "# step 2 create files in folder with claim id\n",
    "\n",
    "# step 3 iterate for every experiment through \n",
    "\n",
    "# step 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "experiment_nr = 37\n",
    "\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "claim_dict = {}\n",
    "\n",
    "# claim features\n",
    "claim_dict['claim'] = {}\n",
    "claim_dict['claim']['nr_words'] = sum(n_grams.values())\n",
    "claim_dict['claim']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "\n",
    "for key, count in n_grams.items():\n",
    "    tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "    pos_id = tag_2_id_dict[tag]\n",
    "    claim_dict['claim']['nr_words_per_pos'][pos_id] += count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-143-b597943da182>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-143-b597943da182>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    claim_dict['title']['ids'][id][vocab.method_tokenization[0]]['tf_idf_sum_per_pos_' + str(experiment_nr)][] += tf_idf_value\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# title features\n",
    "claim_dict['title'] = {}\n",
    "claim_dict['title']['ids'] = {}\n",
    "# get_empty_dict_claim()\n",
    "# tf idf per tag\n",
    "experiment_nr = 31\n",
    "vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "# === initialise databases === #\n",
    "path_title_ids = tf_idf_db.path_ids_dict\n",
    "path_title_tf_idf = tf_idf_db.path_tf_idf_dict\n",
    "mydict_ids = SqliteDict(path_title_ids)\n",
    "mydict_tf_idf = SqliteDict(path_title_tf_idf)\n",
    "\n",
    "dictionary = get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "tag_nr = get_tag_2_id_dict()\n",
    "\n",
    "for id, tf_idf_value in dictionary.items():\n",
    "    if id in claim_dict['title']['ids'].keys():\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "    else:\n",
    "        claim_dict['title']['ids'][id] = get_empty_dict_claim()\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db):\n",
    "    \n",
    "    dictionary = {}\n",
    "\n",
    "    for word in n_grams:\n",
    "        try:\n",
    "            word_id_list = mydict_ids[word].split(tf_idf_db.delimiter)[1:]\n",
    "            word_tf_idf_list = mydict_tf_idf[word].split(tf_idf_db.delimiter)[1:]\n",
    "        except KeyError:\n",
    "            print('KeyError', word)\n",
    "            word_id_list = []\n",
    "            word_tf_idf_list = []\n",
    "        for j in range(len(word_id_list)):\n",
    "            id = int(word_id_list[j])       \n",
    "            tf_idf = float(word_tf_idf_list[j])\n",
    "            try:\n",
    "                dictionary[id] = dictionary[id] + tf_idf\n",
    "            except KeyError:\n",
    "                dictionary[id] = tf_idf\n",
    "    return dictionary\n",
    "\n",
    "def get_empty_dict_claim():\n",
    "    empty_dict = {}\n",
    "    empty_dict['nr_words_title'] = None\n",
    "    empty_dict['nr_words_per_pos'] = None\n",
    "    empty_dict['tokenize'] = {}\n",
    "    empty_dict['tokenize']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['tf_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['raw_count_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['idf'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['max_sum_idf'] = 0\n",
    "    empty_dict['tokenize_lemma'] = {}\n",
    "    empty_dict['tokenize_lemma']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['tf_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['max_sum_idf'] = 0\n",
    "    return empty_dict\n",
    "       \n",
    "def get_empty_tag_dict():\n",
    "    tag_2_id_dict = get_tag_2_id_dict()\n",
    "    empty_tag_dict = {}\n",
    "    for pos_id in range(len(tag_2_id_dict)):\n",
    "        empty_tag_dict[str(pos_id)] = 0\n",
    "    return empty_tag_dict\n",
    "\n",
    "def get_tag_2_id_dict():\n",
    "    tag_2_id_dict = {}\n",
    "    tag_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "\n",
    "    for i in range(len(tag_list)):\n",
    "        tag = tag_list[i]\n",
    "        tag_2_id_dict[tag] = i\n",
    "    return tag_2_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_tf_idf_from_exp(experiment_nr):\n",
    "    file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "    path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "\n",
    "    with open(path_experiment) as json_data_file:\n",
    "        data = json.load(json_data_file)\n",
    "\n",
    "    vocab = VocabularySqlite(wiki_database = wiki_database, n_gram = data['n_gram'],\n",
    "        method_tokenization = data['method_tokenization'], tags_in_db_flag = data['tags_in_db_flag'], \n",
    "        source = data['vocabulary_source'], tag_list_selected = data['tag_list_selected'])\n",
    "\n",
    "    tf_idf_db = TFIDFDatabaseSqlite(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "        delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "    return vocab, tf_idf_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "\n",
    "# print 'This will print'\n",
    "\n",
    "# blockPrint()\n",
    "# print \"This won't\"\n",
    "\n",
    "# enablePrint()\n",
    "# print \"This will too\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<1.1.0,>=0.28.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: jsonschema<3.1.0,>=2.6.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: preshed<2.1.0,>=2.0.1 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<2.1.0,>=2.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 92kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/6c/2ef2d6f4c63a197981f4ac01bb17560c857c6721213c7c99998e48cdda2a/srsly-0.0.7-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: idna<2.9,>=2.5 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: pyrsistent>=0.14.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Requirement already up-to-date: six>=1.11.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting setuptools (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: attrs>=17.4.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.2->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: wasabi, numpy, blis, tqdm, srsly, thinc, spacy, urllib3, certifi, setuptools\n",
      "  Found existing installation: numpy 1.16.3\n",
      "    Uninstalling numpy-1.16.3:\n",
      "      Successfully uninstalled numpy-1.16.3\n",
      "  Found existing installation: tqdm 4.32.1\n",
      "    Uninstalling tqdm-4.32.1:\n",
      "      Successfully uninstalled tqdm-4.32.1\n",
      "  Found existing installation: thinc 6.12.1\n",
      "    Uninstalling thinc-6.12.1:\n",
      "      Successfully uninstalled thinc-6.12.1\n",
      "  Found existing installation: spacy 2.0.18\n",
      "    Uninstalling spacy-2.0.18:\n",
      "      Successfully uninstalled spacy-2.0.18\n",
      "  Found existing installation: urllib3 1.25.2\n",
      "    Uninstalling urllib3-1.25.2:\n",
      "      Successfully uninstalled urllib3-1.25.2\n",
      "  Found existing installation: certifi 2019.3.9\n",
      "    Uninstalling certifi-2019.3.9:\n",
      "      Successfully uninstalled certifi-2019.3.9\n",
      "  Found existing installation: setuptools 39.0.1\n",
      "    Uninstalling setuptools-39.0.1:\n",
      "      Successfully uninstalled setuptools-39.0.1\n",
      "Successfully installed blis-0.2.4 certifi-2019.6.16 numpy-1.16.4 setuptools-41.0.1 spacy-2.1.4 srsly-0.0.7 thinc-7.0.4 tqdm-4.32.2 urllib3-1.25.3 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
