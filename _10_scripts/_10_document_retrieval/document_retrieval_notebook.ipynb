{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "from wiki_database import WikiDatabaseSqlite, Text\n",
    "from utils_doc_results_db import get_empty_tag_dict, get_tag_2_id_dict, get_tag_dict, get_tf_idf_from_exp, get_tf_idf_name, get_vocab_tf_idf_from_exp\n",
    "from utils_doc_results_db import get_dict_from_n_gram\n",
    "from utils_db import dict_load_json, dict_save_json, HiddenPrints, load_jsonl\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer, get_tag_word_from_wordtag, ClaimDatabase\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c', 'a', 'a', 'a', 'a', 'a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_tmp = ['c']\n",
    "for i in range(5):\n",
    "    list_tmp+=['a']\n",
    "list_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'NOT ENOUGH INFO'\n",
    "b = 'NOT ENOUGH INFO'\n",
    "a is not b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2, 3, 4, 5, 0]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K=5\n",
    "selected_ids = sorted(range(len(list_tmp)), key=lambda l: list_tmp[l])[-K:]\n",
    "selected_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database\n",
      "- claim database already exists\n"
     ]
    }
   ],
   "source": [
    "# === constants === #\n",
    "\n",
    "# === variables === #\n",
    "n_gram = 1\n",
    "claim_data_set = 'dev'\n",
    "folder_name_score_combination = 'score_combination'\n",
    "\n",
    "# === process === #\n",
    "path_dir_results = os.path.join(config.ROOT, config.RESULTS_DIR, folder_name_score_combination)\n",
    "path_tags = os.path.join(path_dir_results, 'tags_' + claim_data_set + '_n_gram_' + str(n_gram) + '.json')\n",
    "\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "path_claim_data_set = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR, claim_data_set + \".jsonl\")\n",
    "path_dir_claim_database = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "path_raw_data = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR)\n",
    "\n",
    "path_dir_claims = os.path.join(path_dir_results, claim_data_set)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_results, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_results)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_claims, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_claims)\n",
    "\n",
    "claim_database = ClaimDatabase(path_dir_database = path_dir_claim_database, path_raw_data = path_raw_data, claim_data_set = claim_data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 4\n",
    "claim = claim_database.get_claim_from_id(id)\n",
    "document_name_list = claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello', 'asdf']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'hello\\kasdf'.split('\\k')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimFile:\n",
    "    \"\"\"A sample Employee class\"\"\"\n",
    "    def __init__(self, id, path_dir_files):\n",
    "        self.path_claim = os.path.join(path_dir_files, str(id) + '.json')\n",
    "        if os.path.isfile(self.path_claim):\n",
    "            self.claim_dict = dict_load_json(self.path_claim)\n",
    "        else:\n",
    "            self.claim_dict = {}\n",
    "            self.claim_dict['claim'] = {}\n",
    "            self.claim_dict['claim']['1_gram'] = {}\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = None\n",
    "            self.claim_dict['claim']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "            self.claim_dict['title'] = {}\n",
    "            self.claim_dict['title']['1_gram'] = {}\n",
    "#             self.claim_dict['title']['1_gram']['nr_words'] = None\n",
    "#             self.claim_dict['title']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "#             self.claim_dict['title']['ids'] = {}\n",
    "            self.save_claim()\n",
    "    \n",
    "    def process_claims_selected(self, claim_dictionary):\n",
    "        # add ids to selected dictionary which are the proof\n",
    "        claim = Claim(claim_dictionary)\n",
    "        if 'ids_selected' not in self.claim_dict:\n",
    "            interpreter_list = claim.evidence\n",
    "            id_list = []\n",
    "            for interpreter in interpreter_list:\n",
    "                for proof in interpreter:\n",
    "                    title = proof[2]\n",
    "                    if title is not None:\n",
    "                        id = wiki_database.get_id_from_title(title)\n",
    "                        id_list.append(id)\n",
    "            self.claim_dict['ids_selected'] = id_list\n",
    "        \n",
    "        # === add from selected_ids in claim_dictionary === #    \n",
    "        if 'docs_selected' in claim_dictionary:\n",
    "            self.claim_dict['ids_selected'] += claim['docs_selected']\n",
    "            \n",
    "        # === save === #\n",
    "        self.save_claim()\n",
    "        \n",
    "    def process_claim(self, claim):\n",
    "        self.claim_dict['claim']['text'] = claim\n",
    "        self.save_claim()\n",
    "\n",
    "    def process_tags(self, tag_list, n_gram):\n",
    "#         self.claim_dict['claim'][str(n_gram) +'_gram'] = {}\n",
    "        if n_gram == 1:\n",
    "            self.claim_dict['claim'][str(n_gram) +'_gram']['tag_list'] = tag_list\n",
    "        else:\n",
    "            raise ValueError('written for n_gram == 1')\n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_tf_idf_experiment(self, tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf):\n",
    "        tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            \n",
    "            tag_list = [word.pos_ for word in doc]        \n",
    "            # === write tf-idf values === #\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "#             idx = 0\n",
    "            for i in range(len(tag_list)):\n",
    "                tag = tag_list[i]\n",
    "                word = tokenized_claim_list[i]\n",
    "            \n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                \n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "                    \n",
    "                for id in self.claim_dict['ids_selected']:\n",
    "                    # === create dictionary if does not exist === #\n",
    "                    if str(id) not in self.claim_dict['title']['1_gram']:\n",
    "                        self.claim_dict['title']['1_gram'][str(id)] = {}\n",
    "                        title = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "\n",
    "                        doc = tf_idf_db.vocab.wiki_database.nlp(title)\n",
    "                        claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                        n_grams_dict_title, nr_words_title = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                        self.claim_dict['title']['1_gram'][str(id)]['nr_words'] = nr_words_title\n",
    "                    \n",
    "                    if tf_idf_db.vocab.method_tokenization[0] not in self.claim_dict['title']['1_gram'][str(id)].keys():\n",
    "                        self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]] = {}\n",
    "                        if tf_idf_name not in self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]].keys():\n",
    "                            self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name] = get_empty_tag_dict()\n",
    "                    \n",
    "                    if id in dictionary:\n",
    "                        tf_idf_value = dictionary[id]\n",
    "                    else:\n",
    "                        tf_idf_value = 0.0\n",
    "                        \n",
    "                    self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "                    \n",
    "#                 for id, tf_idf_value in dictionary.items():\n",
    "#                     if id in self.claim_dict['ids_selected']:\n",
    "#                         # === create dictionary if does not exist === #\n",
    "#                         if str(id) not in self.claim_dict['title']['1_gram']:\n",
    "#                             self.claim_dict['title']['1_gram'][str(id)] = {}\n",
    "#                             title = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "\n",
    "#                             doc = tf_idf_db.vocab.wiki_database.nlp(title)\n",
    "#                             claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "#                             n_grams_dict_title, nr_words_title = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "#                             self.claim_dict['title']['1_gram'][str(id)]['nr_words'] = nr_words_title\n",
    "\n",
    "#                         if tf_idf_db.vocab.method_tokenization[0] not in self.claim_dict['title']['1_gram'][str(id)].keys():\n",
    "#                             self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]] = {}\n",
    "#                             if tf_idf_name not in self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]].keys():\n",
    "#                                 self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name] = get_empty_tag_dict()\n",
    "\n",
    "#                         self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "#                 idx += 1\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "        \n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_nr_words_per_pos(self, tf_idf_db, tag_2_id_dict):\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            for key, count in n_grams_dict.items():\n",
    "                tag, word = get_tag_word_from_wordtag(key, tf_idf_db.vocab.delimiter_tag_word)\n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "\n",
    "    def save_claim(self):\n",
    "        with HiddenPrints():\n",
    "            dict_save_json(self.claim_dict, self.path_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database\n",
      "- claim database already exists\n",
      "wiki_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "tag:   0%|          | 0/19998 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   0%|          | 41/19998 [00:00<00:49, 406.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags file already exists\n",
      "claim database: insert claim's text and claim's tag_list\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "tag:   0%|          | 77/19998 [00:00<00:51, 387.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|          | 114/19998 [00:00<00:52, 382.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|          | 150/19998 [00:00<00:53, 373.49it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|          | 187/19998 [00:00<00:53, 369.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|          | 223/19998 [00:00<00:53, 366.28it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|▏         | 261/19998 [00:00<00:53, 367.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   1%|▏         | 298/19998 [00:00<00:53, 365.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   2%|▏         | 336/19998 [00:00<00:53, 368.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   2%|▏         | 372/19998 [00:01<00:54, 360.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   2%|▏         | 409/19998 [00:01<00:54, 360.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   2%|▏         | 446/19998 [00:01<00:54, 359.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   2%|▏         | 483/19998 [00:01<00:53, 361.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   3%|▎         | 519/19998 [00:01<00:54, 357.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   3%|▎         | 555/19998 [00:01<00:54, 355.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   3%|▎         | 591/19998 [00:01<00:54, 355.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   3%|▎         | 628/19998 [00:01<00:54, 357.95it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   3%|▎         | 667/19998 [00:01<00:53, 362.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▎         | 705/19998 [00:01<00:52, 364.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▎         | 743/19998 [00:02<00:52, 368.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▍         | 781/19998 [00:02<00:52, 369.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▍         | 818/19998 [00:02<00:51, 369.29it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▍         | 855/19998 [00:02<00:51, 369.09it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   4%|▍         | 892/19998 [00:02<00:52, 367.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   5%|▍         | 929/19998 [00:02<00:52, 362.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   5%|▍         | 966/19998 [00:02<00:52, 359.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag:   5%|▌         | 1023/19998 [00:02<00:47, 402.62it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "tag: 100%|██████████| 19998/19998 [00:02<00:00, 7066.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database: insert nr words per tag for claim\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   0%|          | 0/1000 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   1%|▏         | 13/1000 [00:00<00:08, 119.33it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   2%|▎         | 25/1000 [00:00<00:08, 119.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   4%|▎         | 37/1000 [00:00<00:08, 118.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   5%|▍         | 49/1000 [00:00<00:08, 117.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   6%|▌         | 60/1000 [00:00<00:08, 114.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   7%|▋         | 71/1000 [00:00<00:08, 113.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   8%|▊         | 84/1000 [00:00<00:07, 117.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  10%|█         | 101/1000 [00:00<00:06, 129.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  12%|█▏        | 117/1000 [00:00<00:06, 136.04it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  14%|█▎        | 136/1000 [00:01<00:05, 147.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  15%|█▌        | 151/1000 [00:01<00:06, 140.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  17%|█▋        | 166/1000 [00:01<00:06, 136.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  18%|█▊        | 180/1000 [00:01<00:05, 136.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  19%|█▉        | 194/1000 [00:01<00:05, 137.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  21%|██        | 208/1000 [00:01<00:05, 136.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  22%|██▏       | 224/1000 [00:01<00:05, 142.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  24%|██▍       | 239/1000 [00:01<00:05, 142.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  25%|██▌       | 254/1000 [00:01<00:05, 135.70it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  27%|██▋       | 268/1000 [00:02<00:05, 128.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  28%|██▊       | 281/1000 [00:02<00:05, 121.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  29%|██▉       | 294/1000 [00:02<00:05, 120.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  31%|███       | 307/1000 [00:02<00:05, 119.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  32%|███▏      | 320/1000 [00:02<00:05, 116.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  33%|███▎      | 332/1000 [00:02<00:05, 115.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  34%|███▍      | 344/1000 [00:02<00:05, 113.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  36%|███▌      | 356/1000 [00:02<00:05, 111.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  37%|███▋      | 368/1000 [00:02<00:05, 110.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  38%|███▊      | 380/1000 [00:03<00:05, 105.08it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  39%|███▉      | 392/1000 [00:03<00:05, 107.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  40%|████      | 404/1000 [00:03<00:05, 109.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  42%|████▏     | 417/1000 [00:03<00:05, 112.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  43%|████▎     | 429/1000 [00:03<00:05, 112.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  44%|████▍     | 441/1000 [00:03<00:04, 112.76it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  45%|████▌     | 454/1000 [00:03<00:04, 116.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  47%|████▋     | 466/1000 [00:03<00:04, 113.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  48%|████▊     | 478/1000 [00:03<00:04, 113.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  49%|████▉     | 490/1000 [00:04<00:04, 115.06it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  50%|█████     | 503/1000 [00:04<00:04, 117.47it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  52%|█████▏    | 515/1000 [00:04<00:04, 118.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  53%|█████▎    | 527/1000 [00:04<00:04, 116.63it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  54%|█████▍    | 539/1000 [00:04<00:03, 117.03it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  55%|█████▌    | 551/1000 [00:04<00:03, 115.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  56%|█████▋    | 563/1000 [00:04<00:03, 115.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  57%|█████▊    | 575/1000 [00:04<00:03, 115.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  59%|█████▉    | 588/1000 [00:04<00:03, 118.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  60%|██████    | 601/1000 [00:04<00:03, 121.05it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  61%|██████▏   | 614/1000 [00:05<00:03, 118.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  63%|██████▎   | 626/1000 [00:05<00:03, 116.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  64%|██████▍   | 638/1000 [00:05<00:03, 114.78it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  65%|██████▌   | 650/1000 [00:05<00:03, 114.15it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  66%|██████▌   | 662/1000 [00:05<00:02, 115.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  67%|██████▋   | 674/1000 [00:05<00:02, 116.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  69%|██████▊   | 686/1000 [00:05<00:02, 116.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  70%|██████▉   | 698/1000 [00:05<00:02, 117.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  71%|███████   | 711/1000 [00:05<00:02, 119.26it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  72%|███████▏  | 724/1000 [00:05<00:02, 119.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  74%|███████▎  | 737/1000 [00:06<00:02, 121.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  75%|███████▌  | 752/1000 [00:06<00:01, 127.93it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  76%|███████▋  | 765/1000 [00:06<00:01, 124.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  78%|███████▊  | 779/1000 [00:06<00:01, 126.23it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  79%|███████▉  | 792/1000 [00:06<00:01, 123.07it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  80%|████████  | 805/1000 [00:06<00:01, 121.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  82%|████████▏ | 818/1000 [00:06<00:01, 120.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  83%|████████▎ | 831/1000 [00:06<00:01, 120.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  84%|████████▍ | 844/1000 [00:06<00:01, 120.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  86%|████████▌ | 858/1000 [00:07<00:01, 125.01it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  87%|████████▋ | 871/1000 [00:07<00:01, 121.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  88%|████████▊ | 884/1000 [00:07<00:00, 122.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  90%|████████▉ | 898/1000 [00:07<00:00, 126.54it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  91%|█████████ | 911/1000 [00:07<00:00, 124.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  92%|█████████▏| 924/1000 [00:07<00:00, 122.72it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  94%|█████████▎| 937/1000 [00:07<00:00, 123.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  95%|█████████▌| 950/1000 [00:07<00:00, 120.99it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  96%|█████████▋| 963/1000 [00:07<00:00, 120.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  98%|█████████▊| 976/1000 [00:08<00:00, 120.71it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  99%|█████████▉| 989/1000 [00:08<00:00, 120.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos: 100%|██████████| 1000/1000 [00:08<00:00, 121.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database: insert selected ids\n"
     ]
    }
   ],
   "source": [
    "# === constants === #\n",
    "\n",
    "# === variables === #\n",
    "n_gram = 1\n",
    "claim_data_set = 'dev'\n",
    "folder_name_score_combination = 'score_combination'\n",
    "\n",
    "# === process === #\n",
    "path_dir_results = os.path.join(config.ROOT, config.RESULTS_DIR, folder_name_score_combination)\n",
    "path_tags = os.path.join(path_dir_results, 'tags_' + claim_data_set + '_n_gram_' + str(n_gram) + '.json')\n",
    "\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "path_claim_data_set = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR, claim_data_set + \".jsonl\")\n",
    "path_dir_claim_database = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "path_raw_data = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR)\n",
    "\n",
    "path_dir_claims = os.path.join(path_dir_results, claim_data_set)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_results, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_results)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_claims, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_claims)\n",
    "\n",
    "claim_database = ClaimDatabase(path_dir_database = path_dir_claim_database, path_raw_data = path_raw_data, claim_data_set = claim_data_set)\n",
    "\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "tag_dict = get_tag_dict(claim_data_set, n_gram, path_tags, wiki_database)\n",
    "\n",
    "nr_claims = 1000 # len(results)\n",
    "\n",
    "print('claim database: insert claim\\'s text and claim\\'s tag_list')\n",
    "\n",
    "for str_id, tag_list in tqdm(tag_dict.items(), total = len(tag_dict), desc = 'tag'):\n",
    "    id = int(str_id)\n",
    "    if id < nr_claims:\n",
    "        file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "        file.process_tags(tag_list, n_gram)\n",
    "        claim_dict = claim_database.get_claim_from_id(id)\n",
    "        claim = Claim(claim_dict)\n",
    "        file.process_claim(claim.claim)\n",
    "        file.process_claims_selected(claim_dict)\n",
    "\n",
    "print('claim database: insert nr words per tag for claim')\n",
    "\n",
    "experiment_nr = 37\n",
    "with HiddenPrints():\n",
    "    tf_idf_db = get_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "for id in tqdm(range(nr_claims), desc = 'nr words per pos'):\n",
    "    file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "    file.process_nr_words_per_pos(tf_idf_db, tag_2_id_dict)\n",
    "\n",
    "print('claim database: insert selected ids')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: [31, 37]\n",
      "load tf_idf nr_words_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   1%|          | 1/100 [00:00<00:23,  4.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   4%|▍         | 4/100 [00:00<00:17,  5.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   6%|▌         | 6/100 [00:00<00:14,  6.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   9%|▉         | 9/100 [00:00<00:11,  8.27it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  11%|█         | 11/100 [00:00<00:08,  9.97it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  13%|█▎        | 13/100 [00:00<00:07, 11.43it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  16%|█▌        | 16/100 [00:01<00:06, 12.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  19%|█▉        | 19/100 [00:01<00:05, 14.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  22%|██▏       | 22/100 [00:01<00:04, 16.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  25%|██▌       | 25/100 [00:01<00:04, 18.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  28%|██▊       | 28/100 [00:01<00:03, 20.50it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  31%|███       | 31/100 [00:01<00:03, 18.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  34%|███▍      | 34/100 [00:01<00:03, 18.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  37%|███▋      | 37/100 [00:02<00:03, 18.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  39%|███▉      | 39/100 [00:02<00:03, 18.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  41%|████      | 41/100 [00:02<00:03, 17.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  43%|████▎     | 43/100 [00:02<00:03, 17.90it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  45%|████▌     | 45/100 [00:02<00:03, 14.12it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  48%|████▊     | 48/100 [00:02<00:03, 15.79it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  50%|█████     | 50/100 [00:02<00:03, 16.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  52%|█████▏    | 52/100 [00:03<00:03, 15.85it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  55%|█████▌    | 55/100 [00:03<00:02, 17.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  57%|█████▋    | 57/100 [00:03<00:02, 16.02it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  59%|█████▉    | 59/100 [00:03<00:02, 16.56it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  61%|██████    | 61/100 [00:03<00:02, 16.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  63%|██████▎   | 63/100 [00:03<00:02, 16.86it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  65%|██████▌   | 65/100 [00:03<00:02, 15.82it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  67%|██████▋   | 67/100 [00:04<00:02, 11.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  70%|███████   | 70/100 [00:04<00:02, 13.73it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  73%|███████▎  | 73/100 [00:04<00:01, 15.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  75%|███████▌  | 75/100 [00:04<00:01, 16.67it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  77%|███████▋  | 77/100 [00:04<00:01, 15.36it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  79%|███████▉  | 79/100 [00:04<00:01, 16.35it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  81%|████████  | 81/100 [00:04<00:01, 13.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  83%|████████▎ | 83/100 [00:05<00:01, 12.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  85%|████████▌ | 85/100 [00:05<00:01, 11.57it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  87%|████████▋ | 87/100 [00:05<00:01, 12.83it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  89%|████████▉ | 89/100 [00:05<00:00, 13.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  91%|█████████ | 91/100 [00:05<00:00, 13.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  93%|█████████▎| 93/100 [00:05<00:00, 14.16it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  96%|█████████▌| 96/100 [00:05<00:00, 15.81it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  98%|█████████▊| 98/100 [00:06<00:00, 15.44it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos: 100%|██████████| 100/100 [00:06<00:00, 15.61it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "experiment: [31, 37]\n",
      "load tf_idf nr_words_pos\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   0%|          | 0/100 [00:00<?, ?it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   1%|          | 1/100 [00:00<00:19,  5.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   4%|▍         | 4/100 [00:00<00:14,  6.41it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   6%|▌         | 6/100 [00:00<00:12,  7.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:   9%|▉         | 9/100 [00:00<00:09,  9.39it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  11%|█         | 11/100 [00:00<00:08, 10.92it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  13%|█▎        | 13/100 [00:00<00:07, 12.32it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  16%|█▌        | 16/100 [00:01<00:05, 14.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  19%|█▉        | 19/100 [00:01<00:05, 16.10it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  22%|██▏       | 22/100 [00:01<00:04, 16.68it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  25%|██▌       | 25/100 [00:01<00:04, 18.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  28%|██▊       | 28/100 [00:01<00:03, 20.14it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  31%|███       | 31/100 [00:01<00:03, 17.59it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  33%|███▎      | 33/100 [00:01<00:03, 17.87it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  35%|███▌      | 35/100 [00:02<00:03, 17.48it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  37%|███▋      | 37/100 [00:02<00:03, 16.20it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  39%|███▉      | 39/100 [00:02<00:03, 15.60it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  41%|████      | 41/100 [00:02<00:05, 11.64it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  43%|████▎     | 43/100 [00:02<00:04, 11.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  45%|████▌     | 45/100 [00:02<00:04, 11.46it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  48%|████▊     | 48/100 [00:03<00:04, 12.94it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  50%|█████     | 50/100 [00:03<00:03, 14.25it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  52%|█████▏    | 52/100 [00:03<00:03, 13.31it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  54%|█████▍    | 54/100 [00:03<00:03, 14.52it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  56%|█████▌    | 56/100 [00:03<00:03, 14.11it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  59%|█████▉    | 59/100 [00:03<00:02, 15.65it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  61%|██████    | 61/100 [00:03<00:02, 16.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  63%|██████▎   | 63/100 [00:04<00:02, 13.22it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  65%|██████▌   | 65/100 [00:04<00:02, 12.17it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  67%|██████▋   | 67/100 [00:04<00:03, 10.21it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  70%|███████   | 70/100 [00:04<00:02, 12.13it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  72%|███████▏  | 72/100 [00:04<00:02, 12.77it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  74%|███████▍  | 74/100 [00:04<00:01, 13.96it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  77%|███████▋  | 77/100 [00:05<00:01, 16.42it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  79%|███████▉  | 79/100 [00:05<00:01, 14.24it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  81%|████████  | 81/100 [00:05<00:01, 10.74it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  83%|████████▎ | 83/100 [00:05<00:01, 10.45it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  85%|████████▌ | 85/100 [00:05<00:01,  9.51it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  87%|████████▋ | 87/100 [00:06<00:01,  9.55it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  89%|████████▉ | 89/100 [00:06<00:01, 10.66it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  91%|█████████ | 91/100 [00:06<00:00, 11.30it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  93%|█████████▎| 93/100 [00:06<00:00, 12.53it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  96%|█████████▌| 96/100 [00:06<00:00, 14.37it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos:  98%|█████████▊| 98/100 [00:06<00:00, 14.00it/s]\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "nr words per pos: 100%|██████████| 100/100 [00:07<00:00, 14.58it/s]\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "from sqlitedict import SqliteDict\n",
    "from utils_doc_results_db import get_tf_idf_from_exp\n",
    "from utils_db import HiddenPrints\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === run experiment === #\n",
    "experiment_list = [31, 37]\n",
    "\n",
    "nr_claims_selected = 100 #nr_claims\n",
    "\n",
    "for experiment_nr in experiment_list:\n",
    "    print('experiment:', experient_nr)\n",
    "    print('load tf_idf nr_words_pos')\n",
    "    with HiddenPrints():\n",
    "        tf_idf_db = get_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "    mydict_ids = SqliteDict(tf_idf_db.path_ids_dict)\n",
    "    mydict_tf_idf = SqliteDict(tf_idf_db.path_tf_idf_dict)\n",
    "\n",
    "    for id in tqdm(range(nr_claims_selected), desc = 'nr words per pos'):\n",
    "        file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "        file.process_tf_idf_experiment(tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def get_list_properties(dict, temp_key_list, list_keys_list, list_values):\n",
    "    for key, value in dict.items():\n",
    "        if isinstance(value, collections.Mapping):\n",
    "            list_keys_list, list_values = get_list_properties(dict[key], temp_key_list + [key], list_keys_list, list_values)\n",
    "        else:\n",
    "            if type(value) is not str and type(value) is not list:\n",
    "                list_values.append(value)\n",
    "                list_keys_list.append(temp_key_list + [key])\n",
    "    return list_keys_list, list_values\n",
    "\n",
    "def get_value_if_exists(dict, list_keys):\n",
    "    tmp = dict\n",
    "    for key in list_keys:\n",
    "        try:\n",
    "            tmp = tmp[key]\n",
    "        except KeyError:\n",
    "            return 0\n",
    "    if isinstance(tmp, collections.Mapping):\n",
    "        raise ValueError('Get a Dictionary whereas we expect a value')\n",
    "    value = tmp\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "id = 5\n",
    "\n",
    "file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "\n",
    "id_list = list(file.claim_dict['title']['1_gram'].keys())\n",
    "\n",
    "observation_key_list_claim, _ = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "observation_key_list_title, _ = get_list_properties(file.claim_dict['title']['1_gram'][id_list[0]], [], [], [])\n",
    "\n",
    "nr_claims = 10\n",
    "nr_variables = len(observation_key_list_claim) + len(observation_key_list_title)\n",
    "\n",
    "data_matrix = np.zeros((nr_claims, nr_variables))\n",
    "\n",
    "for id in range(nr_claims):\n",
    "    _, values_claim = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "    _, values_title = get_list_properties(file.claim_dict['title']['1_gram'][id_list[0]], [], [], [])\n",
    "    data_matrix[id, :] = values_claim + values_title\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "data_matrix = np.array([1,2,3,4,5])\n",
    "data_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=576, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 6 * 6, 120)  # 6*6 from image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "\n",
    "net = Net()\n",
    "print(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 3, 3])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0270,  0.0295, -0.0137,  0.1104, -0.1436, -0.0219, -0.0160, -0.0985,\n",
      "          0.0642, -0.0511]], grad_fn=<ThAddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 1, 32, 32)\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4623, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "target = torch.randn(10)  # a dummy target, for example\n",
    "target = target.view(1, -1)  # make it the same shape as output\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "\n",
    "        D_in: input dimension\n",
    "        H: dimension of hidden layer\n",
    "        D_out: output dimension\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(D_in, H) \n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Variable of input data and we must \n",
    "        return a Variable of output data. We can use Modules defined in the \n",
    "        constructor as well as arbitrary operators on Variables.\n",
    "        \"\"\"\n",
    "        h_relu = F.relu(self.linear1(x))\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is the dimension of the hidden layer; D_out is output dimension.\n",
    "N, D_in, H, D_out = 32, 100, 50, 10\n",
    "\n",
    "# Create random Tensors to hold inputs and outputs, and wrap them in Variables\n",
    "x = Variable(torch.randn(N, D_in))  # dim: 32 x 100\n",
    "\n",
    "# Construct our model by instantiating the class defined above\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# Forward pass: Compute predicted y by passing x to the model\n",
    "y_pred = model(x)   # dim: 32 x 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, list_IDs, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list_IDs\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = torch.load('data/' + ID + '.pt')\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "from my_classes import Dataset\n",
    "\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "cudnn.benchmark = True\n",
    "\n",
    "# Parameters\n",
    "params = {'batch_size': 64,\n",
    "          'shuffle': True,\n",
    "          'num_workers': 6}\n",
    "max_epochs = 100\n",
    "\n",
    "# Datasets\n",
    "partition = # IDs\n",
    "labels = # Labels\n",
    "\n",
    "# Generators\n",
    "training_set = Dataset(partition['train'], labels)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "\n",
    "validation_set = Dataset(partition['validation'], labels)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "\n",
    "# Loop over epochs\n",
    "for epoch in range(max_epochs):\n",
    "    # Training\n",
    "    for local_batch, local_labels in training_generator:\n",
    "        # Transfer to GPU\n",
    "        local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        # Model computations\n",
    "        [...]\n",
    "\n",
    "    # Validation\n",
    "    with torch.set_grad_enabled(False):\n",
    "        for local_batch, local_labels in validation_generator:\n",
    "            # Transfer to GPU\n",
    "            local_batch, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "            # Model computations\n",
    "            [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[127089, 141573, 'Andrew_Kevin_Walker', 0]]]\n"
     ]
    }
   ],
   "source": [
    "from utils_db import load_jsonl\n",
    "\n",
    "path_results = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp/thr_0.01_/ex_term_frequency_inverse_document_frequency_title/01_results/predicted_labels_20.json'\n",
    "dict_results = load_jsonl(path_results)\n",
    "claim_nr = 9\n",
    "interpreter_list = dict_results[claim_nr]['evidence']\n",
    "print(interpreter_list)\n",
    "\n",
    "id_list = []\n",
    "for interpreter in interpreter_list:\n",
    "    for proof in interpreter:\n",
    "        title = proof[2]\n",
    "        if title is not None:\n",
    "            id = wiki_database.get_id_from_title(title)\n",
    "            id_list.append(id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 108281,\n",
       " 'verifiable': 'VERIFIABLE',\n",
       " 'label': 'REFUTES',\n",
       " 'claim': 'Andrew Kevin Walker is only Chinese.',\n",
       " 'evidence': [[[127089, 141573, 'Andrew_Kevin_Walker', 0]]],\n",
       " 'docs_selected': [3680518,\n",
       "  3680749,\n",
       "  3682302,\n",
       "  4041240,\n",
       "  4323813,\n",
       "  4548726,\n",
       "  4566445,\n",
       "  4617659,\n",
       "  4724012,\n",
       "  2611161,\n",
       "  2703062,\n",
       "  531639,\n",
       "  480771,\n",
       "  484565,\n",
       "  465538,\n",
       "  2666840,\n",
       "  2654995,\n",
       "  5182741,\n",
       "  1001990,\n",
       "  3614807],\n",
       " 'results': {'e_score': 1.0, 'f_score': 1, 'f_score_labelled': 1}}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_db import load_jsonl\n",
    "\n",
    "path_results = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp/thr_0.01_/ex_term_frequency_inverse_document_frequency_title/01_results/predicted_labels_20.json'\n",
    "dict_results = load_jsonl(path_results)\n",
    "claim_nr = 9\n",
    "interpreter_list = dict_results[claim_nr]\n",
    "interpreter_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overwriting file: tmp.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_db import dict_save_json, dict_load_json\n",
    "path_dict = 'tmp.json'\n",
    "dict = {}\n",
    "dict['Simón_Bolívar'] = 1\n",
    "dict_save_json(dict, path_dict)\n",
    "dict['Simón_Bolívar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SqliteDict('tmp.sqlite') as dict_tmp:\n",
    "    dict_tmp['Simón_Bolívar'] = 1\n",
    "    dict_tmp.commit()\n",
    "with SqliteDict('tmp.sqlite') as dict_tmp:\n",
    "    print(dict_tmp['Simón_Bolívar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Simón_Bolívar'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import unicodedata\n",
    "unicodedata.normalize('NFD', 'Simón_Bolívar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_titles = list(wiki_database.title_2_id_db.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5416536it [01:13, 74033.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "id_list = []\n",
    "for key, value in tqdm(wiki_database.title_2_id_db.items()):\n",
    "    if 'n_Bol' in key:\n",
    "        id_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = 460\n",
    "unicodedata.normalize('NFD', id_list[460]) == unicodedata.normalize('NFD', 'Simón_Bolívar')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'St._Francis_de_Sales_High_School_-LRB-Detroit,_Michigan-RRB-', 'text': 'St. Francis de Sales High School was a coeducational Catholic high school in Detroit , Michigan . The school closed in 1971 . Since 1994 , the school building has been home to Loyola High School . ', 'lines': '0\\tSt. Francis de Sales High School was a coeducational Catholic high school in Detroit , Michigan .\\tDetroit\\tDetroit\\tMichigan\\tMichigan\\tCatholic\\tCatholic\\n1\\tThe school closed in 1971 .\\n2\\tSince 1994 , the school building has been home to Loyola High School .\\tLoyola High School\\tLoyola High School (Detroit)\\n3\\t'}\n",
      "{'id': 'St._Francis_de_Sales_High_School_-LRB-Detroit,_Michigan-RRB-', 'text': 'St. Francis de Sales High School was a coeducational Catholic high school in Detroit , Michigan . The school closed in 1971 . Since 1994 , the school building has been home to Loyola High School . ', 'lines': '0\\tSt. Francis de Sales High School was a coeducational Catholic high school in Detroit , Michigan .\\tDetroit\\tDetroit\\tMichigan\\tMichigan\\tCatholic\\tCatholic\\n1\\tThe school closed in 1971 .\\n2\\tSince 1994 , the school building has been home to Loyola High School .\\tLoyola High School\\tLoyola High School (Detroit)\\n3\\t'}\n"
     ]
    }
   ],
   "source": [
    "from utils_db import load_jsonl\n",
    "\n",
    "path_text = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_01_data/_02_wikipedia_pages/wiki-pages/wiki-090.jsonl'\n",
    "\n",
    "text_list = load_jsonl(path_text)\n",
    "id = 2001\n",
    "print(text_list[id])\n",
    "print(text_list[id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_db_simon = claim_db.get_claim_from_id(26)['evidence'][0][0][2]\n",
    "# wiki_db_simon = unicodedata.normalize('NFD', normalise_text(id_list[460]))\n",
    "# claim_db_simon == normalise_text(id_list[460])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simón Bolívar\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "with SqliteDict('tmp.sqlite') as dict_tmp:\n",
    "    dict_tmp[normalise_text(id_list[460])] = normalise_text(id_list[460])\n",
    "    dict_tmp.commit()\n",
    "with SqliteDict('tmp.sqlite') as dict_tmp:\n",
    "    print(dict_tmp[claim_db_simon])\n",
    "    print(dict_tmp[claim_db_simon] == claim_db_simon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5416536it [02:07, 42616.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "id_list = []\n",
    "for key, value in tqdm(wiki_database.title_2_id_db.items()):\n",
    "    if 'n_Bol' in key:\n",
    "        id_list.append(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dict = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/id_2_title.json'\n",
    "dict = dict_load_json(path_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1986 NBA Finals'"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict['1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database already exists\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75eb601e87fc43ceb291468ddb69ed10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1000), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import config \n",
    "\n",
    "claim_data_set = 'dev'\n",
    "path_dir_database = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "path_raw_data = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR)\n",
    "\n",
    "claim_db = ClaimDatabase(path_dir_database = path_dir_database, path_raw_data = path_raw_data, claim_data_set = claim_data_set)\n",
    "\n",
    "for i in tnrange(1000):\n",
    "    claim_db.get_claim_from_id(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unicodedata\n",
    "from utils_db import dict_load_json, dict_save_json, load_jsonl\n",
    "from tqdm import tqdm\n",
    "\n",
    "class ClaimDatabase:\n",
    "    def __init__(self, path_dir_database, path_raw_data, claim_data_set):\n",
    "        self.path_dir_database = path_dir_database\n",
    "        self.path_raw_data = path_raw_data\n",
    "        self.claim_data_set = claim_data_set\n",
    "        \n",
    "        self.path_dir_database_claims = os.path.join(self.path_dir_database, 'claims_' + str(self.claim_data_set))\n",
    "        self.path_raw_claims = os.path.join(path_raw_data, str(self.claim_data_set) + '.jsonl')\n",
    "        self.path_settings = os.path.join(path_dir_database_claims, 'settings.json')\n",
    "        \n",
    "        if not os.path.isdir(self.path_dir_database_claims):\n",
    "            print('create claim database')\n",
    "            os.makedirs(self.path_dir_database_claims)\n",
    "            self.create_database()\n",
    "        else:\n",
    "            print('claim database already exists')\n",
    "        \n",
    "        if os.path.isfile(self.path_settings):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "            self.nr_claims = settings['nr_claims']\n",
    "        else:\n",
    "            raise ValueError('settings file should exist')\n",
    "        \n",
    "        \n",
    "    def create_database(self):\n",
    "        list_claim_dicts = load_jsonl(self.path_raw_claims)\n",
    "        self.nr_claims = len(list_claim_dicts)\n",
    "        for id in tqdm(range(self.nr_claims)):\n",
    "            path_claim = os.path.join(self.path_dir_database_claims, str(id) + '.json')\n",
    "            dict_claim_id = list_claim_dicts[id]\n",
    "            dict_claim_id['verifiable'] = unicodedata.normalize('NFD', normalise_text(dict_claim_id['verifiable']))\n",
    "            dict_claim_id['claim'] = unicodedata.normalize('NFD', normalise_text(dict_claim_id['claim']))\n",
    "            for interpreter in range(len(dict_claim_id['evidence'])):\n",
    "                for proof in range(len(dict_claim_id['evidence'][interpreter])):\n",
    "                    if dict_claim_id['evidence'][interpreter][proof][2] != None:\n",
    "                        dict_claim_id['evidence'][interpreter][proof][2] = unicodedata.normalize('NFD', normalise_text(dict_claim_id['evidence'][interpreter][proof][2]))\n",
    "            \n",
    "            dict_save_json(dict_claim_id, path_claim)\n",
    "        \n",
    "        if os.path.isfile(self.path_settings):\n",
    "            settings = dict_load_json(self.path_settings)\n",
    "            \n",
    "        else:\n",
    "            settings = {}\n",
    "        \n",
    "        settings['nr_claims'] = self.nr_claims\n",
    "        dict_save_json(settings, self.path_settings)\n",
    "    \n",
    "    def get_claim_from_id(self, id):\n",
    "        path_claim = os.path.join(self.path_dir_database_claims, str(id) + '.json')\n",
    "        dict_claim_id = dict_load_json(path_claim)\n",
    "        return dict_claim_id\n",
    "    \n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'doc_results_db_utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-d79bc696e011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mutils_db\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdict_save_json\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdoc_results_db_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_tf_idf_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mwiki_database\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mdoc_results_db\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_dict_from_n_gram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtnrange\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'doc_results_db_utils'"
     ]
    }
   ],
   "source": [
    "from utils_db import dict_save_json\n",
    "from doc_results_db_utils import get_tf_idf_name\n",
    "from wiki_database import Text\n",
    "from doc_results_db import get_dict_from_n_gram\n",
    "from tqdm import tnrange, tqdm\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer\n",
    "\n",
    "class ClaimFile:\n",
    "    \"\"\"A sample Employee class\"\"\"\n",
    "    def __init__(self, id, path_dir_files):\n",
    "        self.path_claim = os.path.join(path_dir_files, str(id) + '.json')\n",
    "        if os.path.isfile(self.path_claim):\n",
    "            self.claim_dict = dict_load_json(self.path_claim)\n",
    "        else:\n",
    "            self.claim_dict = {}\n",
    "            self.claim_dict['claim'] = {}\n",
    "            self.claim_dict['claim']['1_gram'] = {}\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = None\n",
    "            self.claim_dict['claim']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "            self.claim_dict['title'] = {}\n",
    "            self.claim_dict['title']['1_gram'] = {}\n",
    "#             self.claim_dict['title']['1_gram']['nr_words'] = None\n",
    "#             self.claim_dict['title']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "#             self.claim_dict['title']['ids'] = {}\n",
    "            self.save_claim()\n",
    "    \n",
    "    def process_claims_selected(self, claim_dictionary):\n",
    "        # add ids to selected dictionary which are the proof\n",
    "        if 'ids_selected' not in self.claim_dict:\n",
    "            interpreter_list = claim_dictionary.evidence\n",
    "            id_list = []\n",
    "            for interpreter in interpreter_list:\n",
    "                for proof in interpreter:\n",
    "                    title = proof[2]\n",
    "                    if title is not None:\n",
    "                        id = wiki_database.get_id_from_title(title)\n",
    "                        id_list.append(id)\n",
    "            self.claim_dict['ids_selected'] = id_list\n",
    "        \n",
    "        # === add from selected_ids in claim_dictionary === #    \n",
    "        if 'docs_selected' in claim_dictionary:\n",
    "            claim = Claim(claim_dictionary)\n",
    "            self.claim_dict['ids_selected'] += claim['docs_selected']\n",
    "            \n",
    "        # === save === #\n",
    "        self.save_claim()\n",
    "        \n",
    "    def process_claim(self, claim):\n",
    "        self.claim_dict['claim']['text'] = claim\n",
    "        self.save_claim()\n",
    "\n",
    "    def process_tags(self, tag_list, n_gram):\n",
    "#         self.claim_dict['claim'][str(n_gram) +'_gram'] = {}\n",
    "        if n_gram == 1:\n",
    "            self.claim_dict['claim'][str(n_gram) +'_gram']['tag_list'] = tag_list\n",
    "        else:\n",
    "            raise ValueError('written for n_gram == 1')\n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_tf_idf_experiment(self, tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf):\n",
    "        tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            \n",
    "            tag_list = [word.pos_ for word in doc]        \n",
    "#             claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "#             n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "            # === write tf-idf values === #\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "#             print(tag_list, tokenized_claim_list)\n",
    "            idx = 0\n",
    "            for i in range(len(tag_list)):\n",
    "                tag = tag_list[i]\n",
    "                word = tokenized_claim_list[i]\n",
    "            \n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                \n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "#                 print(len(dictionary))\n",
    "                if len(dictionary) < 2000:\n",
    "                    for id, tf_idf_value in dictionary.items():\n",
    "                        # === create dictionary if does not exist === #\n",
    "                        if str(id) not in self.claim_dict['title']['1_gram']:\n",
    "                            self.claim_dict['title']['1_gram'][str(id)] = {}\n",
    "                            title = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "\n",
    "                            doc = tf_idf_db.vocab.wiki_database.nlp(title)\n",
    "                            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                            n_grams_dict_title, nr_words_title = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                            self.claim_dict['title']['1_gram'][str(id)]['nr_words'] = nr_words_title\n",
    "\n",
    "                        if tf_idf_db.vocab.method_tokenization[0] not in self.claim_dict['title']['1_gram'][str(id)].keys():\n",
    "                            self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]] = {}\n",
    "                            if tf_idf_name not in self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]].keys():\n",
    "                                self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name] = get_empty_tag_dict()\n",
    "\n",
    "                        self.claim_dict['title']['1_gram'][str(id)][tf_idf_db.vocab.method_tokenization[0]][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "                idx += 1\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "        \n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_nr_words_per_pos(self, tf_idf_db, tag_2_id_dict):\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            for key, count in n_grams_dict.items():\n",
    "                tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "\n",
    "    def save_claim(self):\n",
    "        with HiddenPrints():\n",
    "            dict_save_json(self.claim_dict, self.path_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<1.1.0,>=0.28.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: jsonschema<3.1.0,>=2.6.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: preshed<2.1.0,>=2.0.1 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<2.1.0,>=2.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 92kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/6c/2ef2d6f4c63a197981f4ac01bb17560c857c6721213c7c99998e48cdda2a/srsly-0.0.7-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: idna<2.9,>=2.5 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: pyrsistent>=0.14.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Requirement already up-to-date: six>=1.11.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting setuptools (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: attrs>=17.4.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.2->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: wasabi, numpy, blis, tqdm, srsly, thinc, spacy, urllib3, certifi, setuptools\n",
      "  Found existing installation: numpy 1.16.3\n",
      "    Uninstalling numpy-1.16.3:\n",
      "      Successfully uninstalled numpy-1.16.3\n",
      "  Found existing installation: tqdm 4.32.1\n",
      "    Uninstalling tqdm-4.32.1:\n",
      "      Successfully uninstalled tqdm-4.32.1\n",
      "  Found existing installation: thinc 6.12.1\n",
      "    Uninstalling thinc-6.12.1:\n",
      "      Successfully uninstalled thinc-6.12.1\n",
      "  Found existing installation: spacy 2.0.18\n",
      "    Uninstalling spacy-2.0.18:\n",
      "      Successfully uninstalled spacy-2.0.18\n",
      "  Found existing installation: urllib3 1.25.2\n",
      "    Uninstalling urllib3-1.25.2:\n",
      "      Successfully uninstalled urllib3-1.25.2\n",
      "  Found existing installation: certifi 2019.3.9\n",
      "    Uninstalling certifi-2019.3.9:\n",
      "      Successfully uninstalled certifi-2019.3.9\n",
      "  Found existing installation: setuptools 39.0.1\n",
      "    Uninstalling setuptools-39.0.1:\n",
      "      Successfully uninstalled setuptools-39.0.1\n",
      "Successfully installed blis-0.2.4 certifi-2019.6.16 numpy-1.16.4 setuptools-41.0.1 spacy-2.1.4 srsly-0.0.7 thinc-7.0.4 tqdm-4.32.2 urllib3-1.25.3 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
