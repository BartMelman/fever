{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "from utils_db import load_jsonl\n",
    "from vocabulary import VocabularySqlite\n",
    "from tfidf_database import TFIDFDatabaseSqlite\n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "# from text_database import TextDatabaseSqlite\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "from utils_db import load_jsonl\n",
    "from vocabulary import VocabularySqlite\n",
    "from tfidf_database import TFIDFDatabaseSqlite\n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "# from text_database import TextDatabaseSqlite\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer\n",
    "\n",
    "import config\n",
    "\n",
    "def get_vocab_tf_idf_from_exp(experiment_nr):\n",
    "    file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "    path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "\n",
    "    with open(path_experiment) as json_data_file:\n",
    "        data = json.load(json_data_file)\n",
    "\n",
    "    vocab = VocabularySqlite(wiki_database = wiki_database, n_gram = data['n_gram'],\n",
    "        method_tokenization = data['method_tokenization'], tags_in_db_flag = data['tags_in_db_flag'], \n",
    "        source = data['vocabulary_source'], tag_list_selected = data['tag_list_selected'])\n",
    "\n",
    "    tf_idf_db = TFIDFDatabaseSqlite(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "        delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "    return vocab, tf_idf_db\n",
    "\n",
    "def get_dict_from_n_gram(n_gram_list, mydict_ids, mydict_tf_idf, tf_idf_db):\n",
    "    \n",
    "    dictionary = {}\n",
    "\n",
    "    for word in n_gram_list:\n",
    "        try:\n",
    "            word_id_list = mydict_ids[word].split(tf_idf_db.delimiter)[1:]\n",
    "            word_tf_idf_list = mydict_tf_idf[word].split(tf_idf_db.delimiter)[1:]\n",
    "        except KeyError:\n",
    "            print('KeyError', word)\n",
    "            word_id_list = []\n",
    "            word_tf_idf_list = []\n",
    "        for j in range(len(word_id_list)):\n",
    "            id = int(word_id_list[j])       \n",
    "            tf_idf = float(word_tf_idf_list[j])\n",
    "            try:\n",
    "                dictionary[id] = dictionary[id] + tf_idf\n",
    "            except KeyError:\n",
    "                dictionary[id] = tf_idf\n",
    "    return dictionary\n",
    "\n",
    "def get_empty_dict_claim():\n",
    "    empty_dict = {}\n",
    "    empty_dict['nr_words_title'] = None\n",
    "    empty_dict['nr_words_per_pos'] = None\n",
    "    empty_dict['tokenize'] = {}\n",
    "    empty_dict['tokenize']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['tf_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['raw_count_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['idf'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['max_sum_idf'] = 0\n",
    "    empty_dict['tokenize_lemma'] = {}\n",
    "    empty_dict['tokenize_lemma']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['tf_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['max_sum_idf'] = 0\n",
    "    return empty_dict\n",
    "       \n",
    "def get_empty_tag_dict():\n",
    "    tag_2_id_dict = get_tag_2_id_dict()\n",
    "    empty_tag_dict = {}\n",
    "    for pos_id in range(len(tag_2_id_dict)):\n",
    "        empty_tag_dict[pos_id] = 0\n",
    "    return empty_tag_dict\n",
    "\n",
    "def get_tag_2_id_dict():\n",
    "    tag_2_id_dict = {}\n",
    "    tag_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "\n",
    "    for i in range(len(tag_list)):\n",
    "        tag = tag_list[i]\n",
    "        tag_2_id_dict[tag] = i\n",
    "    return tag_2_id_dict\n",
    "\n",
    "def get_tf_idf_name(experiment_nr):\n",
    "    if experiment_nr in [31,34, 37]:\n",
    "        return 'tf_idf'\n",
    "    elif experiment_nr in [32,35, 38]:\n",
    "        return 'raw_count_idf'\n",
    "    elif experiment_nr in [33,36]:\n",
    "        return 'idf'\n",
    "    else:\n",
    "        raise ValueError('experiment_nr not in selection', experiment_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "\n",
    "# === process claim === #\n",
    "experiment_nr = 37\n",
    "\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "claim_dict = {}\n",
    "\n",
    "claim_dict['claim'] = {}\n",
    "claim_dict['claim']['nr_words'] = sum(n_grams.values())\n",
    "claim_dict['claim']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "claim_dict['title'] = {}\n",
    "claim_dict['title']['ids'] = {}\n",
    "\n",
    "tag_list = []\n",
    "for key, count in n_grams.items():\n",
    "    tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "    tag_list.append(tag)\n",
    "    pos_id = tag_2_id_dict[tag]\n",
    "    claim_dict['claim']['nr_words_per_pos'][pos_id] += count\n",
    "\n",
    "# === process titles === #\n",
    "experiment_nr = 31\n",
    "\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "path_title_ids = tf_idf_db.path_ids_dict\n",
    "path_title_tf_idf = tf_idf_db.path_tf_idf_dict\n",
    "mydict_ids = SqliteDict(path_title_ids)\n",
    "mydict_tf_idf = SqliteDict(path_title_tf_idf)\n",
    "\n",
    "idx = 0\n",
    "for key, count in n_grams.items():\n",
    "    tag = tag_list[idx]\n",
    "\n",
    "    dictionary = get_dict_from_n_gram([key], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "    tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "    tag_nr = tag_2_id_dict[tag]\n",
    "\n",
    "    for id, tf_idf_value in dictionary.items():\n",
    "        if id in claim_dict['title']['ids'].keys():\n",
    "            claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][tag_nr] += tf_idf_value\n",
    "        else:\n",
    "            claim_dict['title']['ids'][id] = get_empty_dict_claim()\n",
    "            claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][tag_nr] += tf_idf_value\n",
    "\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87535"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(claim_dict['title']['ids'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 save list claim tags\n",
    "\n",
    "# step 2 create files in folder with claim id\n",
    "\n",
    "# step 3 iterate for every experiment through \n",
    "\n",
    "# step 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing settings file\n",
      "Load title dictionary\n"
     ]
    }
   ],
   "source": [
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp\n",
      "Load existing settings file\n",
      "Document count dictionary already exists\n",
      "Directory  /home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp/thr_0.01_  already exists\n",
      "Directory  /home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp/thr_0.01_/ex_term_frequency_inverse_document_frequency_title  already exists\n",
      "load total TF-IDF dictionary\n",
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_title_1_lp/thr_0.01_/ex_term_frequency_inverse_document_frequency_title/title_total_tf_idf.json\n",
      "empty database already exists\n",
      "database already filled\n"
     ]
    }
   ],
   "source": [
    "experiment_nr = 37\n",
    "file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "\n",
    "with open(path_experiment) as json_data_file:\n",
    "    data = json.load(json_data_file)\n",
    "\n",
    "# === run === #\n",
    "vocab = VocabularySqlite(wiki_database = wiki_database, n_gram = data['n_gram'],\n",
    "    method_tokenization = data['method_tokenization'], tags_in_db_flag = data['tags_in_db_flag'], \n",
    "    source = data['vocabulary_source'], tag_list_selected = data['tag_list_selected'])\n",
    "\n",
    "tf_idf_db = TFIDFDatabaseSqlite(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "    delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_data_set_names = ['dev']\n",
    "claim_data_set = claim_data_set_names[0]\n",
    "path_dev_set = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR, claim_data_set + \".jsonl\")\n",
    "results = load_jsonl(path_dev_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_doc_results import Claim, ClaimDocTokenizer\n",
    "\n",
    "results\n",
    "i = 0\n",
    "claim = Claim(results[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PROPN Colin\n",
      "PROPN Kaepernick\n",
      "VERB become\n",
      "DET a\n",
      "VERB start\n",
      "NOUN quarterback\n",
      "ADP during\n",
      "DET the\n",
      "NOUN 49er\n",
      "NUM 63rd\n",
      "NOUN season\n",
      "ADP in\n",
      "PROPN National\n",
      "PROPN Football\n",
      "PROPN League\n",
      "PUNCT .\n"
     ]
    }
   ],
   "source": [
    "from utils_doc_results import get_tag_word_from_wordtag\n",
    "for key, count in n_grams.items():\n",
    "    tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "    print(tag, word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_pos_tags_claim(n_grams):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "experiment_nr = 37\n",
    "\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "claim_dict = {}\n",
    "\n",
    "# claim features\n",
    "claim_dict['claim'] = {}\n",
    "claim_dict['claim']['nr_words'] = sum(n_grams.values())\n",
    "claim_dict['claim']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "\n",
    "for key, count in n_grams.items():\n",
    "    tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "    pos_id = tag_2_id_dict[tag]\n",
    "    claim_dict['claim']['nr_words_per_pos'][pos_id] += count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-143-b597943da182>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-143-b597943da182>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    claim_dict['title']['ids'][id][vocab.method_tokenization[0]]['tf_idf_sum_per_pos_' + str(experiment_nr)][] += tf_idf_value\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# title features\n",
    "claim_dict['title'] = {}\n",
    "claim_dict['title']['ids'] = {}\n",
    "# get_empty_dict_claim()\n",
    "# tf idf per tag\n",
    "experiment_nr = 31\n",
    "vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "# === initialise databases === #\n",
    "path_title_ids = tf_idf_db.path_ids_dict\n",
    "path_title_tf_idf = tf_idf_db.path_tf_idf_dict\n",
    "mydict_ids = SqliteDict(path_title_ids)\n",
    "mydict_tf_idf = SqliteDict(path_title_tf_idf)\n",
    "\n",
    "dictionary = get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "tag_nr = get_tag_2_id_dict()\n",
    "\n",
    "for id, tf_idf_value in dictionary.items():\n",
    "    if id in claim_dict['title']['ids'].keys():\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "    else:\n",
    "        claim_dict['title']['ids'][id] = get_empty_dict_claim()\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_name(experiment_nr):\n",
    "    if experiment_nr in [31,34, 37]:\n",
    "        return 'tf_idf'\n",
    "    elif experiment_nr in [32,35, 38]:\n",
    "        return 'raw_count_idf'\n",
    "    elif experiment_nr in [33,36]:\n",
    "        return 'idf'\n",
    "    else:\n",
    "        raise ValueError('experiment_nr not in selection', experiment_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db):\n",
    "    \n",
    "    dictionary = {}\n",
    "\n",
    "    for word in n_grams:\n",
    "        try:\n",
    "            word_id_list = mydict_ids[word].split(tf_idf_db.delimiter)[1:]\n",
    "            word_tf_idf_list = mydict_tf_idf[word].split(tf_idf_db.delimiter)[1:]\n",
    "        except KeyError:\n",
    "            print('KeyError', word)\n",
    "            word_id_list = []\n",
    "            word_tf_idf_list = []\n",
    "        for j in range(len(word_id_list)):\n",
    "            id = int(word_id_list[j])       \n",
    "            tf_idf = float(word_tf_idf_list[j])\n",
    "            try:\n",
    "                dictionary[id] = dictionary[id] + tf_idf\n",
    "            except KeyError:\n",
    "                dictionary[id] = tf_idf\n",
    "    return dictionary\n",
    "\n",
    "def get_empty_dict_claim():\n",
    "    empty_dict = {}\n",
    "    empty_dict['nr_words_title'] = None\n",
    "    empty_dict['nr_words_per_pos'] = None\n",
    "    empty_dict['tokenize'] = {}\n",
    "    empty_dict['tokenize']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['tf_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['raw_count_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['idf'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['max_sum_idf'] = 0\n",
    "    empty_dict['tokenize_lemma'] = {}\n",
    "    empty_dict['tokenize_lemma']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['tf_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['max_sum_idf'] = 0\n",
    "    return empty_dict\n",
    "       \n",
    "def get_empty_tag_dict():\n",
    "    tag_2_id_dict = get_tag_2_id_dict()\n",
    "    empty_tag_dict = {}\n",
    "    for pos_id in range(len(tag_2_id_dict)):\n",
    "        empty_tag_dict[pos_id] = 0\n",
    "    return empty_tag_dict\n",
    "\n",
    "def get_tag_2_id_dict():\n",
    "    tag_2_id_dict = {}\n",
    "    tag_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "\n",
    "    for i in range(len(tag_list)):\n",
    "        tag = tag_list[i]\n",
    "        tag_2_id_dict[tag] = i\n",
    "    return tag_2_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_tf_idf_from_exp(experiment_nr):\n",
    "    file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "    path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "\n",
    "    with open(path_experiment) as json_data_file:\n",
    "        data = json.load(json_data_file)\n",
    "\n",
    "    vocab = VocabularySqlite(wiki_database = wiki_database, n_gram = data['n_gram'],\n",
    "        method_tokenization = data['method_tokenization'], tags_in_db_flag = data['tags_in_db_flag'], \n",
    "        source = data['vocabulary_source'], tag_list_selected = data['tag_list_selected'])\n",
    "\n",
    "    tf_idf_db = TFIDFDatabaseSqlite(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "        delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "    return vocab, tf_idf_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "\n",
    "# print 'This will print'\n",
    "\n",
    "# blockPrint()\n",
    "# print \"This won't\"\n",
    "\n",
    "# enablePrint()\n",
    "# print \"This will too\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<1.1.0,>=0.28.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: jsonschema<3.1.0,>=2.6.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: preshed<2.1.0,>=2.0.1 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<2.1.0,>=2.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 92kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/6c/2ef2d6f4c63a197981f4ac01bb17560c857c6721213c7c99998e48cdda2a/srsly-0.0.7-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: idna<2.9,>=2.5 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: pyrsistent>=0.14.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Requirement already up-to-date: six>=1.11.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting setuptools (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: attrs>=17.4.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.2->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: wasabi, numpy, blis, tqdm, srsly, thinc, spacy, urllib3, certifi, setuptools\n",
      "  Found existing installation: numpy 1.16.3\n",
      "    Uninstalling numpy-1.16.3:\n",
      "      Successfully uninstalled numpy-1.16.3\n",
      "  Found existing installation: tqdm 4.32.1\n",
      "    Uninstalling tqdm-4.32.1:\n",
      "      Successfully uninstalled tqdm-4.32.1\n",
      "  Found existing installation: thinc 6.12.1\n",
      "    Uninstalling thinc-6.12.1:\n",
      "      Successfully uninstalled thinc-6.12.1\n",
      "  Found existing installation: spacy 2.0.18\n",
      "    Uninstalling spacy-2.0.18:\n",
      "      Successfully uninstalled spacy-2.0.18\n",
      "  Found existing installation: urllib3 1.25.2\n",
      "    Uninstalling urllib3-1.25.2:\n",
      "      Successfully uninstalled urllib3-1.25.2\n",
      "  Found existing installation: certifi 2019.3.9\n",
      "    Uninstalling certifi-2019.3.9:\n",
      "      Successfully uninstalled certifi-2019.3.9\n",
      "  Found existing installation: setuptools 39.0.1\n",
      "    Uninstalling setuptools-39.0.1:\n",
      "      Successfully uninstalled setuptools-39.0.1\n",
      "Successfully installed blis-0.2.4 certifi-2019.6.16 numpy-1.16.4 setuptools-41.0.1 spacy-2.1.4 srsly-0.0.7 thinc-7.0.4 tqdm-4.32.2 urllib3-1.25.3 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
