{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from sqlitedict import SqliteDict\n",
    "import shutil\n",
    "from tqdm import tnrange\n",
    "\n",
    "from utils_db import dict_save_json, dict_load_json, load_jsonl\n",
    "from text_database import TextDatabase, Text\n",
    "from vocabulary import Vocabulary\n",
    "from vocabulary import count_n_grams\n",
    "from tfidf_database import TFIDFDatabase\n",
    "\n",
    "import config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_jsonl(filename, dic_list):\n",
    "    # description: only use for wikipedia dump\n",
    "    output_file = open(filename, 'w', encoding='utf-8')\n",
    "    for dic in dic_list:\n",
    "        json.dump(dic, output_file) \n",
    "        output_file.write(\"\\n\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Claim:\n",
    "    def __init__(self, claim_dictionary):\n",
    "        self.id = claim_dictionary['id']\n",
    "        self.verifiable = claim_dictionary['verifiable']\n",
    "        self.label = claim_dictionary['label']\n",
    "        self.claim = claim_dictionary['claim']\n",
    "        self.evidence = claim_dictionary['evidence']\n",
    "        if 'docs_selected' in claim_dictionary:\n",
    "            self.docs_selected = claim_dictionary['docs_selected']\n",
    "    def get_tokenized_claim(self, method_tokenization):\n",
    "        claim_without_dot = self.claim[:-1]  # remove . at the end\n",
    "        text = Text(claim_without_dot, \"claim\")\n",
    "        tokenized_claim = text.process(method_tokenization)\n",
    "        return tokenized_claim\n",
    "    def get_n_grams(self, method_tokenization, n_gram):\n",
    "        return count_n_grams(self.get_tokenized_claim(method_tokenization), n_gram, 'str')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "__root__ = os.path.dirname(os.path.dirname(os.getcwd()))\n",
    "__data_dir__ = \"_01_data\"\n",
    "__wiki_dir__ = \"_02_wikipedia_pages\"\n",
    "__dep_dir__ = \"_90_dependencies\"\n",
    "__raw_data_dir__ = \"_01_raw_data\"\n",
    "__jupyter_notebook_dir__ = \"_98_jupyter_notebook\"\n",
    "path_mydict_tf_idf = \"mydict_tf_idf.sqlite\"\n",
    "path_mydict_ids = \"mydict_ids.sqlite\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/wiki.db\n",
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_text_2_t_mlc\n",
      "Load existing settings file\n",
      "Word count count dictionary already exists\n",
      "Document count dictionary already exists\n",
      "Load title_2_id and id_2_title dictionaries\n",
      "Directory  /home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_text_2_t_mlc/thr_0.001_  already exists\n",
      "Directory  /home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_text_2_t_mlc/thr_0.001_/ex_raw_count_inverse_document_frequency_title  already exists\n",
      "selected vocabulary dictionary already exists\n",
      "empty database already exists\n",
      "database already filled\n"
     ]
    }
   ],
   "source": [
    "experiment_nr = 3\n",
    "file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "with open(path_experiment) as json_data_file:\n",
    "    data = json.load(json_data_file)\n",
    "\n",
    "vocab = Vocabulary(path_wiki_database = os.path.join(config.ROOT, data['path_large_wiki_database']), \n",
    "    table_name_wiki = data['table_name_wiki'], n_gram = data['n_gram'],\n",
    "    method_tokenization = data['method_tokenization'], source = data['vocabulary_source'])\n",
    "\n",
    "tf_idf_db = TFIDFDatabase(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "    delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "\n",
    "path_title_unigrams_ids = tf_idf_db.path_ids_dict\n",
    "path_title_unigrams_tf_idf = tf_idf_db.path_tf_idf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 91198,\n",
       "  'verifiable': 'NOT VERIFIABLE',\n",
       "  'label': 'NOT ENOUGH INFO',\n",
       "  'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.',\n",
       "  'evidence': [[[108548, None, None, None]]]},\n",
       " {'id': 194462,\n",
       "  'verifiable': 'NOT VERIFIABLE',\n",
       "  'label': 'NOT ENOUGH INFO',\n",
       "  'claim': 'Tilda Swinton is a vegan.',\n",
       "  'evidence': [[[227768, None, None, None]]]},\n",
       " {'id': 137334,\n",
       "  'verifiable': 'VERIFIABLE',\n",
       "  'label': 'SUPPORTS',\n",
       "  'claim': 'Fox 2000 Pictures released the film Soul Food.',\n",
       "  'evidence': [[[289914, 283015, 'Soul_Food_-LRB-film-RRB-', 0]],\n",
       "   [[291259, 284217, 'Soul_Food_-LRB-film-RRB-', 0]],\n",
       "   [[293412, 285960, 'Soul_Food_-LRB-film-RRB-', 0]],\n",
       "   [[337212, 322620, 'Soul_Food_-LRB-film-RRB-', 0]],\n",
       "   [[337214, 322622, 'Soul_Food_-LRB-film-RRB-', 0]]]},\n",
       " {'id': 166626,\n",
       "  'verifiable': 'NOT VERIFIABLE',\n",
       "  'label': 'NOT ENOUGH INFO',\n",
       "  'claim': 'Anne Rice was born in New Jersey.',\n",
       "  'evidence': [[[191656, None, None, None], [191657, None, None, None]]]},\n",
       " {'id': 111897,\n",
       "  'verifiable': 'VERIFIABLE',\n",
       "  'label': 'REFUTES',\n",
       "  'claim': 'Telemundo is a English-language television network.',\n",
       "  'evidence': [[[131371, 146144, 'Telemundo', 0]],\n",
       "   [[131371, 146148, 'Telemundo', 1]],\n",
       "   [[131371, 146150, 'Telemundo', 4],\n",
       "    [131371, 146150, 'Hispanic_and_Latino_Americans', 0]],\n",
       "   [[131371, 146151, 'Telemundo', 5]]]}]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_dev_set = os.path.join(__root__, __data_dir__, __raw_data_dir__, \"dev.jsonl\")\n",
    "results = load_jsonl(path_dev_set)\n",
    "results[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exists\n"
     ]
    }
   ],
   "source": [
    "# === constants === #\n",
    "delimiter ='\\k'\n",
    "\n",
    "# === variables === #\n",
    "K = 20\n",
    "\n",
    "# === load claims === #\n",
    "path_dev_set = os.path.join(__root__, __data_dir__, __raw_data_dir__, \"dev.jsonl\")\n",
    "results = load_jsonl(path_dev_set)\n",
    "\n",
    "# === process === #\n",
    "method_tokenization = vocab.method_tokenization #[\"tokenize\", \"make_lower_case\"]\n",
    "\n",
    "mydict_ids = SqliteDict(path_title_bigrams_ids)\n",
    "mydict_tf_idf = SqliteDict(path_title_bigrams_tf_idf)\n",
    "\n",
    "file_name = 'predicted_labels_' + str(K) + '.json'\n",
    "path_predicted_documents = os.path.join(tf_idf_db.base_dir, file_name)\n",
    "\n",
    "if os.path.isfile(path_predicted_documents):\n",
    "    print('file already exists')\n",
    "else:\n",
    "    for i in tnrange(len(results)):\n",
    "        claim = Claim(results[i])\n",
    "\n",
    "        dictionary = {}\n",
    "\n",
    "        n_grams, nr_words = claim.get_n_grams(method_tokenization, vocab.n_gram)\n",
    "\n",
    "        for word in n_grams:\n",
    "            try:\n",
    "                word_id_list = mydict_ids[word].split(delimiter)[1:]\n",
    "                word_tf_idf_list = mydict_tf_idf[word].split(delimiter)[1:]\n",
    "            except KeyError:\n",
    "                word_id_list = []\n",
    "                word_tf_idf_list = []\n",
    "            for j in range(len(word_id_list)):\n",
    "                id = int(word_id_list[j])       \n",
    "                tf_idf = float(word_tf_idf_list[j])\n",
    "                try:\n",
    "                    dictionary[id] = dictionary[id] + tf_idf\n",
    "                except KeyError:\n",
    "                    dictionary[id] = tf_idf\n",
    "\n",
    "        keys_list = list(dictionary.keys())\n",
    "        tf_idf_list = list(dictionary.values())\n",
    "\n",
    "        dictionary = {}\n",
    "\n",
    "        # make K best selection based on score\n",
    "        selected_ids = sorted(range(len(tf_idf_list)), key=lambda l: tf_idf_list[l])[-K:]\n",
    "        selected_ids = [keys_list[l] for l in selected_ids]\n",
    "\n",
    "        results[i]['docs_selected'] = selected_ids\n",
    "        \n",
    "    write_jsonl(path_predicted_documents, results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8190818e631404b9d747758f635f034",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=19998), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.6465595634550955\n"
     ]
    }
   ],
   "source": [
    "results = load_jsonl(path_predicted_documents)\n",
    "\n",
    "nr_correct = 0\n",
    "nr_claims = 0\n",
    "nr_no_evidence = 0\n",
    "nr_title_not_in_dict = 0\n",
    "\n",
    "method_list = [\"min_one\", \"overall_score\"]\n",
    "method = method_list[0]\n",
    "\n",
    "for i in tnrange(len(results)):\n",
    "    claim = Claim(results[i])\n",
    "    \n",
    "    # scoring the selection\n",
    "    score = \"incorrect\"  # 'correct', 'no_evidence'\n",
    "    if method == \"min_one\":\n",
    "        for interpreter in claim.evidence:\n",
    "            for proof in interpreter:\n",
    "                title_proof = proof[2]\n",
    "                if title_proof == None:\n",
    "                    score = \"no_evidence\"\n",
    "                else:\n",
    "                    try:\n",
    "                        id_proof = vocab.title_2_id_dict[title_proof]\n",
    "                        if id_proof in claim.docs_selected:\n",
    "                            score = \"correct\"\n",
    "                    except KeyError:\n",
    "                        score = \"title_not_in_dictionary\"\n",
    "#                         print(\"title not in dictionary, check out\", title_proof)\n",
    "                        break\n",
    "    nr_claims += 1\n",
    "    if score == 'title_not_in_dictionary':\n",
    "        nr_title_not_in_dict += 1\n",
    "    elif score == \"no_evidence\":\n",
    "        nr_no_evidence += 1\n",
    "    elif score == \"correct\":\n",
    "        nr_correct += 1\n",
    "print(nr_correct / float(nr_claims - nr_no_evidence - nr_title_not_in_dict + 0.000001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from text_database import TextDatabase\n",
    "\n",
    "# path_large_wiki_database = '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/wiki.db' \n",
    "# # path_wiki_database = 'wiki.db'\n",
    "# table_name_wiki = 'wikipages'\n",
    "\n",
    "# text_db = TextDatabase(path_large_wiki_database, table_name_wiki)\n",
    "# path_dev_set = os.path.join(__root__, __data_dir__, __raw_data_dir__, \"dev.jsonl\")\n",
    "# results = load_jsonl(path_dev_set)\n",
    "# i = 5\n",
    "# method_tokenization = [\"tokenize\", \"make_lower_case\"]\n",
    "# claim = Claim(results[i])\n",
    "# print(claim.claim)\n",
    "# print(claim.evidence)\n",
    "# id = title_2_id_dict['Damon_Albarn']\n",
    "# print(text_db.get_tokenized_title_from_id(id))\n",
    "# print(text_db.get_tokenized_text_from_id(id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import shutil\n",
    "# import json\n",
    "# import config\n",
    "\n",
    "# from vocabulary import Vocabulary\n",
    "\n",
    "# from tfidf_database import TFIDFDatabase\n",
    "\n",
    "# # === unigram titles === #\n",
    "# experiment_nr = 4\n",
    "# file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "# path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "# with open(path_experiment) as json_data_file:\n",
    "#     data = json.load(json_data_file)\n",
    "\n",
    "# vocab_1 = Vocabulary(path_wiki_database = os.path.join(config.ROOT, data['path_large_wiki_database']), \n",
    "#     table_name_wiki = data['table_name_wiki'], n_gram = data['n_gram'],\n",
    "#     method_tokenization = data['method_tokenization'], source = data['vocabulary_source'])\n",
    "\n",
    "# tf_idf_db_1 = TFIDFDatabase(vocabulary = vocab_1, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "#     delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "\n",
    "# # === bigram titles === #\n",
    "# experiment_nr = 3\n",
    "# file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "# path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "# with open(path_experiment) as json_data_file:\n",
    "#     data = json.load(json_data_file)\n",
    "\n",
    "# vocab_2 = Vocabulary(path_wiki_database = os.path.join(config.ROOT, data['path_large_wiki_database']), \n",
    "#     table_name_wiki = data['table_name_wiki'], n_gram = data['n_gram'],\n",
    "#     method_tokenization = data['method_tokenization'], source = data['vocabulary_source'])\n",
    "\n",
    "# tf_idf_db_2 = TFIDFDatabase(vocabulary = vocab_2, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "#     delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "# path_title_unigrams_ids = tf_idf_db_1.path_ids_dict\n",
    "# path_title_unigrams_tf_idf = tf_idf_db_1.path_tf_idf_dict\n",
    "# path_title_bigrams_ids = tf_idf_db_2.path_ids_dict\n",
    "# path_title_bigrams_tf_idf = tf_idf_db_2.path_tf_idf_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-4e3ae5d33f3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdelimiter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'\\k'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_large_wiki_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtable_name_wiki\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_gram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_tokenization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;31m# tf_idf_db = TFIDFDatabase(vocab, method_tf, method_df, delimiter, threshold, 'text')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "path_large_wiki_database = '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/wiki.db' \n",
    "# path_wiki_database = 'wiki.db'\n",
    "table_name_wiki = 'wikipages'\n",
    "# table_name_tf_idf = 'tf_idf'\n",
    "# path_tf_idf_database = 'tf_idf.db'\n",
    "# path_mydict_tf_idf = 'mydict_tf_idf.sqlite'\n",
    "# path_mydict_ids = 'mydict_ids.sqlite'\n",
    "\n",
    "# === settings experiment === #\n",
    "n_gram = 2\n",
    "# method_tokenization = ['tokenize', 'remove_space', 'make_lower_case', 'lemmatization_get_nouns']\n",
    "method_tokenization = ['tokenize', 'make_lower_case']\n",
    "threshold = 0.001\n",
    "method_tf = 'raw_count' # raw_count term_frequency\n",
    "method_df = 'inverse_document_frequency' # \n",
    "delimiter = '\\k'\n",
    "\n",
    "vocab = Vocabulary(path_large_wiki_database, table_name_wiki, n_gram, method_tokenization, 'text')\n",
    "# tf_idf_db = TFIDFDatabase(vocab, method_tf, method_df, delimiter, threshold, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'following',\n",
       " 'are',\n",
       " 'the',\n",
       " 'football',\n",
       " '-lrb-',\n",
       " 'soccer',\n",
       " '-rrb-',\n",
       " 'events',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " '1928',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_nr=1\n",
    "\n",
    "vocab.text_database.get_tokenized_text_from_id(id_nr, method_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
