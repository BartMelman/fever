{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from sqlitedict import SqliteDict\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "\n",
    "from wiki_database import WikiDatabaseSqlite, Text\n",
    "from utils_doc_results_db import get_empty_tag_dict, get_tag_2_id_dict_unigrams, get_tag_2_id_dict_bigrams, get_tag_dict, get_tf_idf_from_exp, get_tf_idf_name, get_vocab_tf_idf_from_exp\n",
    "from utils_doc_results_db import get_dict_from_n_gram, get_list_properties, get_value_if_exists, label_2_num\n",
    "from utils_db import dict_load_json, dict_save_json, HiddenPrints, load_jsonl, mkdir_if_not_exist\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer, get_tag_word_from_wordtag, ClaimDatabase\n",
    "from doc_results import PerformanceTFIDF\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimFile:\n",
    "    \"\"\"A sample Employee class\"\"\"\n",
    "    def __init__(self, id, path_dir_files):\n",
    "        # description: dictionary of a claim id with all the information from the different experiments\n",
    "        # input:\n",
    "        # - id : claim id number\n",
    "        # - path_dir_files : directory at which it needs to be saved\n",
    "\n",
    "        self.path_claim = os.path.join(path_dir_files, str(id) + '.json')\n",
    "        if os.path.isfile(self.path_claim):\n",
    "            self.claim_dict = dict_load_json(self.path_claim)\n",
    "        else:\n",
    "            self.claim_dict = {}\n",
    "            self.claim_dict['claim'] = {}\n",
    "            self.claim_dict['claim']['1_gram'] = {}\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = None\n",
    "            self.claim_dict['claim']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict(n_gram = 1)\n",
    "            self.claim_dict['title'] = {}\n",
    "            self.claim_dict['title']['1_gram'] = {}\n",
    "            self.claim_dict['text'] = {}\n",
    "            self.claim_dict['text']['1_gram'] = {}\n",
    "            self.save_claim()\n",
    "    \n",
    "    def process_claims_selected(self, claim_dictionary, wiki_database):\n",
    "        # description: add ids to selected dictionary which are the proof\n",
    "        # input: \n",
    "        # - claim_dictionary: dictionary of the claim\n",
    "        claim = Claim(claim_dictionary)\n",
    "        if 'ids_selected' not in self.claim_dict:\n",
    "            interpreter_list = claim.evidence\n",
    "            id_list = []\n",
    "            for interpreter in interpreter_list:\n",
    "                for proof in interpreter:\n",
    "                    title = proof[2]\n",
    "                    if title is not None:\n",
    "                        id = wiki_database.get_id_from_title(title)\n",
    "                        id_list.append(id)\n",
    "            self.claim_dict['ids_correct_docs'] = id_list\n",
    "            self.claim_dict['ids_selected'] = id_list\n",
    "        \n",
    "        # === add from selected_ids in claim_dictionary === #    \n",
    "\n",
    "        if 'docs_selected' in claim_dictionary:\n",
    "            self.claim_dict['ids_selected'] += claim_dictionary['docs_selected']\n",
    "\n",
    "        # === save === #\n",
    "        self.save_claim()\n",
    "        \n",
    "    def process_claim(self, claim):\n",
    "        # description: adds text and label of claim to claim_dict\n",
    "        # input\n",
    "        # - claim: claim [class Claim]\n",
    "\n",
    "        self.claim_dict['claim']['text'] = claim.claim\n",
    "        self.claim_dict['claim']['label'] = claim.label\n",
    "        self.save_claim()\n",
    "\n",
    "    def process_tags(self, tag_list, n_gram):\n",
    "        # description: add tags of unigrams and bigrams to claim_dict\n",
    "        # input:\n",
    "        # - n_gram: n-gram [int]\n",
    "        # - tag_list: list of tags [list of str]\n",
    "\n",
    "        if n_gram == 1:\n",
    "            self.claim_dict['claim'][str(n_gram) +'_gram']['tag_list'] = tag_list\n",
    "        else:\n",
    "            raise ValueError('written for n_gram == 1')\n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_tf_idf_experiment(self, experiment_nr, tf_idf_db, mydict_ids, mydict_tf_idf):\n",
    "        # description:\n",
    "        # input:\n",
    "        # - tf_idf_db\n",
    "        # - mydict_ids : key: word, value: list of document ids [Sqlite database]\n",
    "        # - mydict_tf_idf: key: word, value: list of document tf_idfs [Sqlite database]\n",
    "\n",
    "        tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "        source = tf_idf_db.source # 'text', 'title'\n",
    "        method_tokenization = tf_idf_db.vocab.method_tokenization[0]\n",
    "\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            tag_2_id_dict = get_tag_2_id_dict_unigrams()\n",
    "        elif tf_idf_db.n_gram == 2:\n",
    "            tag_2_id_dict = get_tag_2_id_dict_bigrams(tf_idf_db.vocab.tag_list_selected)\n",
    "        else:\n",
    "            raise ValueError('Code is only written for unigrams and bigrams', tf_idf_db.n_gram)\n",
    "\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "\n",
    "            tag_list = [word.pos_ for word in doc]      \n",
    "            nr_words_claim = len(tag_list)\n",
    "\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "            \n",
    "            for i in range(nr_words_claim):\n",
    "                tag = tag_list[i]\n",
    "                word = tokenized_claim_list[i]\n",
    "            \n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                \n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "                for id in self.claim_dict['ids_selected']:\n",
    "                    # save number of words claim/title/text and total tf idf\n",
    "                    if str(id) not in self.claim_dict[source]['1_gram']:\n",
    "                        self.claim_dict[source]['1_gram'][str(id)] = {}\n",
    "\n",
    "                        if source == 'title':\n",
    "                            text = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "                        elif source == 'text':\n",
    "                            text = tf_idf_db.vocab.wiki_database.get_text_from_id(id)\n",
    "                        else:\n",
    "                            raise ValueError('source not in options', source)\n",
    "\n",
    "                        doc = tf_idf_db.vocab.wiki_database.nlp(text)\n",
    "                        claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                        _, nr_words_text_source = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                        self.claim_dict[source]['1_gram'][str(id)]['nr_words'] = nr_words_text_source\n",
    "\n",
    "                    # create empty tag dictionary for method for id if does not exist\n",
    "                    if method_tokenization not in self.claim_dict[source]['1_gram'][str(id)].keys():\n",
    "                        self.claim_dict[source]['1_gram'][str(id)][method_tokenization] = {}\n",
    "                        if tf_idf_name not in self.claim_dict[source]['1_gram'][str(id)][method_tokenization].keys():\n",
    "                            self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name] = get_empty_tag_dict(n_gram = 1)\n",
    "\n",
    "                    # enter total tf_idf if not in dictionary \n",
    "                    if 'total_tf_idf' not in self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name]:\n",
    "                        self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name]['total_tf_idf'] = tf_idf_db.id_2_total_tf_idf[str(id)]\n",
    "\n",
    "                    if id in dictionary:\n",
    "                        tf_idf_value = dictionary[id]\n",
    "                        self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name][str(pos_id)] += tf_idf_value \n",
    "\n",
    "                # for id, tf_idf_value in dictionary.items():\n",
    "                #     # only save the tf idf of ids in the selected id list\n",
    "                #     if id in self.claim_dict['ids_selected']:\n",
    "                #         # save number of words claim/title/text and total tf idf\n",
    "                #         if str(id) not in self.claim_dict[source]['1_gram']:\n",
    "                #             self.claim_dict[source]['1_gram'][str(id)] = {}\n",
    "\n",
    "                #             if source == 'title':\n",
    "                #                 text = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "                #             elif source == 'text':\n",
    "                #                 text = tf_idf_db.vocab.wiki_database.get_text_from_id(id)\n",
    "                #             else:\n",
    "                #                 raise ValueError('source not in options', source)\n",
    "\n",
    "                #             doc = tf_idf_db.vocab.wiki_database.nlp(text)\n",
    "                #             claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                #             _, nr_words_text_source = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                #             self.claim_dict[source]['1_gram'][str(id)]['nr_words'] = nr_words_text_source\n",
    "\n",
    "                #         # create empty tag dictionary for method for id if does not exist\n",
    "                #         if method_tokenization not in self.claim_dict[source]['1_gram'][str(id)].keys():\n",
    "                #             self.claim_dict[source]['1_gram'][str(id)][method_tokenization] = {}\n",
    "                #             if tf_idf_name not in self.claim_dict[source]['1_gram'][str(id)][method_tokenization].keys():\n",
    "                #                 self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name] = get_empty_tag_dict(n_gram = 1)\n",
    "\n",
    "                #         # enter total tf_idf if not in dictionary \n",
    "                #         if 'total_tf_idf' not in self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name]:\n",
    "                #             self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name]['total_tf_idf'] = tf_idf_db.id_2_total_tf_idf[str(id)]\n",
    "\n",
    "                #         # save tf_idf value \n",
    "                #         self.claim_dict[source]['1_gram'][str(id)][method_tokenization][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "        \n",
    "        elif tf_idf_db.n_gram == 2:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "\n",
    "            tag_list = [word.pos_ for word in doc]      \n",
    "            nr_words_claim = len(tag_list)\n",
    "\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "            \n",
    "            for i in range(nr_words_claim-1):\n",
    "                tag_1 = tag_list[i]\n",
    "                tag_2 = tag_list[i+1]\n",
    "\n",
    "                word1 = tokenized_claim_list[i]\n",
    "                word2 = tokenized_claim_list[i+1]\n",
    "\n",
    "                pos_id = tag_2_id_dict[tag1 + tag2]\n",
    "                \n",
    "                word = tf_idf_db.vocab.delimiter_words.join([word1, word2])\n",
    "\n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "                for id, tf_idf_value in dictionary.items():\n",
    "                    # only save the tf idf of ids in the selected id list\n",
    "                    if id in self.claim_dict['ids_selected']:\n",
    "                        # create dictionary if id not in dictionary\n",
    "                        if str(id) not in self.claim_dict[source]['2_gram']:\n",
    "                            self.claim_dict[source]['2_gram'][str(id)] = {}\n",
    "\n",
    "                        # create empty tag dictionary for method for id if does not exist\n",
    "                        if method_tokenization not in self.claim_dict[source]['2_gram'][str(id)].keys():\n",
    "                            self.claim_dict[source]['2_gram'][str(id)][method_tokenization] = {}\n",
    "                            if tf_idf_name not in self.claim_dict[source]['2_gram'][str(id)][method_tokenization].keys():\n",
    "                                self.claim_dict[source]['2_gram'][str(id)][method_tokenization][tf_idf_name] = get_empty_tag_dict(n_gram = 2)\n",
    "\n",
    "                        # enter total tf_idf if not in dictionary \n",
    "                        if 'total_tf_idf' not in self.claim_dict[source]['2_gram'][str(id)][method_tokenization][tf_idf_name]:\n",
    "                            self.claim_dict[source]['2_gram'][str(id)][method_tokenization][tf_idf_name]['total_tf_idf'] = tf_idf_db.id_2_total_tf_idf[str(id)]\n",
    "\n",
    "                        # save tf_idf value \n",
    "                        self.claim_dict[source]['2_gram'][str(id)][method_tokenization][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "        \n",
    "        else:\n",
    "            raise ValueError('Function only written for unigrams and bigrams')\n",
    "        \n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_nr_words_per_pos(self, tf_idf_db, tag_2_id_dict):\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            for key, count in n_grams_dict.items():\n",
    "                tag, word = get_tag_word_from_wordtag(key, tf_idf_db.vocab.delimiter_tag_word)\n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "\n",
    "        elif tf_idf_db.n_gram == 2:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            delimiter_position_tag = '\\z'\n",
    "\n",
    "            for key, count in n_grams_dict.items():\n",
    "                splitted_key = key.split(tf_idf_db.delimiter_words)\n",
    "                key1_splitted = splitted_key[0].split(delimiter_position_tag)\n",
    "                key2_splitted = splitted_key[1].split(delimiter_position_tag)\n",
    "                tag1 = splitted_key[0].split(delimiter_position_tag)[0]\n",
    "                tag2 = splitted_key[1].split(delimiter_position_tag)[0]\n",
    "                word1 = splitted_key[0].split(delimiter_position_tag)[1]\n",
    "                word2 = splitted_key[1].split(delimiter_position_tag)[1]\n",
    "\n",
    "                pos_id = tag_2_id_dict[tag1 + tag2]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "            \n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "\n",
    "    def save_claim(self):\n",
    "        with HiddenPrints():\n",
    "            dict_save_json(self.claim_dict, self.path_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClaimTensorDatabase():\n",
    "    def __init__(self, path_wiki_pages, path_wiki_database_dir, setup):\n",
    "        # === variables === #\n",
    "        if setup == 1:\n",
    "            self.claim_data_set = 'dev'\n",
    "            self.experiment_list = [31, 37] # [31,32,33,34,35,36,37]\n",
    "        elif setup == 2:\n",
    "            self.claim_data_set = 'dev'\n",
    "            self.experiment_list = [31, 37, 41]\n",
    "\n",
    "        # === process === #\n",
    "        self.path_dir_claim_database = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "        self.path_raw_claim_data = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR)\n",
    "        self.path_results_dir = os.path.join(config.ROOT, config.RESULTS_DIR, config.SCORE_COMBINATION_DIR)\n",
    "        self.path_setup_dir = os.path.join(self.path_results_dir, 'setup_' + str(setup))\n",
    "        self.path_tags_unigram = os.path.join(self.path_setup_dir, 'tags_' + self.claim_data_set + '_n_gram_' + str(1) + '.json')\n",
    "        self.path_claims_dir = os.path.join(self.path_setup_dir, 'claims')\n",
    "        self.path_label_correct_evidence_false_dir = os.path.join(self.path_setup_dir, self.claim_data_set + '_correct_false_tensor')\n",
    "        self.path_label_correct_evidence_true_dir = os.path.join(self.path_setup_dir, self.claim_data_set + '_correct_true_tensor')\n",
    "        self.path_label_refuted_evidence_false_dir = os.path.join(self.path_setup_dir, self.claim_data_set + '_refuted_false_tensor')\n",
    "        self.path_label_refuted_evidence_true_dir = os.path.join(self.path_setup_dir, self.claim_data_set + '_refuted_true_tensor')\n",
    "        self.path_settings_dict = os.path.join(self.path_setup_dir, 'settings.json')\n",
    "        self.tag_list_selected = [\"INTJ\", \"NOUN\", \"NUM\", \"PROPN\", \"SYM\", \"X\", \"ADJ\"]\n",
    "\n",
    "        if not os.path.isdir(self.path_setup_dir):\n",
    "            self.settings = {}\n",
    "            mkdir_if_not_exist(self.path_setup_dir)\n",
    "            mkdir_if_not_exist(self.path_claims_dir)\n",
    "            mkdir_if_not_exist(self.path_label_correct_evidence_false_dir)\n",
    "            mkdir_if_not_exist(self.path_label_correct_evidence_true_dir)\n",
    "            mkdir_if_not_exist(self.path_label_refuted_evidence_false_dir)\n",
    "            mkdir_if_not_exist(self.path_label_refuted_evidence_true_dir)\n",
    "            self.get_results()\n",
    "        else:\n",
    "            self.settings = dict_load_json(self.path_settings_dict)\n",
    "        \n",
    "\n",
    "    def get_results(self):\n",
    "        self.wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "        self.claim_database = ClaimDatabase(path_dir_database = self.path_dir_claim_database, path_raw_data = self.path_raw_claim_data, claim_data_set = self.claim_data_set)\n",
    "\n",
    "        self.claim_database.nr_claims = 100\n",
    "        self.nr_claims = self.claim_database.nr_claims\n",
    "\n",
    "        self.tag_2_id_unigram_dict = get_tag_2_id_dict_unigrams()\n",
    "        self.tag_2_id_bigram_dict = get_tag_2_id_dict_bigrams(self.tag_list_selected)\n",
    "        self.tag_dict_unigrams = get_tag_dict(self.claim_database, 1, self.path_tags_unigram, self.wiki_database)\n",
    "        self.process_claim_tag_list()\n",
    "        self.process_nr_words_per_tag(n_gram = 1)\n",
    "        # self.process_nr_words_per_tag(n_gram = 2)\n",
    "        self.process_claims_in_selection_list(experiment_nr = 37, K = 5, score_method = 'f_score', title_tf_idf_normalise_flag = False)\n",
    "        self.process_selected_ids()\n",
    "        self.save_2_tensor()\n",
    "\n",
    "\n",
    "    def process_claims_in_selection_list(self, experiment_nr, K, score_method, title_tf_idf_normalise_flag):\n",
    "        # description: add documents which are close to the claim to the dataset\n",
    "        # input:\n",
    "        # - K : [int]\n",
    "        # - score_method : \n",
    "        # - title_tf_idf_normalise_flag : \n",
    "\n",
    "        performance_tf_idf = PerformanceTFIDF(self.wiki_database, experiment_nr, \n",
    "            self.claim_data_set, K, score_method, title_tf_idf_normalise_flag)\n",
    "\n",
    "        for id in tqdm(range(self.nr_claims), total = self.nr_claims, desc = 'process_claims_selected'):\n",
    "            if id < self.nr_claims:\n",
    "                file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "                claim_dict = performance_tf_idf.claim_database.get_claim_from_id(id)\n",
    "                file.process_claims_selected(claim_dict, self.wiki_database)\n",
    "\n",
    "    def process_claim_tag_list(self):\n",
    "        print('claim database: insert claim\\'s text and claim\\'s tag_list')\n",
    "\n",
    "        n_gram = 1\n",
    "        for str_id, tag_list in tqdm(self.tag_dict_unigrams.items(), total = len(self.tag_dict_unigrams), desc = 'tag'):\n",
    "            id = int(str_id)\n",
    "            if id < self.nr_claims:\n",
    "                file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "                file.process_tags(tag_list, n_gram)\n",
    "                claim_dict = self.claim_database.get_claim_from_id(id)\n",
    "                claim = Claim(claim_dict)\n",
    "                file.process_claim(claim)\n",
    "                file.process_claims_selected(claim_dict, self.wiki_database)\n",
    "\n",
    "    def process_nr_words_per_tag(self, n_gram):\n",
    "        print('claim database: insert nr words per tag for claim')\n",
    "        if n_gram == 1:\n",
    "            experiment_nr = 37\n",
    "            tag_2_id_dict = self.tag_2_id_unigram_dict\n",
    "        elif n_gram == 2:\n",
    "            experiment_nr = 41\n",
    "            tag_2_id_dict = self.tag_2_id_bigram_dict\n",
    "        else:\n",
    "            raise ValueError('only written for unigrams and bigrams', n_gram)\n",
    "\n",
    "        with HiddenPrints():\n",
    "            tf_idf_db = get_tf_idf_from_exp(experiment_nr, self.wiki_database)\n",
    "            \n",
    "        for id in tqdm(range(self.nr_claims), desc = 'nr words per pos'):\n",
    "            file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "            file.process_nr_words_per_pos(tf_idf_db, self.tag_2_id_unigram_dict)\n",
    "\n",
    "    def process_selected_ids(self):\n",
    "        print('claim database: insert selected ids')\n",
    "\n",
    "        for experiment_nr in self.experiment_list:\n",
    "            print('experiment:', experiment_nr)\n",
    "            with HiddenPrints():\n",
    "                tf_idf_db = get_tf_idf_from_exp(experiment_nr, self.wiki_database)\n",
    "\n",
    "            mydict_ids = SqliteDict(tf_idf_db.path_ids_dict)\n",
    "            mydict_tf_idf = SqliteDict(tf_idf_db.path_tf_idf_dict)\n",
    "\n",
    "            for id in tqdm(range(self.nr_claims), desc = 'nr words per pos'):\n",
    "                file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "                file.process_tf_idf_experiment(experiment_nr, tf_idf_db, mydict_ids, mydict_tf_idf)\n",
    "\n",
    "    def save_2_tensor(self):\n",
    "        print('claim database: save results to folder with tensors')\n",
    "\n",
    "        settings_dict = {}\n",
    "\n",
    "        id = 5\n",
    "        file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "\n",
    "        id_list_title = list(file.claim_dict['title']['1_gram'].keys())\n",
    "        id_list_text = list(file.claim_dict['text']['1_gram'].keys())\n",
    "\n",
    "        observation_key_list_claim, _ = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "\n",
    "        if len(id_list_title) > 0:\n",
    "            observation_key_list_title , _ = get_list_properties(file.claim_dict['title']['1_gram'][id_list_title[0]], [], [], []) \n",
    "        else:\n",
    "            observation_key_list_title = []\n",
    "\n",
    "        if len(id_list_text) > 0:\n",
    "            observation_key_list_text,  _ = get_list_properties(file.claim_dict['text']['1_gram'][id_list_text[0]], [], [], [])\n",
    "        else:\n",
    "            observation_key_list_text = []\n",
    "\n",
    "        settings_dict['observation_key_list_claim'] = observation_key_list_claim\n",
    "        settings_dict['observation_key_list_title'] = observation_key_list_title\n",
    "        settings_dict['observation_key_list_text'] = observation_key_list_text\n",
    "\n",
    "        idx = 0\n",
    "\n",
    "        nr_correct_false = 0\n",
    "        nr_correct_true = 0\n",
    "        nr_refuted_false = 0\n",
    "        nr_refuted_true = 0\n",
    "\n",
    "        for id in tqdm(range(self.claim_database.nr_claims), desc = 'save_2_tensor'):\n",
    "            file = ClaimFile(id = id, path_dir_files = self.path_claims_dir)\n",
    "\n",
    "            label = file.claim_dict['claim']['label']\n",
    "\n",
    "            if label != 'NOT ENOUGH INFO':\n",
    "                label_nr = label_2_num(label)\n",
    "\n",
    "                for id_document in file.claim_dict['ids_selected']:\n",
    "\n",
    "                    if label == 'SUPPORTS':\n",
    "                        if id_document in file.claim_dict['ids_correct_docs']:\n",
    "                            file_name_variables = os.path.join(self.path_label_correct_evidence_true_dir, 'variable_' + str(nr_correct_true) + '.pt')\n",
    "                            file_name_label = os.path.join(self.path_label_correct_evidence_true_dir, 'label_' + str(nr_correct_true) + '.pt')\n",
    "                            nr_correct_true += 1\n",
    "                        else:\n",
    "                            file_name_variables = os.path.join(self.path_label_correct_evidence_false_dir, 'variable_' + str(nr_correct_false) + '.pt')\n",
    "                            file_name_label = os.path.join(self.path_label_correct_evidence_false_dir, 'label_' + str(nr_correct_false) + '.pt')\n",
    "                            nr_correct_false += 1\n",
    "\n",
    "                    elif label == 'REFUTES':\n",
    "                        if id_document in file.claim_dict['ids_correct_docs']:\n",
    "                            file_name_variables = os.path.join(self.path_label_refuted_evidence_true_dir, 'variable_' + str(nr_refuted_true) + '.pt')\n",
    "                            file_name_label = os.path.join(self.path_label_refuted_evidence_true_dir, 'label_' + str(nr_refuted_true) + '.pt')\n",
    "                            nr_refuted_true += 1\n",
    "                        else:\n",
    "                            file_name_variables = os.path.join(self.path_label_refuted_evidence_false_dir, 'variable_' + str(nr_refuted_false) + '.pt')\n",
    "                            file_name_label = os.path.join(self.path_label_refuted_evidence_false_dir, 'label_' + str(nr_refuted_false) + '.pt')\n",
    "                            nr_refuted_false += 1\n",
    "                    else:\n",
    "                        raise ValueError('label not correct', label)\n",
    "\n",
    "                    list_variables = []\n",
    "\n",
    "                    id_list = list(file.claim_dict['title']['1_gram'].keys())\n",
    "\n",
    "                    _, values_claim = get_list_properties(file.claim_dict['claim']['1_gram'], [], [], [])\n",
    "                    list_variables += values_claim\n",
    "\n",
    "                    if id_document in file.claim_dict['title']['1_gram']:\n",
    "                        _, values_title = get_list_properties(file.claim_dict['title']['1_gram'][id_document], [], [], [])\n",
    "                        list_variables += values_title\n",
    "\n",
    "                    if id_document in file.claim_dict['text']['1_gram']:\n",
    "                        _, values_text  = get_list_properties(file.claim_dict['text']['1_gram'][id_document], [], [], [])\n",
    "                        list_variables += values_text\n",
    "\n",
    "\n",
    "                    numpy_array = np.array(list_variables)\n",
    "                    tensor_variable = torch.from_numpy(numpy_array)      \n",
    "                    tensor_label = torch.tensor([label_nr])\n",
    "\n",
    "                    torch.save(tensor_variable, file_name_variables)\n",
    "                    torch.save(tensor_label, file_name_label)\n",
    "                    \n",
    "                    idx += 1\n",
    "\n",
    "        nr_variables = 0\n",
    "        list_keys = ['observation_key_list_claim', 'observation_key_list_title', 'observation_key_list_text']\n",
    "        for key in list_keys:\n",
    "            nr_variables += len(settings_dict[key])\n",
    "\n",
    "        settings_dict['nr_variables'] = nr_variables\n",
    "        settings_dict['nr_total'] = idx\n",
    "        settings_dict['nr_correct_false'] = nr_correct_false\n",
    "        settings_dict['nr_correct_true'] = nr_correct_true\n",
    "        settings_dict['nr_refuted_false'] = nr_refuted_false\n",
    "        settings_dict['nr_refuted_true'] = nr_refuted_true\n",
    "\n",
    "        settings_dict['nr_claims'] = self.claim_database.nr_claims\n",
    "\n",
    "        dict_save_json(settings_dict, self.path_settings_dict)\n",
    "\n",
    "\n",
    "claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 1\n",
    "\n",
    "tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = 1\n",
    "file = ClaimFile(id = id, path_dir_files = tensor_db.path_claims_dir)\n",
    "tmp = file.claim_dict['title']['1_gram']['4940229']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-776bb641c7fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_list_properties\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtmp_torch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
