{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wiki_database import WikiDatabaseSqlite\n",
    "from utils_db import mkdir_if_not_exist\n",
    "import config\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_database\n",
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    }
   ],
   "source": [
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "# mkdir_if_not_exist(path_wiki_database_dir)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClaimDatabaseStage2\n",
      "stage_2 train\n",
      "sqlite database already created train\n",
      "stage_2 validation\n",
      "sqlite database already created validation\n",
      "stage_3 train\n",
      "sqlite database already created train\n",
      "stage_3 validation\n",
      "sqlite database already created validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "save pickle:   0%|          | 0/167 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle:  30%|██▉       | 50/167 [00:00<00:00, 498.16it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stage_2 dev\n",
      "sqlite database already created dev\n",
      "stage_3 dev\n",
      "sqlite database already created dev\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "save pickle:  54%|█████▍    | 91/167 [00:00<00:00, 462.78it/s]\u001b[A\n",
      "save pickle:  82%|████████▏ | 137/167 [00:00<00:00, 457.96it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 167/167 [00:00<00:00, 439.13it/s]\u001b[A\n",
      "save pickle:   0%|          | 0/24 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 24/24 [00:00<00:00, 429.20it/s]\u001b[A\n",
      "save pickle:   0%|          | 0/331 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle:  10%|█         | 34/331 [00:00<00:00, 339.98it/s]\u001b[A\n",
      "save pickle:  24%|██▍       | 80/331 [00:00<00:00, 365.10it/s]\u001b[A\n",
      "save pickle:  38%|███▊      | 126/331 [00:00<00:00, 385.46it/s]\u001b[A\n",
      "save pickle:  51%|█████▏    | 170/331 [00:00<00:00, 400.19it/s]\u001b[A\n",
      "save pickle:  65%|██████▍   | 215/331 [00:00<00:00, 412.97it/s]\u001b[A\n",
      "save pickle:  79%|███████▊  | 260/331 [00:00<00:00, 422.77it/s]\u001b[A\n",
      "save pickle:  92%|█████████▏| 303/331 [00:00<00:00, 423.66it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 331/331 [00:00<00:00, 424.62it/s]\u001b[A\n",
      "save pickle:   0%|          | 0/125 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle:  37%|███▋      | 46/125 [00:00<00:00, 456.06it/s]\u001b[A\n",
      "save pickle:  61%|██████    | 76/125 [00:00<00:00, 389.20it/s]\u001b[A\n",
      "save pickle:  96%|█████████▌| 120/125 [00:00<00:00, 394.20it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 125/125 [00:00<00:00, 380.73it/s]\u001b[A\n",
      "save pickle:   0%|          | 0/21 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 21/21 [00:00<00:00, 449.36it/s]\u001b[A\n",
      "save pickle:   0%|          | 0/227 [00:00<?, ?it/s]\u001b[A\n",
      "save pickle:  15%|█▌        | 35/227 [00:00<00:00, 344.62it/s]\u001b[A\n",
      "save pickle:  34%|███▍      | 77/227 [00:00<00:00, 363.75it/s]\u001b[A\n",
      "save pickle:  53%|█████▎    | 120/227 [00:00<00:00, 381.27it/s]\u001b[A\n",
      "save pickle:  72%|███████▏  | 164/227 [00:00<00:00, 394.85it/s]\u001b[A\n",
      "save pickle:  91%|█████████ | 206/227 [00:00<00:00, 400.50it/s]\u001b[A\n",
      "save pickle: 100%|██████████| 227/227 [00:00<00:00, 403.58it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "from utils_db import load_jsonl\n",
    "\n",
    "import config\n",
    "\n",
    "path_dir_database = os.path.join(os.getcwd(),'claim_db')\n",
    "path_raw_data = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR)\n",
    "fraction_validation = 0.1\n",
    "method_combination = 'equal' # 'equal'\n",
    "claim_database = ClaimDatabaseStage2(path_database_dir = path_dir_database, \n",
    "                                     path_raw_data = path_raw_data, \n",
    "                                     fraction_validation = fraction_validation,\n",
    "                                     method_combination = method_combination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_db import dict_load_json, dict_save_json\n",
    "import spacy\n",
    "import random\n",
    "from random import shuffle\n",
    "from sqlitedict import SqliteDict\n",
    "import sqlite3   \n",
    "from utils_db import mkdir_if_not_exist\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "\n",
    "class ClaimDatabaseStage2:\n",
    "    def __init__(self, path_database_dir, path_raw_data, fraction_validation, method_combination):\n",
    "        # description: create a database in which the \n",
    "        \n",
    "        # folder layout:n\n",
    "        # base_folder_name\n",
    "        # - settings.json\n",
    "        # - databases\n",
    "            # - train\n",
    "                # - \n",
    "            # - validation\n",
    "                # - \n",
    "            # - test\n",
    "                # - \n",
    "        \n",
    "        self.path_database_dir = path_database_dir\n",
    "        self.path_raw_data = path_raw_data\n",
    "        self.fraction_validation = fraction_validation\n",
    "        self.method_combination = method_combination\n",
    "        \n",
    "        base_folder_name = 'claim_database'\n",
    "        self.path_base_folder_dir = os.path.join(self.path_database_dir, base_folder_name)\n",
    "        mkdir_if_not_exist(self.path_base_folder_dir)\n",
    "        self.path_settings = os.path.join(self.path_base_folder_dir, 'settings.json')\n",
    "        self.path_databases_dir = os.path.join(self.path_base_folder_dir, 'databases')\n",
    "        self.pickle_files_dir = os.path.join(self.path_base_folder_dir, 'pickle_files')\n",
    "        mkdir_if_not_exist(self.pickle_files_dir)\n",
    "        mkdir_if_not_exist(self.path_databases_dir)\n",
    "\n",
    "        print('ClaimDatabaseStage2')\n",
    "        if os.path.isfile(self.path_settings):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "        else:\n",
    "            self.settings = {}\n",
    "        \n",
    "        # --- set the paths for the databases --- #\n",
    "        stage_list = ['stage_2', 'stage_3']\n",
    "        data_set_type_list = ['train', 'validation', 'dev']\n",
    "        label_list = ['correct', 'refuted', 'nei']\n",
    "        self.path_db = {}\n",
    "        for stage in stage_list:\n",
    "            if stage not in self.path_db:\n",
    "                self.path_db[stage] = {}\n",
    "            for label in label_list:\n",
    "                if label not in self.path_db[stage]:\n",
    "                    self.path_db[stage][label] = {}\n",
    "                for data_set_type in data_set_type_list:\n",
    "                    self.path_db[stage][label][data_set_type] = os.path.join(\n",
    "                        self.path_databases_dir, \n",
    "                        stage + '_' + data_set_type + '_' + label + '.sqlite')\n",
    "\n",
    "        # --- set paths for pickle file --- #\n",
    "        method_combination_list = ['equal', 'one_from_every_claim']\n",
    "        \n",
    "        self.path_pickle = {}\n",
    "        for stage in stage_list:\n",
    "            if stage not in self.path_pickle:\n",
    "                self.path_pickle[stage] = {}\n",
    "            for data_set_type in data_set_type_list:\n",
    "                if data_set_type not in self.path_pickle[stage]:\n",
    "                    self.path_pickle[stage][data_set_type] = {}\n",
    "                for method_combination in method_combination_list:\n",
    "                    self.path_pickle[stage][data_set_type][method_combination] = os.path.join(\n",
    "                        self.pickle_files_dir, \n",
    "                        stage + '_' + data_set_type + '_' + method_combination + \"_data.pkl\")\n",
    "\n",
    "        # --- get total count in data sets --- #\n",
    "        if 'total_nr_claims_train_set' in self.settings:\n",
    "            self.total_nr_claims_train_set = self.settings['total_nr_claims_train_set']\n",
    "        else:\n",
    "            print('- total_nr_claims_train_set')\n",
    "            self.total_nr_claims_train_set = self.get_nr_claims(data_set_type = 'train')\n",
    "            self.settings['total_nr_claims_train_set'] = self.total_nr_claims_train_set\n",
    "            self.save_settings()\n",
    "            \n",
    "        if 'total_nr_claims_dev_set' in self.settings:\n",
    "            self.total_nr_claims_dev_set = self.settings['total_nr_claims_dev_set']\n",
    "        else:\n",
    "            print('- total_nr_claims_dev_set')\n",
    "            self.total_nr_claims_dev_set = self.get_nr_claims(data_set_type = 'dev')\n",
    "            self.settings['total_nr_claims_dev_set'] = self.total_nr_claims_train_set\n",
    "            self.save_settings()\n",
    "\n",
    "        self.random_seed_nr = 1       \n",
    "        self.nlp = spacy.load('en', disable=[\"parser\", \"ner\"])\n",
    "        \n",
    "        if 'partition' in self.settings:\n",
    "            self.partition = self.settings['partition']\n",
    "        else:\n",
    "            print('- get partition')\n",
    "            self.partition = self.get_partition_list(fraction_validation = fraction_validation)\n",
    "            self.settings['partition'] = self.partition\n",
    "            self.save_settings()\n",
    "        \n",
    "        self.nr_claims_train = self.settings['nr_claims_train']\n",
    "        self.nr_claims_validation = self.settings['nr_claims_validation']\n",
    "        self.nr_claims_dev = self.settings['nr_claims_dev']\n",
    "        \n",
    "        \n",
    "        # --- create database train and validation set --- #\n",
    "        claim_dict_list = self.get_raw_data(data_set_type = 'train')\n",
    "        \n",
    "        for stage in stage_list:\n",
    "            for dataset_type in ['train', 'validation']:\n",
    "                self.create_database(dataset_type = dataset_type, \n",
    "                                             claim_dict_list = claim_dict_list, \n",
    "                                             partition = self.settings['partition'][dataset_type], \n",
    "                                             stage = stage)\n",
    "        \n",
    "        # --- create database dev set --- #\n",
    "        claim_dict_list = self.get_raw_data(data_set_type = 'dev')\n",
    "        \n",
    "        for stage in stage_list:\n",
    "            self.create_database(dataset_type = 'dev', \n",
    "                                 claim_dict_list = claim_dict_list, \n",
    "                                 partition = self.settings['partition']['dev'], \n",
    "                                 stage = stage)\n",
    "        \n",
    "        # --- save in pickle format for stage 2 and 3 and all datasets --- #\n",
    "        for stage in stage_list:\n",
    "            for dataset_type in data_set_type_list:\n",
    "                self.save_2_pickle(dataset_type = dataset_type, \n",
    "                                   method_combination = self.method_combination, \n",
    "                                   stage = stage) \n",
    " \n",
    "    def get_raw_data(self, data_set_type):\n",
    "        path_data_set = os.path.join(self.path_raw_data, data_set_type + '.jsonl')\n",
    "        return load_jsonl(path_data_set)\n",
    "    \n",
    "    def save_2_pickle(self, dataset_type, method_combination, stage):\n",
    "        path_save = self.path_pickle[stage][dataset_type][method_combination]\n",
    "#         dict_out['premises'] = word_list_claim\n",
    "#         dict_out['ids'] = str(claim_dict['id']) + '_' + str(claim_nr) + '_' + str(interpreters_nr) + '_correct'\n",
    "#         dict_out['labels'] = claim_dict['label']\n",
    "#         dict_out['primises_tags'] = tag_list_claim\n",
    "#         text_hypothesis = wiki_database.get_line_from_title(title, line_nr)\n",
    "#         tag_list_hypotheses, word_list_hypotheses = get_word_tag_list_from_text(text_str = text_hypothesis, nlp = claim_database.nlp, method_tokenization_str = method_tokenization)\n",
    "#         dict_out['hypotheses'] = word_list_hypotheses\n",
    "#         dict_out['hypotheses_tags'] = tag_list_hypotheses\n",
    "        if os.path.isfile(path_save):\n",
    "            print('pickle file already exists: ' + dataset_type + '_' + method_combination)\n",
    "        else:\n",
    "            if method_combination == 'equal':\n",
    "                min_nr_observations = min(self.settings[stage][dataset_type]['nr_supports'], \n",
    "                                          self.settings[stage][dataset_type]['nr_refuted'],\n",
    "                                          self.settings[stage][dataset_type]['nr_nei'])\n",
    "                path_stage_2_correct_db = self.path_db[stage]['correct'][dataset_type]\n",
    "                path_stage_2_refuted_db = self.path_db[stage]['refuted'][dataset_type]\n",
    "                path_stage_2_nei_db = self.path_db[stage]['nei'][dataset_type]\n",
    "\n",
    "                stage_2_correct_db = SqliteDict(path_stage_2_correct_db)\n",
    "                stage_2_refuted_db = SqliteDict(path_stage_2_refuted_db)\n",
    "                stage_2_nei_db = SqliteDict(path_stage_2_nei_db)\n",
    "\n",
    "                correct_keys_list = list(stage_2_correct_db.keys())\n",
    "                refuted_keys_list = list(stage_2_refuted_db.keys())\n",
    "                nei_keys_list = list(stage_2_nei_db.keys())\n",
    "\n",
    "                ids = []\n",
    "                premises = []\n",
    "                hypotheses = []\n",
    "                labels = []\n",
    "                for idx in tqdm(range(min_nr_observations), desc = 'save pickle'):\n",
    "                    # update correct\n",
    "                    key_correct = correct_keys_list[idx]\n",
    "                    claim_dict = stage_2_correct_db[key_correct][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "                    # update refuted\n",
    "                    key_refuted = refuted_keys_list[idx]\n",
    "                    claim_dict = stage_2_refuted_db[key_refuted][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "                    # update not enough info\n",
    "                    key_nei = nei_keys_list[idx]\n",
    "                    claim_dict = stage_2_nei_db[key_nei][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "\n",
    "            elif method_combination == 'one_from_every_claim':\n",
    "                min_nr_observations = min(self.settings[stage][dataset_type]['nr_supports'], \n",
    "                                          self.settings[stage][dataset_type]['nr_refuted'],\n",
    "                                          self.settings[stage][dataset_type]['nr_nei'])\n",
    "#                 print(self.settings[dataset_type]['nr_supports'], self.settings[dataset_type]['nr_refuted'], \n",
    "#                       self.settings[dataset_type]['nr_nei'])\n",
    "                path_stage_2_correct_db = self.path_db[stage]['correct'][dataset_type]\n",
    "                path_stage_2_refuted_db = self.path_db[stage]['refuted'][dataset_type]\n",
    "                path_stage_2_nei_db = self.path_db[stage]['nei'][dataset_type]\n",
    "\n",
    "                stage_2_correct_db = SqliteDict(path_stage_2_correct_db)\n",
    "                stage_2_refuted_db = SqliteDict(path_stage_2_refuted_db)\n",
    "                stage_2_nei_db = SqliteDict(path_stage_2_nei_db)\n",
    "\n",
    "                correct_keys_list = list(stage_2_correct_db.keys())\n",
    "                refuted_keys_list = list(stage_2_refuted_db.keys())\n",
    "                nei_keys_list = list(stage_2_nei_db.keys())\n",
    "\n",
    "                ids = []\n",
    "                premises = []\n",
    "                hypotheses = []\n",
    "                labels = []\n",
    "                for idx in tqdm(range(self.settings[stage][dataset_type]['nr_refuted']), desc='save pickle_refuted_'+dataset_type):\n",
    "                    # update refuted\n",
    "                    key_refuted = refuted_keys_list[idx]\n",
    "                    claim_dict = stage_2_refuted_db[key_refuted][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "                for idx in tqdm(range(self.settings[stage][dataset_type]['nr_nei']), desc='save pickle_nei_'+dataset_type):\n",
    "                    # update not enough info\n",
    "                    key_nei = nei_keys_list[idx]\n",
    "                    claim_dict = stage_2_nei_db[key_nei][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "                for idx in tqdm(range(self.settings[stage][dataset_type]['nr_nei'] - self.settings[stage][dataset_type]['nr_refuted']), desc='save pickle_correct_'+dataset_type):\n",
    "                    # update correct\n",
    "                    key_correct = correct_keys_list[idx]\n",
    "                    claim_dict = stage_2_correct_db[key_correct][0]\n",
    "                    ids.append(claim_dict['ids'])\n",
    "                    premises.append(claim_dict['premises'])\n",
    "                    hypotheses.append(claim_dict['hypotheses'])\n",
    "                    labels.append(claim_dict['labels'])\n",
    "\n",
    "            c = list(zip(ids, premises, hypotheses, labels))\n",
    "            random.shuffle(c)\n",
    "\n",
    "            ids, premises, hypotheses, labels = zip(*c)\n",
    "\n",
    "            target_dict = {\"ids\": ids,\n",
    "                \"premises\": premises,\n",
    "                \"hypotheses\": hypotheses,\n",
    "                \"labels\": labels}\n",
    "            with open(os.path.join(self.pickle_files_dir, path_save), \"wb\") as pkl_file:\n",
    "                pickle.dump(target_dict, pkl_file)\n",
    "            \n",
    "    def save_settings(self):\n",
    "        dict_save_json(self.settings, self.path_settings)\n",
    "        \n",
    "    def create_database(self, dataset_type, claim_dict_list, partition, stage):\n",
    "        print(stage, dataset_type)\n",
    "        path_stage_2_correct_db = self.path_db[stage]['correct'][dataset_type]\n",
    "        path_stage_2_refuted_db = self.path_db[stage]['refuted'][dataset_type]\n",
    "        path_stage_2_nei_db = self.path_db[stage]['nei'][dataset_type]\n",
    "        \n",
    "        mkdir_if_not_exist(os.path.dirname(path_stage_2_correct_db))\n",
    "        \n",
    "        if os.path.isfile(path_stage_2_correct_db) and os.path.isfile(path_stage_2_refuted_db) and os.path.isfile(path_stage_2_nei_db):\n",
    "            print('sqlite database already created', dataset_type)\n",
    "        else:\n",
    "            if dataset_type not in self.settings:\n",
    "                self.settings[dataset_type] = {}\n",
    "            \n",
    "            if stage not in self.settings:\n",
    "                self.settings[stage] = {}\n",
    "            if dataset_type not in self.settings[stage]:\n",
    "                self.settings[stage][dataset_type] = {}\n",
    "                \n",
    "            self.settings[stage][dataset_type]['nr_supports'] = 0\n",
    "            self.settings[stage][dataset_type]['nr_refuted'] = 0\n",
    "            self.settings[stage][dataset_type]['nr_nei'] = 0\n",
    "\n",
    "            with SqliteDict(path_stage_2_correct_db) as stage_2_correct_db:\n",
    "                with SqliteDict(path_stage_2_refuted_db) as stage_2_refuted_db:\n",
    "                    with SqliteDict(path_stage_2_nei_db) as stage_2_nei_db:\n",
    "                        for claim_nr in tqdm(partition, total = len(partition), desc='create database '+dataset_type):\n",
    "                            if claim_nr < 1000:\n",
    "                                claim_dict = claim_dict_list[claim_nr]\n",
    "                                if stage == 'stage_2':\n",
    "                                    correct_evidence_list, incorrect_evidence_list = get_claim_dict_stage_2(\n",
    "                                        claim_dict, wiki_database, claim_nr, self.nlp)\n",
    "                                elif stage == 'stage_3':\n",
    "                                    correct_evidence_list, incorrect_evidence_list = get_claim_dict_stage_3(\n",
    "                                        claim_dict, wiki_database, claim_nr, self.nlp)\n",
    "                                else:\n",
    "                                    raise ValueError('correct evidence not in list')\n",
    "\n",
    "                                label = claim_dict['label']\n",
    "\n",
    "                                if label == 'SUPPORTS':\n",
    "                                    if len(correct_evidence_list) > 0:\n",
    "                                        stage_2_correct_db[claim_nr] = correct_evidence_list\n",
    "                                        self.settings[stage][dataset_type]['nr_supports'] += 1\n",
    "                                elif label  == 'REFUTES':\n",
    "                                    if len(correct_evidence_list) > 0:\n",
    "                                        stage_2_refuted_db[claim_nr] = correct_evidence_list\n",
    "                                        self.settings[stage][dataset_type]['nr_refuted'] += 1\n",
    "                                if len(incorrect_evidence_list) > 0:\n",
    "                                    stage_2_nei_db[claim_nr] = incorrect_evidence_list\n",
    "                                    self.settings[stage][dataset_type]['nr_nei'] += 1\n",
    "\n",
    "                        stage_2_nei_db.commit()\n",
    "                    stage_2_refuted_db.commit()\n",
    "                stage_2_correct_db.commit()\n",
    "            self.save_settings()\n",
    "\n",
    "    def get_nr_claims(self, data_set_type):\n",
    "        path_data_set = os.path.join(self.path_raw_data, data_set_type + '.jsonl')\n",
    "        claim_dict_list = load_jsonl(path_data_set)\n",
    "        nr_claims = len(claim_dict_list)\n",
    "        return nr_claims\n",
    "    \n",
    "    def get_partition_list(self, fraction_validation):\n",
    "        list_total_shuffled = list(range(self.total_nr_claims_train_set))\n",
    "        random.seed(self.random_seed_nr)\n",
    "        shuffle(list_total_shuffled)\n",
    "        \n",
    "        partition = {}\n",
    "        partition['train'] = list_total_shuffled[int(self.total_nr_claims_train_set*fraction_validation):self.total_nr_claims_train_set]\n",
    "        partition['validation'] = list_total_shuffled[0:int(self.total_nr_claims_train_set*fraction_validation)]\n",
    "        partition['dev'] = list(range(self.total_nr_claims_dev_set))\n",
    "        self.settings['nr_claims_train'] = len(partition['train'])\n",
    "        self.settings['nr_claims_validation'] = len(partition['validation'])\n",
    "        self.settings['nr_claims_dev'] = len(partition['dev'])\n",
    "\n",
    "        return partition\n",
    "\n",
    "def get_claim_dict_stage_3(claim_dict, wiki_database, claim_nr, nlp):\n",
    "    method_tokenization = 'tokenize_text_pos'\n",
    "    \n",
    "    sentence_dict_list = []\n",
    "    sentence_dict_total = {}\n",
    "    list_old_proofs = []\n",
    "    for interpreter in claim_dict['evidence']:\n",
    "        sentence_dict = {}\n",
    "        tmp_proof_list = []\n",
    "        for proof in interpreter:\n",
    "            title = proof[2]\n",
    "            if title is not None:\n",
    "                normalised_title = normalise_text(title)\n",
    "                line_nr = proof[3]\n",
    "                tmp_proof_list.append(title + str(line_nr))\n",
    "                evidence_sentence = wiki_database.get_line_from_title(normalised_title, line_nr)\n",
    "                if normalised_title in sentence_dict:\n",
    "                    if line_nr not in sentence_dict[normalised_title]:\n",
    "                        sentence_dict[normalised_title].append(line_nr)\n",
    "                else:\n",
    "                    sentence_dict[normalised_title] = [line_nr]\n",
    "                    \n",
    "                if normalised_title in sentence_dict_total:\n",
    "                    sentence_dict_total[normalised_title].append(line_nr)\n",
    "                else:\n",
    "                    sentence_dict_total[normalised_title] = [line_nr]\n",
    "        proof_str =  '' + ' '.join(sorted(tmp_proof_list))           \n",
    "        if proof_str not in list_old_proofs:\n",
    "            sentence_dict_list.append(sentence_dict)\n",
    "            list_old_proofs.append(proof_str)\n",
    "           \n",
    "    text_claim = normalise_text(claim_dict['claim'])\n",
    "    tag_list_claim, word_list_claim = get_word_tag_list_from_text(text_str = text_claim, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "    \n",
    "    list_correct_observations = []\n",
    "    list_nei_observations = []\n",
    "    \n",
    "    # --- iterate over interpreters --- #\n",
    "    interpreters_nr = 0\n",
    "    for sentence_dict in sentence_dict_list:\n",
    "        \n",
    "        correct_dict_list = []\n",
    "        potential_dict_list = []\n",
    "        old_processed_claims = []\n",
    "        \n",
    "        # --- iterate over different documents --- #\n",
    "        for title, sentences_correct_list in sentence_dict.items():\n",
    "            for line_nr in sentences_correct_list:\n",
    "                # get the sentences and a list of 5 alternatives for every document\n",
    "                # create selection of proof + random select other sentences\n",
    "                # correct\n",
    "                dict_tmp = {}\n",
    "                text_hypothesis = wiki_database.get_line_from_title(title, line_nr)\n",
    "                tag_list_hypotheses, word_list_hypotheses = get_word_tag_list_from_text(text_str = text_hypothesis, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "                dict_tmp['hypotheses'] = word_list_hypotheses\n",
    "                dict_tmp['hypotheses_tags'] = tag_list_hypotheses\n",
    "                correct_dict_list.append(dict_tmp)\n",
    "            \n",
    "            # incorrect\n",
    "            lines_file = wiki_database.get_lines_list_from_title(title)\n",
    "            cosine_distance_list = []\n",
    "            for line_nr in range(len(lines_file)):\n",
    "                text_line = lines_file[line_nr]\n",
    "            \n",
    "                if ( len(text_line)>4 ) and ( line_nr not in sentence_dict_total[title] ):\n",
    "                    cosine_distance = get_cosine(text_claim, text_line)\n",
    "                else:\n",
    "                    cosine_distance = 0 \n",
    "                cosine_distance_list.append(cosine_distance)\n",
    "            index_list = get_indices_top_K_values_list(cosine_distance_list, min(9, len(lines_file)))\n",
    "            for index in index_list:\n",
    "                dict_tmp = {}\n",
    "                text_hypothesis_incorrect = lines_file[index]\n",
    "                tag_list_hypotheses, word_list_hypotheses = get_word_tag_list_from_text(text_str = text_hypothesis_incorrect, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "                dict_tmp['hypotheses'] = word_list_hypotheses\n",
    "                dict_tmp['hypotheses_tags'] = tag_list_hypotheses\n",
    "                potential_dict_list.append(dict_tmp)\n",
    "        \n",
    "        nr_correct_sentences = len(correct_dict_list)\n",
    "        nr_random_sentences = len(potential_dict_list)\n",
    "        # --- only add correct claim if enough random sentences --- #\n",
    "        if nr_correct_sentences + nr_random_sentences >= 5:\n",
    "            correct_generated_observation_flag = True\n",
    "            indices_selected_list = random.sample(range(nr_random_sentences), max(0, min(nr_random_sentences, 5-nr_correct_sentences)))\n",
    "            \n",
    "            combination_list_correct = correct_dict_list\n",
    "            \n",
    "            for index in indices_selected_list:\n",
    "                combination_list_correct.append(potential_dict_list[index])\n",
    "                \n",
    "            shuffle(combination_list_correct)\n",
    "        else:\n",
    "            correct_generated_observation_flag = False\n",
    "            \n",
    "        # --- only add not enough info if enough random sentences --- #\n",
    "        if nr_random_sentences >= 5:\n",
    "            random_generated_observation_flag = True\n",
    "            indices_selected_list = random.sample(range(nr_random_sentences), 5)\n",
    "            \n",
    "            combination_list_random = []\n",
    "            for index in indices_selected_list:\n",
    "                combination_list_random.append(potential_dict_list[index])\n",
    "                \n",
    "            shuffle(combination_list_random)\n",
    "        else:\n",
    "            random_generated_observation_flag = False\n",
    "        # --- process --- #\n",
    "        # --- correct --- #\n",
    "        if correct_generated_observation_flag == True:\n",
    "            dict_out = {}\n",
    "            dict_out['premises'] = word_list_claim\n",
    "            dict_out['ids'] = str(claim_dict['id']) + '_' + str(claim_nr) + '_' + str(interpreters_nr) + '_correct'\n",
    "            dict_out['labels'] = claim_dict['label']\n",
    "            dict_out['primises_tags'] = tag_list_claim\n",
    "            dict_out['hypotheses'] = []\n",
    "            dict_out['hypotheses_tags'] = []\n",
    "\n",
    "            for tmp_dict in combination_list_correct:           \n",
    "                dict_out['hypotheses'] += dict_tmp['hypotheses']\n",
    "                dict_out['hypotheses_tags'] += dict_tmp['hypotheses_tags']\n",
    "            list_correct_observations.append(dict_out)\n",
    "    \n",
    "        # --- random --- #\n",
    "        if random_generated_observation_flag == True:\n",
    "            dict_out = {}\n",
    "            dict_out['premises'] = word_list_claim\n",
    "            dict_out['ids'] = str(claim_dict['id']) + '_' + str(claim_nr) + '_' + str(interpreters_nr) + '_random'\n",
    "            dict_out['labels'] = claim_dict['label']\n",
    "            dict_out['primises_tags'] = tag_list_claim\n",
    "            dict_out['hypotheses'] = []\n",
    "            dict_out['hypotheses_tags'] = []\n",
    "\n",
    "            for tmp_dict in combination_list_correct:           \n",
    "                dict_out['hypotheses'] += dict_tmp['hypotheses']\n",
    "                dict_out['hypotheses_tags'] += dict_tmp['hypotheses_tags']    \n",
    "            list_nei_observations.append(dict_out)\n",
    "            \n",
    "        interpreters_nr += 1\n",
    "        \n",
    "    return list_correct_observations, list_nei_observations\n",
    "\n",
    "def get_indices_top_K_values_list(input_list, K):\n",
    "    # description: return the indices of the highest K values of a list\n",
    "    tmp = [value for value in input_list]\n",
    "    input_list.sort()\n",
    "    return [tmp.index(input_list[-i]) for i in range(1, K+1) if input_list[-i]>0]\n",
    "\n",
    "# save observation, n correct, n closest\n",
    "from utils_wiki_database import normalise_text\n",
    "\n",
    "def get_claim_dict_stage_2(claim_dict, wiki_database, claim_nr, nlp):\n",
    "    method_tokenization = 'tokenize_text_pos'\n",
    "    \n",
    "    sentence_dict = {}\n",
    "    for interpreter in claim_dict['evidence']:\n",
    "        for proof in interpreter:\n",
    "            title = proof[2]\n",
    "            if title is not None:\n",
    "                normalised_title = normalise_text(title)\n",
    "                line_nr = proof[3]\n",
    "                evidence_sentence = wiki_database.get_line_from_title(normalised_title, line_nr)\n",
    "                if normalised_title in sentence_dict:\n",
    "                    sentence_dict[normalised_title].append(line_nr)\n",
    "                else:\n",
    "                    sentence_dict[normalised_title] = [line_nr]\n",
    "                    \n",
    "    correct_evidence_list = []\n",
    "    incorrect_evidence_list = []\n",
    "    interpreters_nr = 0\n",
    "    for title, sentences_correct_list in sentence_dict.items():\n",
    "        sentences_correct_list = list(set(sentences_correct_list)) # remove duplicates\n",
    "        for line_nr in sentences_correct_list:\n",
    "            # correct\n",
    "            dict_out = {}\n",
    "            text_claim = normalise_text(claim_dict['claim'])\n",
    "            tag_list_claim, word_list_claim = get_word_tag_list_from_text(text_str = text_claim, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "            dict_out['premises'] = word_list_claim\n",
    "            dict_out['ids'] = str(claim_dict['id']) + '_' + str(claim_nr) + '_' + str(interpreters_nr) + '_correct'\n",
    "            dict_out['labels'] = claim_dict['label']\n",
    "            dict_out['primises_tags'] = tag_list_claim\n",
    "            text_hypothesis = wiki_database.get_line_from_title(title, line_nr)\n",
    "            tag_list_hypotheses, word_list_hypotheses = get_word_tag_list_from_text(text_str = text_hypothesis, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "            dict_out['hypotheses'] = word_list_hypotheses\n",
    "            dict_out['hypotheses_tags'] = tag_list_hypotheses\n",
    "            \n",
    "            correct_evidence_list.append(dict_out)\n",
    "            # incorrect\n",
    "            lines_file = wiki_database.get_lines_list_from_title(title)\n",
    "            cosine_distance_list = []\n",
    "            for i in range(len(lines_file)):\n",
    "                text_line = lines_file[i]\n",
    "                if len(text_line)>4 and (i not in sentences_correct_list):\n",
    "                    cosine_distance = get_cosine(text_claim, text_line)\n",
    "                else:\n",
    "                    cosine_distance = 0 \n",
    "                cosine_distance_list.append(cosine_distance)\n",
    "            index_largest_cosine_distance = cosine_distance_list.index(max(cosine_distance_list))\n",
    "            text_hypothesis_incorrect = lines_file[index_largest_cosine_distance]\n",
    "            \n",
    "            dict_out['premises'] = word_list_claim\n",
    "            dict_out['ids'] = str(claim_dict['id']) + '_' + str(claim_nr) + '_' + str(interpreters_nr) + '_random'\n",
    "            dict_out['labels'] = claim_dict['label']\n",
    "            dict_out['primises_tags'] = tag_list_claim\n",
    "            tag_list_hypotheses, word_list_hypotheses = get_word_tag_list_from_text(text_str = text_hypothesis_incorrect, nlp = nlp, method_tokenization_str = method_tokenization)\n",
    "            dict_out['hypotheses'] = word_list_hypotheses\n",
    "            dict_out['hypotheses_tags'] = tag_list_hypotheses\n",
    "            incorrect_evidence_list.append(dict_out)\n",
    "        interpreters_nr += 1\n",
    "    return correct_evidence_list, incorrect_evidence_list\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "import spacy\n",
    "\n",
    "from utils_db import mkdir_if_not_exist, dict_load_json, dict_save_json\n",
    "\n",
    "\n",
    "        \n",
    "#     def create_database(self):\n",
    "        \n",
    "#     def get_data_dict(self, data_set_type):\n",
    "#         # description\n",
    "#         # input:\n",
    "#         # - data_set_type: 'train', 'validation' or 'dev' [str]\n",
    "        \n",
    "#         path_data_type_dir = os.path.join(self.path_base_folder_dir, data_set_type)\n",
    "        \n",
    "#         return data\n",
    "        \n",
    "#     def save_data_dict(self, data):\n",
    "\n",
    "\n",
    "import re, math\n",
    "from collections import Counter\n",
    "\n",
    "WORD = re.compile(r'\\w+')\n",
    "\n",
    "def get_cosine(text1, text2):\n",
    "    vec1 = text_to_vector(text1)\n",
    "    vec2 = text_to_vector(text2)\n",
    "    intersection = set(vec1.keys()) & set(vec2.keys())\n",
    "    numerator = sum([vec1[x] * vec2[x] for x in intersection])\n",
    "\n",
    "    sum1 = sum([vec1[x]**2 for x in vec1.keys()])\n",
    "    sum2 = sum([vec2[x]**2 for x in vec2.keys()])\n",
    "    denominator = math.sqrt(sum1) * math.sqrt(sum2)\n",
    "\n",
    "    if not denominator:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return float(numerator) / denominator\n",
    "\n",
    "def text_to_vector(text):\n",
    "    words = WORD.findall(text)\n",
    "    return Counter(words)\n",
    "\n",
    "from wiki_database import Text\n",
    "\n",
    "def get_word_tag_list_from_text(text_str, nlp, method_tokenization_str):\n",
    "    doc = nlp(text_str)\n",
    "    text = Text(doc)\n",
    "    delimiter = text.delimiter_position_tag\n",
    "    tokenized_text_list = text.process([method_tokenization_str])\n",
    "    word_list = []\n",
    "    tag_list = []\n",
    "\n",
    "    for i in range(len(tokenized_text_list)):\n",
    "        key = tokenized_text_list[i]\n",
    "        tag, word = key.split(delimiter)\n",
    "        if i == 0:\n",
    "            if not(tag == 'PROPN'):\n",
    "                word = word.lower()\n",
    "        word_list.append(word)\n",
    "        tag_list.append(tag)\n",
    "    \n",
    "    return tag_list, word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'The Ten Commandments (1956 film)': [0, 20]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "from wiki_database import Text\n",
    "\n",
    "from utils_wiki_database import normalise_text\n",
    "\n",
    "method_tokenization = 'tokenize_text_pos'\n",
    "\n",
    "claim_nr = 9\n",
    "\n",
    "dict_out = {}\n",
    "claim_dict = claim_dict_list[claim_nr]\n",
    "# doc = claim_database.nlp(claim_dict['claim'])\n",
    "\n",
    "claim_dict\n",
    "\n",
    "sentence_dict = {}\n",
    "for interpreter in claim_dict['evidence']:\n",
    "    for proof in interpreter:\n",
    "        title = proof[2]\n",
    "        if title is not None:\n",
    "            normalised_title = normalise_text(title)\n",
    "            line_nr = proof[3]\n",
    "            evidence_sentence = wiki_database.get_lines_from_title(normalised_title, [line_nr])[0]\n",
    "            if normalised_title in sentence_dict:\n",
    "                sentence_dict[normalised_title].append(line_nr)\n",
    "            else:\n",
    "                sentence_dict[normalised_title] = [line_nr]\n",
    "sentence_dict\n",
    "for title, sentences_correct_list in sentence_dict.items():\n",
    "    \n",
    "#         get_cosine(text1, text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-4ad11a0dfd66>, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-4ad11a0dfd66>\"\u001b[0;36m, line \u001b[0;32m4\u001b[0m\n\u001b[0;31m    Two folders:\u001b[0m\n\u001b[0m              ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Folder Structure\n",
    "# database_stage_2\n",
    "# label_is_evidence\n",
    "# label_is_not_evidence\n",
    "# combined equal\n",
    "# combined not equal\n",
    "\n",
    "# for every claim:\n",
    "# every evidence sentence -> label is_evidence\n",
    "# other sentences in document -> label is_not_evidence\n",
    "# Two folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"ids\": ids,\n",
    "\"premises\": premises,\n",
    "\"hypotheses\": hypotheses,\n",
    "\"labels\": labels}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
