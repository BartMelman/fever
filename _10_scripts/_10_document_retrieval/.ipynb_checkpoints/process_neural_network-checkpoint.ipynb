{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import os \n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 1\n",
    "\n",
    "claim_tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataNeuralNetwork\n",
      "- data already created\n",
      "NeuralNetwork\n",
      "- load model\n"
     ]
    }
   ],
   "source": [
    "from neural_network import NeuralNetwork\n",
    "# load model\n",
    "# === variables === #\n",
    "claim_data_set = 'dev'\n",
    "method_database = 'equal_class' # include_all, equal_class\n",
    "setup = 1\n",
    "settings_model = {}\n",
    "settings_model['fraction_training'] = 0.9\n",
    "settings_model['use_cuda'] = False\n",
    "settings_model['seed'] = 1\n",
    "settings_model['lr'] = 0.001\n",
    "settings_model['momentum'] = 0.9 # 0.5\n",
    "settings_model['params'] = {'batch_size': 64, 'shuffle': True}\n",
    "settings_model['nr_epochs'] = 10\n",
    "settings_model['log_interval'] = 10\n",
    "settings_model['width'] = 2000\n",
    "\n",
    "neural_network = NeuralNetwork(claim_data_set, method_database, setup, settings_model)\n",
    "    \n",
    "model_nn = neural_network.model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "claim database\n",
      "- claim database already exists\n"
     ]
    }
   ],
   "source": [
    "from utils_doc_results import ClaimDatabase\n",
    "\n",
    "claim_database = ClaimDatabase(path_dir_database = claim_tensor_db.path_dir_claim_database, \n",
    "                               path_raw_data = claim_tensor_db.path_raw_claim_data, \n",
    "                               claim_data_set = claim_tensor_db.claim_data_set)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_db import dict_load_json, dict_save_json\n",
    "from utils_doc_results import Claim\n",
    "from tqdm import tqdm \n",
    "from doc_results_db import ClaimFile\n",
    "\n",
    "class PredictLabels():\n",
    "    def __init__(self, K, threshold, method, claim_tensor_db, wiki_database, neural_network):\n",
    "        # --- process input --- #\n",
    "        self.K = K\n",
    "        self.threshold = threshold\n",
    "        self.method = method\n",
    "        self.claim_tensor_db = claim_tensor_db\n",
    "        self.model_nn = neural_network.model\n",
    "        # --- variables --- #\n",
    "        self.nr_claims = self.claim_tensor_db.settings['nr_total']\n",
    "        self.path_predict_label_dir = os.path.join(claim_tensor_db.path_setup_dir, 'Predictions_' + str(K) + '_' + method + '_' + neural_network.file_name)\n",
    "        mkdir_if_not_exist(self.path_predict_label_dir)\n",
    "        self.path_settings = os.path.join(self.path_predict_label_dir, 'settings.json')\n",
    "        \n",
    "        if os.path.isfile(self.path_settings):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "        else:\n",
    "            self.settings = {}\n",
    "            self.get_accuracy_save_results(wiki_database)\n",
    "            dict_save_json(self.settings, self.path_settings)\n",
    "        \n",
    "    def get_accuracy_save_results(self, wiki_database):\n",
    "        nr_correct = 0\n",
    "        nr_documents_selected = 0\n",
    "        \n",
    "        for id_nr in tqdm(range(self.nr_claims)):\n",
    "            path_file = os.path.join(self.claim_tensor_db.path_dict_variable_list_dir, str(id_nr) + '.json')\n",
    "            dict_variables = dict_load_json(path_file)\n",
    "            id = dict_variables['id']\n",
    "            selected_documents_list = list(dict_variables['selected_documents'].keys())\n",
    "            pred_value_list = []\n",
    "            for selected_document_str in selected_documents_list:\n",
    "                flag_process = 0\n",
    "                if method == 'generated':\n",
    "                    if int(selected_document_str) in dict_variables['ids_generated']:\n",
    "                        flag_process = 1\n",
    "                elif method == 'correct':\n",
    "                    if int(selected_document_str) in dict_variables['ids_correct_docs']:\n",
    "                        flag_process = 1\n",
    "                elif method == 'selected':\n",
    "                    flag_process = 1\n",
    "                else:\n",
    "                    raise ValueError('method not in method_list', method)\n",
    "                    \n",
    "                if flag_process == 1:\n",
    "                    variable_list = dict_variables['selected_documents'][selected_document_str]['list_variables']\n",
    "                    variable_tensor = torch.FloatTensor(variable_list)\n",
    "                    pred_value_list += [self.model_nn(variable_tensor.unsqueeze(0)).item()]\n",
    "                    if 'predicted_true' in dict_variables:\n",
    "                        dict_variables['predicted_true'].append(selected_document_str) \n",
    "                    else:\n",
    "                        dict_variables['predicted_true'] = selected_document_str\n",
    "                        \n",
    "            pred_value_list_sorted = [x for x,_ in sorted(zip(pred_value_list, selected_documents_list))]\n",
    "            pred_id_correct_list = [x for _,x in sorted(zip(pred_value_list, selected_documents_list))]\n",
    "            id_correct_list = []\n",
    "            length_list = len(pred_id_correct_list)\n",
    "\n",
    "            for i in range(length_list):\n",
    "                if i<self.K:\n",
    "                    id_correct_list.append(pred_id_correct_list[length_list-1-i])\n",
    "                elif pred_value_list_sorted[length_list-1-i] > self.threshold:\n",
    "                    id_correct_list.append(pred_id_correct_list[length_list-1-i])\n",
    "                else:\n",
    "                    break\n",
    "                nr_documents_selected += 1\n",
    "\n",
    "            file = ClaimFile(id = id, path_dir_files = self.claim_tensor_db.path_claims_dir)\n",
    "            claim_dict = claim_database.get_claim_from_id(id)\n",
    "            claim = Claim(claim_dict)\n",
    "            for interpreter in claim.evidence:\n",
    "                flag_correctly_predicted = True\n",
    "                for proof in interpreter:\n",
    "                    title_proof = proof[2]\n",
    "                    if title_proof == None:\n",
    "                        raise ValueError('should contain proof')\n",
    "                    else:\n",
    "                        id_proof = wiki_database.get_id_from_title(title_proof)\n",
    "                        if str(id_proof) not in id_correct_list:\n",
    "                            flag_correctly_predicted = False\n",
    "                            break\n",
    "                if flag_correctly_predicted == True:\n",
    "                    nr_correct += 1\n",
    "                    break\n",
    "        \n",
    "        print('accuracy', accuracy, nr_documents_selected)\n",
    "        self.settings['accuracy'] = nr_correct / float(nr_claims)\n",
    "        self.settings['nr_documents_selected'] = nr_documents_selected\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wiki_database\n",
      "- Load existing settings file\n",
      "- Load title dictionary\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-590e010df6ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mclaim_tensor_db\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClaimTensorDatabase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_wiki_pages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_wiki_database_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mwiki_database\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWikiDatabaseSqlite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_wiki_database_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_wiki_pages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmethod_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'generated'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'correct'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'selected'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/02_university/06_thesis/01_code/fever/_10_scripts/_10_document_retrieval/wiki_database.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_dir_database, path_wiki_pages)\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_2_title_dict\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_title_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_title_dictionary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/02_university/06_thesis/01_code/fever/_10_scripts/_10_document_retrieval/wiki_database.py\u001b[0m in \u001b[0;36mget_title_dictionary\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_title_dict_flag\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'- Load title dictionary'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle_2_id_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_title_2_id_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid_2_title_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict_load_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_id_2_title_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/02_university/06_thesis/01_code/fever/_10_scripts/_10_document_retrieval/utils_db.py\u001b[0m in \u001b[0;36mdict_load_json\u001b[0;34m(path_file)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0mdictionary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'json file does not exist'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0mcls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobject_hook\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobject_hook\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    298\u001b[0m         \u001b[0mparse_float\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_float\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_int\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparse_int\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 299\u001b[0;31m         parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n\u001b[0m\u001b[1;32m    300\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[1;32m    352\u001b[0m             \u001b[0mparse_int\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mparse_float\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m             parse_constant is None and object_pairs_hook is None and not kw):\n\u001b[0;32m--> 354\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    338\u001b[0m         \"\"\"\n\u001b[0;32m--> 339\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    340\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/json/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \"\"\"\n\u001b[1;32m    354\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m             \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mJSONDecodeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Expecting value\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os \n",
    "from wiki_database import WikiDatabaseSqlite\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "# claim_data_set = 'dev'\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "\n",
    "setup = 1\n",
    "\n",
    "claim_tensor_db = ClaimTensorDatabase(path_wiki_pages, path_wiki_database_dir, setup)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "\n",
    "method_list = ['generated', 'correct', 'selected']\n",
    "method = 'generated'\n",
    "K = 0\n",
    "threshold = 0.2\n",
    "\n",
    "# nr_claims = claim_tensor_db.settings['nr_total']\n",
    "\n",
    "predict_labels_db = PredictLabels(K, threshold, method, claim_tensor_db, wiki_database, neural_network)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
