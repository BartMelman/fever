{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sqlitedict import SqliteDict\n",
    "import sqlite3\n",
    "import tqdm\n",
    "from tqdm import tnrange\n",
    "import shutil\n",
    "\n",
    "from document import Document\n",
    "\n",
    "from utils_db import dict_save_json, dict_load_json\n",
    "# from tf_idf import count_n_grams\n",
    "from dictionary_batch_idf import DictionaryBatchIDF\n",
    "from vocabulary import Vocabulary, count_n_grams\n",
    "from tfidf_database import TFIDFDatabase\n",
    "\n",
    "from _10_scripts._01_database.wiki_database import WikiDatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/wiki.db\n",
      "/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/vocab_text_2_t_mlc\n",
      "Word count count dictionary already exists\n",
      "Document count dictionary already exists\n",
      "Load title_2_id and id_2_title dictionaries\n"
     ]
    }
   ],
   "source": [
    "path_large_wiki_database = '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_01_data/_03_database/wiki.db' \n",
    "# path_wiki_database = 'wiki.db'\n",
    "table_name_wiki = 'wikipages'\n",
    "# table_name_tf_idf = 'tf_idf'\n",
    "# path_tf_idf_database = 'tf_idf.db'\n",
    "# path_mydict_tf_idf = 'mydict_tf_idf.sqlite'\n",
    "# path_mydict_ids = 'mydict_ids.sqlite'\n",
    "\n",
    "# === settings experiment === #\n",
    "n_gram = 2\n",
    "# method_tokenization = ['tokenize', 'remove_space', 'make_lower_case', 'lemmatization_get_nouns']\n",
    "method_tokenization = ['tokenize', 'make_lower_case']\n",
    "threshold = 0.001\n",
    "method_tf = 'raw_count' # raw_count term_frequency\n",
    "method_df = 'inverse_document_frequency' # \n",
    "delimiter = '\\k'\n",
    "\n",
    "vocab = Vocabulary(path_large_wiki_database, table_name_wiki, n_gram, method_tokenization, 'text')\n",
    "# tf_idf_db = TFIDFDatabase(vocab, method_tf, method_df, delimiter, threshold, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the',\n",
       " 'following',\n",
       " 'are',\n",
       " 'the',\n",
       " 'football',\n",
       " '-lrb-',\n",
       " 'soccer',\n",
       " '-rrb-',\n",
       " 'events',\n",
       " 'of',\n",
       " 'the',\n",
       " 'year',\n",
       " '1928',\n",
       " 'throughout',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_nr=1\n",
    "\n",
    "vocab.text_database.get_tokenized_text_from_id(id_nr, method_tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({('hoi', 'this'): 2, ('this', 'hoi'): 1, ('this', 'is'): 1, ('is', 'the'): 1, ('the', 'first'): 1, ('first', '1'): 1, ('1', 'o'): 1, ('o', '1'): 1, ('1', 'meeting'): 1})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from nltk import FreqDist\n",
    "from nltk.util import ngrams    \n",
    "\n",
    "\n",
    "bigramfdist = FreqDist()\n",
    "\n",
    "line = 'hoi this hoi this is the first 1 o 1 meeting'\n",
    "tokens = line.strip().split(' ')\n",
    "\n",
    "bigrams = ngrams(tokens, 2)\n",
    "bigramfdist.update(bigrams)\n",
    "# compute_freq()\n",
    "\n",
    "bigramfdist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctn=0\n",
    "for key in bigrams:\n",
    "    ctn+=1\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
