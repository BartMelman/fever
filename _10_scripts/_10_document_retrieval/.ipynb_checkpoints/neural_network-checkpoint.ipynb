{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "from utils_db import mkdir_if_not_exist, dict_save_json, dict_load_json\n",
    "\n",
    "import config\n",
    "\n",
    "class DataNeuralNetwork():\n",
    "    def __init__(self, method_database, setup):\n",
    "        self.setup = setup\n",
    "        self.method_database = method_database\n",
    "        self.path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "        self.path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "        self.tensor_db = ClaimTensorDatabase(self.path_wiki_pages, self.path_wiki_database_dir, self.setup)\n",
    "        self.path_combined_data_dir = os.path.join(self.tensor_db.path_results_dir, 'neural_network', 'data_setup_' + str(self.setup) + '_' + self.method_database)\n",
    "        self.path_settings = os.path.join(self.path_combined_data_dir, 'settings.json')\n",
    "        \n",
    "        print('DataNeuralNetwork')\n",
    "        if os.path.isdir(self.path_combined_data_dir):\n",
    "            self.settings = dict_load_json(self.path_settings)\n",
    "        else:\n",
    "            self.settings = {}\n",
    "            self.create_folder_combined_files()\n",
    "            \n",
    "    def create_folder_combined_files(self):\n",
    "        mkdir_if_not_exist(self.path_combined_data_dir)\n",
    "\n",
    "        dir_list = [self.tensor_db.path_label_correct_evidence_true_dir, \n",
    "                   self.tensor_db.path_label_correct_evidence_false_dir,\n",
    "                   self.tensor_db.path_label_refuted_evidence_true_dir,\n",
    "                   self.tensor_db.path_label_refuted_evidence_false_dir]\n",
    "        \n",
    "        nr_observations_list = [self.tensor_db.settings['nr_correct_true'], \n",
    "                                self.tensor_db.settings['nr_correct_false'], \n",
    "                                self.tensor_db.settings['nr_refuted_true'], \n",
    "                                self.tensor_db.settings['nr_refuted_false']]\n",
    "\n",
    "        if method_database == 'include_all':\n",
    "            total_nr_observations = sum(nr_observations_list)\n",
    "            random_id_list = list(range(total_nr_observations))\n",
    "            shuffle(random_id_list)\n",
    "\n",
    "            for i in tqdm(range(len(dir_list))):\n",
    "                dir = dir_list[i]\n",
    "                nr_obervations = nr_observations_list[i]\n",
    "                for idx in range(nr_obervations):\n",
    "                    transformed_ids = random_id_list[idx + sum(nr_observations_list[0:i])]\n",
    "                    \n",
    "                    file_name_variables_load = os.path.join(dir, 'variable_' + str(idx) + '.pt')\n",
    "                    file_name_label_load = os.path.join(dir, 'label_' + str(idx) + '.pt')\n",
    "                    \n",
    "                    file_name_variables_write = os.path.join(self.path_combined_data_dir, 'variable_' + str(transformed_ids) + '.pt')\n",
    "                    file_name_label_write = os.path.join(self.path_combined_data_dir, 'label_' + str(transformed_ids) + '.pt')\n",
    "\n",
    "                    X = torch.load(file_name_variables_load)\n",
    "                    Y = torch.load(file_name_label_load)\n",
    "\n",
    "                    torch.save(X, file_name_variables_write)\n",
    "                    torch.save(Y, file_name_label_write)\n",
    "\n",
    "            self.settings['nr_observations'] = len(random_id_list)\n",
    "\n",
    "        elif method_database == 'equal_class':\n",
    "            min_nr_observations = min(nr_observations_list)\n",
    "            random_id_list = list(range(min_nr_observations*4))\n",
    "            shuffle(random_id_list)\n",
    "\n",
    "            for i in tqdm(range(len(dir_list))):\n",
    "                dir = dir_list[i]\n",
    "                nr_obervations = nr_observations_list[i]\n",
    "                random_id_list_setting = list(range(nr_obervations))\n",
    "                shuffle(random_id_list_setting)\n",
    "                j=0\n",
    "                for idx in random_id_list_setting[0:min_nr_observations]:\n",
    "                    transformed_ids = random_id_list[j + min_nr_observations*i]\n",
    "                    \n",
    "                    file_name_variables_load = os.path.join(dir, 'variable_' + str(idx) + '.pt')\n",
    "                    file_name_label_load = os.path.join(dir, 'label_' + str(idx) + '.pt')                    \n",
    "                    file_name_variables_write = os.path.join(self.path_combined_data_dir, 'variable_' + str(transformed_ids) + '.pt')\n",
    "                    file_name_label_write = os.path.join(self.path_combined_data_dir, 'label_' + str(transformed_ids) + '.pt')\n",
    "\n",
    "                    X = torch.load(file_name_variables_load)\n",
    "                    Y = torch.load(file_name_label_load)\n",
    "\n",
    "                    torch.save(X, file_name_variables_write)\n",
    "                    torch.save(Y, file_name_label_write)\n",
    "                    j += 1\n",
    "\n",
    "            self.settings['nr_observations'] = len(random_id_list)\n",
    "\n",
    "        else:\n",
    "            raise ValueError('method not in method options', method_database)\n",
    "\n",
    "        dict_save_json(self.settings, self.path_settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, nr_input_variables, nr_hidden_neurons, nr_output_variables):\n",
    "        super(Net, self).__init__()\n",
    "#         self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "#         self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc_input = nn.Linear(nr_input_variables, nr_hidden_neurons)\n",
    "        self.fc_hidden = nn.Linear(nr_hidden_neurons, nr_hidden_neurons)\n",
    "        self.fc_output = nn.Linear(nr_hidden_neurons, nr_output_variables)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.bn1 = nn.BatchNorm1d(nr_hidden_neurons)\n",
    "        self.bn2 = nn.BatchNorm1d(nr_hidden_neurons)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc_input(x))\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(self.fc_hidden(x))\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(self.fc_output(x))\n",
    "    \n",
    "        return self.sigmoid(x)\n",
    "    \n",
    "def train(log_interval, model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data.float())\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "def train_performance(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "            output = model(data.float())\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTraining set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "def test_performance(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device, dtype=torch.int64)\n",
    "            output = model(data.float())\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('Test set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = data_nn.tensor_db.settings\n",
    "# settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_variables = data_nn.tensor_db.settings['nr_variables']\n",
    "# nr_variables\n",
    "# nr_variables = 0\n",
    "# list_keys = ['observation_key_list_claim', 'observation_key_list_title', 'observation_key_list_text']\n",
    "# for key in list_keys:\n",
    "#     nr_variables += len(settings[key])\n",
    "# nr_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, path_data_set, list_ids):\n",
    "        'Initialization'\n",
    "        self.path_data_set = path_data_set\n",
    "        self.list_ids = list_ids\n",
    "        self.nr_observations = len(list_ids)\n",
    "        \n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return self.nr_observations\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        \n",
    "        file_name_variables = os.path.join(self.path_data_set, 'variable_' + str(self.list_ids[index])  + '.pt')\n",
    "        file_name_label = os.path.join(self.path_data_set, 'label_' + str(self.list_ids[index])  + '.pt')\n",
    "        \n",
    "        X = torch.load(file_name_variables)\n",
    "        y = torch.load(file_name_label)[0]\n",
    "        \n",
    "        return X, y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "\n",
    "from doc_results_db import ClaimTensorDatabase\n",
    "\n",
    "import config\n",
    "\n",
    "class NeuralNetwork():\n",
    "    def __init__(self, claim_data_set, method_database, setup, settings_model):\n",
    "        self.claim_data_set = claim_data_set\n",
    "        self.method_database = method_database\n",
    "        self.setup = setup\n",
    "        \n",
    "        self.fraction_training = settings_model['fraction_training']\n",
    "        self.use_cuda = settings_model['use_cuda']\n",
    "        self.seed = settings_model['seed']\n",
    "        self.lr = settings_model['lr']\n",
    "        self.momentum = settings_model['momentum']\n",
    "        self.params = settings_model['params']\n",
    "        self.nr_epochs = settings_model['nr_epochs']\n",
    "        self.log_interval = settings_model['log_interval']\n",
    "        \n",
    "        self.data_nn = self.get_data()\n",
    "        self.nr_observations = self.data_nn.settings['nr_observations']\n",
    "        self.nr_variables = self.data_nn.tensor_db.settings['nr_variables']\n",
    "        \n",
    "        self.path_model_dir = os.path.join(self.data_nn.tensor_db.path_results_dir, 'neural_network', 'model')\n",
    "        self.path_model = os.path.join(self.path_model_dir ,'model.pt')\n",
    "        self.path_settings = os.path.join(self.path_model_dir, 'settings.json')\n",
    "        \n",
    "        mkdir_if_not_exist(self.path_model_dir)\n",
    "        \n",
    "        if os.path.isfile(self.path_model):\n",
    "            self.model = torch.load(self.path_model)\n",
    "        else:\n",
    "            self.partition = self.get_partition()\n",
    "            self.training_data_loader, self.validation_data_loader = self.get_data_generators()\n",
    "            self.device, self.model, self.optimizer = self.initialise_model()\n",
    "            self.model = self.train_model()\n",
    "            torch.save(self.model.state_dict(), self.path_model)\n",
    "            \n",
    "        \n",
    "    def get_data(self):\n",
    "        return DataNeuralNetwork(self.method_database, self.setup)\n",
    "    \n",
    "    def get_partition(self):\n",
    "        partition = {}\n",
    "        partition['train'] = list(range(0, int(self.nr_observations*self.fraction_training)))\n",
    "        partition['validation'] = list(range(int(self.nr_observations*self.fraction_training), self.nr_observations))\n",
    "        return partition\n",
    "    \n",
    "    def get_data_generators(self):\n",
    "        training_set = Dataset(self.data_nn.path_combined_data_dir, self.partition['train'])\n",
    "        training_data_loader = data.DataLoader(training_set, **params)\n",
    "\n",
    "        validation_set = Dataset(self.data_nn.path_combined_data_dir, self.partition['validation'])\n",
    "        validation_data_loader = data.DataLoader(validation_set, **params)\n",
    "        return training_data_loader, validation_data_loader\n",
    "        \n",
    "    def train_model(self):\n",
    "        torch.manual_seed(seed)\n",
    "\n",
    "        kwargs = {'num_workers': 6, 'pin_memory': True} if self.use_cuda else {}\n",
    "\n",
    "        device = torch.device(\"cuda\" if self.use_cuda else \"cpu\")\n",
    "\n",
    "        model = Net(nr_input_variables = self.nr_variables, nr_hidden_neurons = 10, nr_output_variables = 2).to(device)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=self.lr, momentum=self.momentum)\n",
    "        \n",
    "        for epoch in range(1, self.nr_epochs + 1):\n",
    "            train(self.log_interval, model, device, self.training_generator, optimizer, epoch)\n",
    "            train_performance(model, device, self.training_generator)\n",
    "            test_performance(model, device, self.validation_generator)\n",
    "        \n",
    "        return model\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataNeuralNetwork\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/model/model.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1320c8684d38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0msettings_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_interval'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mneural_network\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclaim_data_set\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod_database\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msettings_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-baa77a3da8c8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, claim_data_set, method_database, setup, settings_model)\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bmelman/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/model/model.pt'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel\n",
    "# === constants === #\n",
    "\n",
    "# === variables === #\n",
    "claim_data_set = 'dev'\n",
    "method_database = 'equal_class' # include_all, equal_class\n",
    "setup = 1\n",
    "settings_model = {}\n",
    "settings_model['fraction_training'] = 0.9\n",
    "settings_model['use_cuda'] = False\n",
    "settings_model['seed'] = 1\n",
    "settings_model['lr'] = 0.001\n",
    "settings_model['momentum'] = 0.9\n",
    "settings_model['params'] = {'batch_size': 64, 'shuffle': True}\n",
    "settings_model['nr_epochs'] = 5\n",
    "settings_model['log_interval'] = 10\n",
    "\n",
    "neural_network = NeuralNetwork(claim_data_set, method_database, setup, settings_model)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b7ab93f997bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model = torch.load(path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_6.pt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-1909f773a38e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mfile_name_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/label_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.pt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    380\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_6.pt'"
     ]
    }
   ],
   "source": [
    "id = 6\n",
    "file_name_variables = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/variable_' + str(id) + '.pt'\n",
    "file_name_label = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/neural_network/data_setup_1_include_all/label_' + str(id) + '.pt'\n",
    "\n",
    "X = torch.load(file_name_variables)\n",
    "y = torch.load(file_name_label)\n",
    "y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = '/home/bmelman/Desktop/C_disk/02_university/06_thesis/01_code/fever/_04_results/01_score_combination/setup_1/dev_correct_false_tensor'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]),\n",
       " tensor([9.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 1.0000, 0.0000,\n",
       "         2.0000, 1.0000, 0.0000, 0.0000, 3.0000, 1.0000, 0.0000, 0.0000, 1.0000,\n",
       "         0.0000, 0.0000, 1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 3.0780, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 3.0785, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0780, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 3.0785]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id = 1\n",
    "file_name_variables = os.path.join(dir,'variable_' + str(id) + '.pt')\n",
    "file_name_label = os.path.join(dir,'label_' + str(id) + '.pt')\n",
    "\n",
    "X = torch.load(file_name_variables)\n",
    "y = torch.load(file_name_label)\n",
    "y,X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_nr = 0\n",
    "tensor_label = torch.LongTensor([label_nr])\n",
    "torch.save(tensor_label, 'tmp.pt')\n",
    "tensor_temp = torch.load('tmp.pt')\n",
    "torch.save(tensor_temp, 'tmp.pt')\n",
    "torch.load('tmp.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
