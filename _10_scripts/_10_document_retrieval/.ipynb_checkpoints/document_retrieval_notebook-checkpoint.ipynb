{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "from tqdm import tqdm\n",
    "from sqlitedict import SqliteDict\n",
    "\n",
    "from wiki_database import WikiDatabaseSqlite, Text\n",
    "from doc_results_db_utils import get_empty_tag_dict, get_tag_2_id_dict, get_tag_dict, get_vocab_tf_idf_from_exp, get_tf_idf_name, get_vocab_tf_idf_from_exp\n",
    "from doc_results_db_utils import get_dict_from_n_gram\n",
    "from utils_db import dict_load_json, dict_save_json, HiddenPrints, load_jsonl\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer, get_tag_word_from_wordtag\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load existing settings file\n",
      "Load title dictionary\n",
      "tags file already exists\n"
     ]
    }
   ],
   "source": [
    "# === variables === #\n",
    "n_gram = 1\n",
    "claim_data_set = 'dev'\n",
    "folder_name_score_combination = 'score_combination'\n",
    "\n",
    "path_dir_results = os.path.join(config.ROOT, config.RESULTS_DIR, folder_name_score_combination)\n",
    "path_tags = os.path.join(path_dir_results, 'tags_' + claim_data_set + '_n_gram_' + str(n_gram) + '.json')\n",
    "# path_dir_files = os.path.join(path_dir_results, claim_data_set)\n",
    "path_dir_claims = os.path.join(path_dir_results, 'claims_' + claim_data_set)\n",
    "path_wiki_pages = os.path.join(config.ROOT, config.DATA_DIR, config.WIKI_PAGES_DIR, 'wiki-pages')\n",
    "path_wiki_database_dir = os.path.join(config.ROOT, config.DATA_DIR, config.DATABASE_DIR)\n",
    "path_claim_data_set = os.path.join(config.ROOT, config.DATA_DIR, config.RAW_DATA_DIR, claim_data_set + \".jsonl\")\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_results, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_results)\n",
    "\n",
    "try:\n",
    "    os.makedirs(path_dir_claims, exist_ok=True)\n",
    "except FileExistsError:\n",
    "    print('folder already exists:', path_dir_claim)\n",
    "\n",
    "results = load_jsonl(path_claim_data_set)\n",
    "wiki_database = WikiDatabaseSqlite(path_wiki_database_dir, path_wiki_pages)\n",
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "tag_dict = get_tag_dict(claim_data_set, n_gram, path_tags, wiki_database)\n",
    "nr_claims = 20#len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('claim database: insert claim\\'s text and claim\\'s tag_list')\n",
    "\n",
    "for str_id, tag_list in tqdm(tag_dict.items(), total = len(tag_dict), desc = 'tag'):\n",
    "    id = int(str_id)\n",
    "    if id < nr_claims:\n",
    "        file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "        file.process_tags(tag_list, n_gram)\n",
    "        file.process_claim(results[id])\n",
    "\n",
    "print('claim database: insert nr words per tag for claim')\n",
    "\n",
    "experiment_nr = 37\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "for id in tqdm(range(nr_claims), desc = 'nr words per pos'):\n",
    "    file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "    file.process_nr_words_per_pos(tf_idf_db, tag_2_id_dict)\n",
    "\n",
    "print('claim database: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load tf_idf nr_words_pos\n"
     ]
    }
   ],
   "source": [
    "from sqlitedict import SqliteDict\n",
    "from doc_results_db_utils import get_vocab_tf_idf_from_exp\n",
    "from utils_db import HiddenPrints\n",
    "\n",
    "# === run experiment === #\n",
    "experiment_nr = 31\n",
    "\n",
    "print('load tf_idf nr_words_pos')\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr, wiki_database)\n",
    "\n",
    "mydict_ids = SqliteDict(tf_idf_db.path_ids_dict)\n",
    "mydict_tf_idf = SqliteDict(tf_idf_db.path_tf_idf_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1_gram': {}}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = ClaimFile(id = 0, path_dir_files = path_dir_results)\n",
    "file.claim_dict['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError a\n",
      "KeyError the\n",
      "KeyError in\n",
      "KeyError the\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   0%|          | 1/1000 [00:00<15:26,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError a\n",
      "KeyError .\n",
      "KeyError the\n",
      "KeyError film\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   0%|          | 3/1000 [00:02<14:06,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError born\n",
      "KeyError in\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   0%|          | 4/1000 [00:02<11:48,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError a\n",
      "KeyError -\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   0%|          | 5/1000 [00:03<10:09,  1.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError 's\n",
      "KeyError was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|          | 6/1000 [00:04<14:50,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError in\n",
      "KeyError .\n",
      "KeyError is\n",
      "KeyError a\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "nr words per pos:   1%|          | 8/1000 [00:09<24:47,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError exclusively\n",
      "KeyError a\n",
      "KeyError film\n",
      "KeyError .\n",
      "KeyError in\n",
      "KeyError is\n",
      "KeyError a\n",
      "KeyError by\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|          | 9/1000 [00:14<40:16,  2.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|          | 10/1000 [00:14<30:08,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n",
      "KeyError is\n",
      "KeyError an\n",
      "KeyError who\n",
      "KeyError to\n",
      "KeyError the\n",
      "KeyError of\n",
      "KeyError the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|          | 11/1000 [00:17<37:53,  2.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|          | 12/1000 [00:18<27:39,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError a\n",
      "KeyError .\n",
      "KeyError The\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|▏         | 13/1000 [00:18<20:20,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n",
      "KeyError 's\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   1%|▏         | 14/1000 [00:19<17:54,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError .\n",
      "KeyError is\n",
      "KeyError a\n",
      "KeyError on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 15/1000 [00:22<30:05,  1.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n",
      "KeyError has\n",
      "KeyError been\n",
      "KeyError of\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 16/1000 [00:25<33:00,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n",
      "KeyError is\n",
      "KeyError a\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 17/1000 [00:33<1:05:55,  4.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError from\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 18/1000 [00:41<1:21:32,  4.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError has\n",
      "KeyError as\n",
      "KeyError the\n",
      "KeyError of\n",
      "KeyError for\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 19/1000 [00:48<1:33:18,  5.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError helped\n",
      "KeyError -\n",
      "KeyError in\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 20/1000 [00:55<1:41:31,  6.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError the\n",
      "KeyError that\n",
      "KeyError The\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 21/1000 [01:00<1:34:10,  5.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError in\n",
      "KeyError the\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 22/1000 [01:05<1:28:40,  5.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError a\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 23/1000 [01:08<1:19:53,  4.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError in\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▏         | 24/1000 [01:18<1:41:27,  6.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError an\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   2%|▎         | 25/1000 [01:23<1:37:08,  5.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError incapable\n",
      "KeyError of\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 26/1000 [01:27<1:28:11,  5.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 27/1000 [01:29<1:08:46,  4.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError known\n",
      "KeyError as\n",
      "KeyError .\n",
      "KeyError is\n",
      "KeyError in\n",
      "KeyError the\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 28/1000 [01:32<1:05:18,  4.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError of\n",
      "KeyError the\n",
      "KeyError .\n",
      "KeyError a\n",
      "KeyError the\n",
      "KeyError in\n",
      "KeyError the\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 29/1000 [01:44<1:42:44,  6.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError was\n",
      "KeyError by\n",
      "KeyError someone\n",
      "KeyError who\n",
      "KeyError was\n",
      "KeyError The\n",
      "KeyError of\n",
      "KeyError .\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 30/1000 [01:53<1:56:37,  7.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError is\n",
      "KeyError a\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "nr words per pos:   3%|▎         | 31/1000 [01:54<1:26:18,  5.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KeyError .\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-dc2b9d1ee08d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnr_claims_selected\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'nr words per pos'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClaimFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_dir_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_dir_claims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_tf_idf_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtag_2_id_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_db\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydict_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmydict_tf_idf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-d08ab4e79f91>\u001b[0m in \u001b[0;36mprocess_tf_idf_experiment\u001b[0;34m(self, tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf)\u001b[0m\n\u001b[1;32m     64\u001b[0m                             \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_title_from_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_idf_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwiki_database\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                             \u001b[0mclaim_doc_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClaimDocTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdelimiter_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                             \u001b[0mn_grams_dict_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_words_title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclaim_doc_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_grams\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_idf_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod_tokenization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf_idf_db\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m             )\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomponent_cfg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0mcomponent_cfg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/spacy/language.py\u001b[0m in \u001b[0;36mmake_doc\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgolds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent_cfg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mtokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.Doc.__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc._get_chunker\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_lang_class\u001b[0;34m(lang)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;31m# Check if an entry point is exposed for the language code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0mentry_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_entry_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"spacy_languages\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mentry_point\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mLANGUAGES\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/spacy/util.py\u001b[0m in \u001b[0;36mget_entry_point\u001b[0;34m(key, value)\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0mRETURNS\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mentry\u001b[0m \u001b[0mpoint\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \"\"\"\n\u001b[0;32m--> 260\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mentry_point\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpkg_resources\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_entry_points\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mentry_point\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/C_disk/03_environment/03_fever/lib/python3.6/site-packages/pkg_resources/__init__.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0mentry\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 656\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_entry_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    657\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    658\u001b[0m         )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "nr_claims_selected = 1000 #nr_claims\n",
    "\n",
    "for id in tqdm(range(nr_claims_selected), desc = 'nr words per pos'):\n",
    "    file = ClaimFile(id = id, path_dir_files = path_dir_claims)\n",
    "    file.process_tf_idf_experiment(tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils_db import dict_save_json\n",
    "from doc_results_db_utils import get_tf_idf_name\n",
    "from wiki_database import Text\n",
    "from doc_results_db import get_dict_from_n_gram\n",
    "from tqdm import tnrange, tqdm\n",
    "from utils_doc_results import Claim, ClaimDocTokenizer\n",
    "class ClaimFile:\n",
    "    \"\"\"A sample Employee class\"\"\"\n",
    "    def __init__(self, id, path_dir_files):\n",
    "        self.path_claim = os.path.join(path_dir_files, str(id) + '.json')\n",
    "        if os.path.isfile(self.path_claim):\n",
    "            self.claim_dict = dict_load_json(self.path_claim)\n",
    "        else:\n",
    "            self.claim_dict = {}\n",
    "            self.claim_dict['claim'] = {}\n",
    "            self.claim_dict['claim']['1_gram'] = {}\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = None\n",
    "            self.claim_dict['claim']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "            self.claim_dict['title'] = {}\n",
    "            self.claim_dict['title']['1_gram'] = {}\n",
    "#             self.claim_dict['title']['1_gram']['nr_words'] = None\n",
    "#             self.claim_dict['title']['1_gram']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "#             self.claim_dict['title']['ids'] = {}\n",
    "            self.save_claim()\n",
    "\n",
    "    def process_claim(self, claim):\n",
    "        self.claim_dict['claim']['text'] = claim\n",
    "        self.save_claim()\n",
    "\n",
    "    def process_tags(self, tag_list, n_gram):\n",
    "#         self.claim_dict['claim'][str(n_gram) +'_gram'] = {}\n",
    "        if n_gram == 1:\n",
    "            self.claim_dict['claim'][str(n_gram) +'_gram']['tag_list'] = tag_list\n",
    "        else:\n",
    "            raise ValueError('written for n_gram == 1')\n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_tf_idf_experiment(self, tag_2_id_dict, tf_idf_db, mydict_ids, mydict_tf_idf):\n",
    "        tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            \n",
    "            tag_list = [word.pos_ for word in doc]        \n",
    "#             claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "#             n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "            # === write tf-idf values === #\n",
    "            claim_text = Text(doc)\n",
    "            tokenized_claim_list = claim_text.process(tf_idf_db.vocab.method_tokenization)\n",
    "#             print(tag_list, tokenized_claim_list)\n",
    "            idx = 0\n",
    "            for i in range(len(tag_list)):\n",
    "                tag = tag_list[i]\n",
    "                word = tokenized_claim_list[i]\n",
    "            \n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                \n",
    "                with HiddenPrints():\n",
    "                    dictionary = get_dict_from_n_gram([word], mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "#                 print(len(dictionary))\n",
    "                if len(dictionary) < 2000:\n",
    "                    for id, tf_idf_value in dictionary.items():\n",
    "                        # === create dictionary if does not exist === #\n",
    "                        if str(id) not in self.claim_dict['title']['1_gram']:\n",
    "                            self.claim_dict['title']['1_gram'][str(id)] = {}\n",
    "                            title = tf_idf_db.vocab.wiki_database.get_title_from_id(id)\n",
    "\n",
    "                            doc = tf_idf_db.vocab.wiki_database.nlp(title)\n",
    "                            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "                            n_grams_dict_title, nr_words_title = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "                            self.claim_dict['title']['1_gram'][str(id)]['nr_words'] = nr_words_title\n",
    "\n",
    "                        if vocab.method_tokenization[0] not in self.claim_dict['title']['1_gram'][str(id)].keys():\n",
    "                            self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]] = {}\n",
    "                            if tf_idf_name not in self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]].keys():\n",
    "                                self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]][tf_idf_name] = get_empty_tag_dict()\n",
    "\n",
    "                        self.claim_dict['title']['1_gram'][str(id)][vocab.method_tokenization[0]][tf_idf_name][str(pos_id)] += tf_idf_value   \n",
    "                idx += 1\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "        \n",
    "        self.save_claim()\n",
    "    \n",
    "    def process_nr_words_per_pos(self, tf_idf_db, tag_2_id_dict):\n",
    "        if tf_idf_db.n_gram == 1:\n",
    "            doc = tf_idf_db.vocab.wiki_database.nlp(self.claim_dict['claim']['text'])\n",
    "            claim_doc_tokenizer = ClaimDocTokenizer(doc, tf_idf_db.vocab.delimiter_words)\n",
    "            n_grams_dict, nr_words = claim_doc_tokenizer.get_n_grams(tf_idf_db.vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "            self.claim_dict['claim']['1_gram']['nr_words'] = sum(n_grams_dict.values())\n",
    "            \n",
    "            for key, count in n_grams_dict.items():\n",
    "                tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "                pos_id = tag_2_id_dict[tag]\n",
    "                self.claim_dict['claim']['1_gram']['nr_words_per_pos'][str(pos_id)] += count\n",
    "            self.save_claim()\n",
    "        else:\n",
    "            raise ValueError('Adapt function for bigrams')\n",
    "\n",
    "    def save_claim(self):\n",
    "        with HiddenPrints():\n",
    "            dict_save_json(self.claim_dict, self.path_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 save list claim tags\n",
    "\n",
    "# step 2 create files in folder with claim id\n",
    "\n",
    "# step 3 iterate for every experiment through \n",
    "\n",
    "# step 4\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_2_id_dict = get_tag_2_id_dict()\n",
    "\n",
    "experiment_nr = 37\n",
    "\n",
    "with HiddenPrints():\n",
    "    vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "claim_dict = {}\n",
    "\n",
    "# claim features\n",
    "claim_dict['claim'] = {}\n",
    "claim_dict['claim']['nr_words'] = sum(n_grams.values())\n",
    "claim_dict['claim']['nr_words_per_pos'] = get_empty_tag_dict()\n",
    "\n",
    "for key, count in n_grams.items():\n",
    "    tag, word = get_tag_word_from_wordtag(key, vocab.delimiter_tag_word)\n",
    "    pos_id = tag_2_id_dict[tag]\n",
    "    claim_dict['claim']['nr_words_per_pos'][pos_id] += count\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-143-b597943da182>, line 22)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-143-b597943da182>\"\u001b[0;36m, line \u001b[0;32m22\u001b[0m\n\u001b[0;31m    claim_dict['title']['ids'][id][vocab.method_tokenization[0]]['tf_idf_sum_per_pos_' + str(experiment_nr)][] += tf_idf_value\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# title features\n",
    "claim_dict['title'] = {}\n",
    "claim_dict['title']['ids'] = {}\n",
    "# get_empty_dict_claim()\n",
    "# tf idf per tag\n",
    "experiment_nr = 31\n",
    "vocab, tf_idf_db = get_vocab_tf_idf_from_exp(experiment_nr)\n",
    "\n",
    "doc = vocab.wiki_database.nlp(claim.claim)\n",
    "claim_doc_tokenizer = ClaimDocTokenizer(doc, vocab.delimiter_words)\n",
    "n_grams, nr_words = claim_doc_tokenizer.get_n_grams(vocab.method_tokenization, tf_idf_db.vocab.n_gram)\n",
    "\n",
    "# === initialise databases === #\n",
    "path_title_ids = tf_idf_db.path_ids_dict\n",
    "path_title_tf_idf = tf_idf_db.path_tf_idf_dict\n",
    "mydict_ids = SqliteDict(path_title_ids)\n",
    "mydict_tf_idf = SqliteDict(path_title_tf_idf)\n",
    "\n",
    "dictionary = get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db)\n",
    "\n",
    "tf_idf_name = get_tf_idf_name(experiment_nr)\n",
    "tag_nr = get_tag_2_id_dict()\n",
    "\n",
    "for id, tf_idf_value in dictionary.items():\n",
    "    if id in claim_dict['title']['ids'].keys():\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "    else:\n",
    "        claim_dict['title']['ids'][id] = get_empty_dict_claim()\n",
    "        claim_dict['title']['ids'][id][vocab.method_tokenization[0]][tf_idf_name][] += tf_idf_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dict_from_n_gram(mydict_ids, mydict_tf_idf, tf_idf_db):\n",
    "    \n",
    "    dictionary = {}\n",
    "\n",
    "    for word in n_grams:\n",
    "        try:\n",
    "            word_id_list = mydict_ids[word].split(tf_idf_db.delimiter)[1:]\n",
    "            word_tf_idf_list = mydict_tf_idf[word].split(tf_idf_db.delimiter)[1:]\n",
    "        except KeyError:\n",
    "            print('KeyError', word)\n",
    "            word_id_list = []\n",
    "            word_tf_idf_list = []\n",
    "        for j in range(len(word_id_list)):\n",
    "            id = int(word_id_list[j])       \n",
    "            tf_idf = float(word_tf_idf_list[j])\n",
    "            try:\n",
    "                dictionary[id] = dictionary[id] + tf_idf\n",
    "            except KeyError:\n",
    "                dictionary[id] = tf_idf\n",
    "    return dictionary\n",
    "\n",
    "def get_empty_dict_claim():\n",
    "    empty_dict = {}\n",
    "    empty_dict['nr_words_title'] = None\n",
    "    empty_dict['nr_words_per_pos'] = None\n",
    "    empty_dict['tokenize'] = {}\n",
    "    empty_dict['tokenize']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['tf_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['raw_count_idf'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize']['idf'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "#     empty_dict['tokenize']['max_sum_idf'] = 0\n",
    "    empty_dict['tokenize_lemma'] = {}\n",
    "    empty_dict['tokenize_lemma']['matches_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['tf_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['idf_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['raw_count_sum_per_pos'] = get_empty_tag_dict()\n",
    "    empty_dict['tokenize_lemma']['max_sum_idf'] = 0\n",
    "    return empty_dict\n",
    "       \n",
    "def get_empty_tag_dict():\n",
    "    tag_2_id_dict = get_tag_2_id_dict()\n",
    "    empty_tag_dict = {}\n",
    "    for pos_id in range(len(tag_2_id_dict)):\n",
    "        empty_tag_dict[str(pos_id)] = 0\n",
    "    return empty_tag_dict\n",
    "\n",
    "def get_tag_2_id_dict():\n",
    "    tag_2_id_dict = {}\n",
    "    tag_list = ['ADJ', 'ADP', 'ADV', 'AUX', 'CONJ', 'CCONJ', 'DET', 'INTJ', 'NOUN', 'NUM', 'PART', 'PRON', 'PROPN', 'PUNCT', 'SCONJ', 'SYM', 'VERB', 'X', 'SPACE']\n",
    "\n",
    "    for i in range(len(tag_list)):\n",
    "        tag = tag_list[i]\n",
    "        tag_2_id_dict[tag] = i\n",
    "    return tag_2_id_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab_tf_idf_from_exp(experiment_nr):\n",
    "    file_name = 'experiment_%.2d.json'%(experiment_nr)\n",
    "    path_experiment = os.path.join(config.ROOT, config.CONFIG_DIR, file_name)\n",
    "\n",
    "    with open(path_experiment) as json_data_file:\n",
    "        data = json.load(json_data_file)\n",
    "\n",
    "    vocab = VocabularySqlite(wiki_database = wiki_database, n_gram = data['n_gram'],\n",
    "        method_tokenization = data['method_tokenization'], tags_in_db_flag = data['tags_in_db_flag'], \n",
    "        source = data['vocabulary_source'], tag_list_selected = data['tag_list_selected'])\n",
    "\n",
    "    tf_idf_db = TFIDFDatabaseSqlite(vocabulary = vocab, method_tf = data['method_tf'], method_df = data['method_df'],\n",
    "        delimiter = data['delimiter'], threshold = data['threshold'], source = data['tf_idf_source'])\n",
    "    return vocab, tf_idf_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "\n",
    "# Disable\n",
    "def blockPrint():\n",
    "    sys.stdout = open(os.devnull, 'w')\n",
    "\n",
    "# Restore\n",
    "def enablePrint():\n",
    "    sys.stdout = sys.__stdout__\n",
    "\n",
    "\n",
    "# print 'This will print'\n",
    "\n",
    "# blockPrint()\n",
    "# print \"This won't\"\n",
    "\n",
    "# enablePrint()\n",
    "# print \"This will too\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting spacy\n",
      "  Using cached https://files.pythonhosted.org/packages/a1/5b/0fab3fa533229436533fb504bb62f4cf7ea29541a487a9d1a0749876fc23/spacy-2.1.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: requests<3.0.0,>=2.13.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: murmurhash<1.1.0,>=0.28.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting wasabi<1.1.0,>=0.2.0 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/f4/c1/d76ccdd12c716be79162d934fe7de4ac8a318b9302864716dde940641a79/wasabi-0.2.2-py3-none-any.whl\n",
      "Collecting blis<0.3.0,>=0.2.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/34/46/b1d0bb71d308e820ed30316c5f0a017cb5ef5f4324bcbc7da3cf9d3b075c/blis-0.2.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: jsonschema<3.1.0,>=2.6.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting thinc<7.1.0,>=7.0.2 (from spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/a9/f1/3df317939a07b2fc81be1a92ac10bf836a1d87b4016346b25f8b63dee321/thinc-7.0.4-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: preshed<2.1.0,>=2.0.1 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: plac<1.0.0,>=0.9.6 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Requirement already up-to-date: cymem<2.1.0,>=2.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from spacy)\n",
      "Collecting numpy>=1.15.0 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/87/2d/e4656149cbadd3a8a0369fcd1a9c7d61cc7b87b3903b85389c70c989a696/numpy-1.16.4-cp36-cp36m-manylinux1_x86_64.whl (17.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 17.3MB 92kB/s  eta 0:00:01\n",
      "\u001b[?25hCollecting srsly<1.1.0,>=0.0.5 (from spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/aa/6c/2ef2d6f4c63a197981f4ac01bb17560c857c6721213c7c99998e48cdda2a/srsly-0.0.7-cp36-cp36m-manylinux1_x86_64.whl (180kB)\n",
      "\u001b[K    100% |████████████████████████████████| 184kB 4.6MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/60/247f23a7121ae632d62811ba7f273d0e58972d75e58a94d329d51550a47d/urllib3-1.25.3-py2.py3-none-any.whl (150kB)\n",
      "\u001b[K    100% |████████████████████████████████| 153kB 5.6MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: idna<2.9,>=2.5 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/69/1b/b853c7a9d4f6a6d00749e94eb6f3a041e342a885b87340b79c1ef73e3a78/certifi-2019.6.16-py2.py3-none-any.whl (157kB)\n",
      "\u001b[K    100% |████████████████████████████████| 163kB 5.4MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: chardet<3.1.0,>=3.0.2 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy)\n",
      "Requirement already up-to-date: pyrsistent>=0.14.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Requirement already up-to-date: six>=1.11.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting setuptools (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "  Using cached https://files.pythonhosted.org/packages/ec/51/f45cea425fd5cb0b0380f5b0f048ebc1da5b417e48d304838c02d6288a1e/setuptools-41.0.1-py2.py3-none-any.whl\n",
      "Requirement already up-to-date: attrs>=17.4.0 in /home/bmelman/C_disk/03_environment/03_fever/lib/python3.6/site-packages (from jsonschema<3.1.0,>=2.6.0->spacy)\n",
      "Collecting tqdm<5.0.0,>=4.10.0 (from thinc<7.1.0,>=7.0.2->spacy)\n",
      "  Downloading https://files.pythonhosted.org/packages/9f/3d/7a6b68b631d2ab54975f3a4863f3c4e9b26445353264ef01f465dc9b0208/tqdm-4.32.2-py2.py3-none-any.whl (50kB)\n",
      "\u001b[K    100% |████████████████████████████████| 51kB 6.4MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: wasabi, numpy, blis, tqdm, srsly, thinc, spacy, urllib3, certifi, setuptools\n",
      "  Found existing installation: numpy 1.16.3\n",
      "    Uninstalling numpy-1.16.3:\n",
      "      Successfully uninstalled numpy-1.16.3\n",
      "  Found existing installation: tqdm 4.32.1\n",
      "    Uninstalling tqdm-4.32.1:\n",
      "      Successfully uninstalled tqdm-4.32.1\n",
      "  Found existing installation: thinc 6.12.1\n",
      "    Uninstalling thinc-6.12.1:\n",
      "      Successfully uninstalled thinc-6.12.1\n",
      "  Found existing installation: spacy 2.0.18\n",
      "    Uninstalling spacy-2.0.18:\n",
      "      Successfully uninstalled spacy-2.0.18\n",
      "  Found existing installation: urllib3 1.25.2\n",
      "    Uninstalling urllib3-1.25.2:\n",
      "      Successfully uninstalled urllib3-1.25.2\n",
      "  Found existing installation: certifi 2019.3.9\n",
      "    Uninstalling certifi-2019.3.9:\n",
      "      Successfully uninstalled certifi-2019.3.9\n",
      "  Found existing installation: setuptools 39.0.1\n",
      "    Uninstalling setuptools-39.0.1:\n",
      "      Successfully uninstalled setuptools-39.0.1\n",
      "Successfully installed blis-0.2.4 certifi-2019.6.16 numpy-1.16.4 setuptools-41.0.1 spacy-2.1.4 srsly-0.0.7 thinc-7.0.4 tqdm-4.32.2 urllib3-1.25.3 wasabi-0.2.2\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
